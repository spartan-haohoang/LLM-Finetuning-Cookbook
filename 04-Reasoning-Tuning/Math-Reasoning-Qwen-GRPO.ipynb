{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tune DeepSeek-R1 with Synthetic Reasoning Dataset\n\nThis notebook demonstrates how to fine-tune **DeepSeek-R1** using **synthetic reasoning data** for solving Python coding problems.\n\n## \ud83e\udde0 What is DeepSeek-R1?\n\n**DeepSeek-R1** is a groundbreaking reasoning model that:\n- Rivals OpenAI's O1 in reasoning capabilities\n- Simulates step-by-step logical thinking\n- Excels at mathematics, coding, logic, law, medicine\n- Uses explicit reasoning chains (`<think>` tags)\n- Available as open-source models\n\n## \ud83c\udfaf What You'll Learn\n\n- How to generate synthetic reasoning datasets\n- How to fine-tune DeepSeek-R1 with Unsloth\n- How to work with reasoning chains\n- How to solve coding problems with reasoning\n- How to evaluate reasoning model performance\n\n## \ud83d\udca1 Why Synthetic Data?\n\n**Advantages:**\n- \u2705 **Data scarcity solution**: Create data for niche domains\n- \u2705 **Cost-effective**: No manual labeling required\n- \u2705 **Customizable**: Tailor to specific tasks\n- \u2705 **Scalable**: Generate thousands of examples\n- \u2705 **Quality control**: Iterate and improve\n\n## \ud83d\udd27 Requirements\n\n- **GPU**: 16GB+ VRAM (for 1.5B model with 4-bit quantization)\n- **Time**: 2-4 hours (dataset generation + training)\n- **Model**: DeepSeek-R1-Distill-Qwen-1.5B (4-bit)\n- **Library**: Unsloth (optimized fine-tuning)\n\n## \ud83d\udcca Key Stats\n\n| Metric | Value |\n|--------|-------|\n| Base Model | DeepSeek-R1-Distill-Qwen-1.5B |\n| Quantization | 4-bit (BitsAndBytes) |\n| LoRA Rank | 16 |\n| Dataset Size | 500 examples |\n| Training Time | 1-2 hours |\n| GPU Memory | ~10-12GB |\n\n## \ud83d\udcd6 Table of Contents\n\n1. [Generate Reasoning Dataset](#1-generate-reasoning-dataset)\n2. [Setup and Load Model](#2-setup-and-load-model)\n3. [Prepare Training Data](#3-prepare-training-data)\n4. [Fine-Tune with Unsloth](#4-fine-tune-with-unsloth)\n5. [Inference and Evaluation](#5-inference-and-evaluation)\n\n---\n\n**Credits**: Based on the tutorial by [Sara Han D\u00edaz](https://huggingface.co/blog/sdiazlor/deepseek-synthetic-data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Generate Reasoning Dataset\n\n### About Synthetic Data Generation\n\nWe'll use the **Synthetic Data Generator** to create high-quality reasoning data.\n\n**What it does:**\n- Generates reasoning chains for coding problems\n- Uses DeepSeek-R1 to create training examples\n- Produces structured prompt-completion pairs\n- Includes step-by-step thinking process\n\n### Options for Dataset Generation\n\n**Option 1: Use Synthetic Data Generator (Recommended)**\n- User-friendly Hugging Face Space\n- No code required\n- Configurable with different models\n- Automatic upload to Hub\n\n**Option 2: Use Pre-generated Dataset**\n- We'll use `sdiazlor/python-reasoning-dataset`\n- 500 examples already generated\n- Ready for immediate use\n\n**Option 3: Generate Programmatically**\n- Use the Distilabel library\n- Full control over generation\n- Scriptable and reproducible\n\n### Dataset Format\n\nEach example has:\n- **prompt**: Python coding problem\n- **completion**: Reasoning chain + solution (with `<think>` tags)\n\n### Using Synthetic Data Generator\n\nIf you want to generate your own dataset:\n\n1. Visit the [Synthetic Data Generator Space](https://huggingface.co/spaces/argilla/synthetic-data-generator)\n2. Duplicate the Space\n3. Set `MODEL_COMPLETION` to `deepseek-ai/DeepSeek-R1-Distill-Qwen-32B`\n4. Navigate to \"Chat Data\" tab\n5. Describe your task: \"an assistant that solves python coding problems\"\n6. Configure:\n   - System prompt\n   - Number of examples (e.g., 500)\n   - Temperature for generation (0.6-0.9)\n7. Generate and export to Hub\n\n**Note**: This process takes ~2 hours for 500 examples with DeepSeek."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preview of Reasoning Dataset\n\nLet's look at what a reasoning example looks like:\n\n**Example Prompt:**\n```\nHow can I get the prime numbers from 0 to 125?\n```\n\n**Example Completion (Reasoning Chain):**\n```\n<think>\nOkay, so I need to find all the prime numbers between 0 and 125. \nHmm, primes are numbers greater than 1 that have no divisors other \nthan 1 and themselves.\n\nI remember the Sieve of Eratosthenes is efficient for this.\nSteps:\n1. Create a boolean array for numbers 0 to 125\n2. Mark 0 and 1 as non-prime\n3. For each number from 2 to sqrt(125):\n   - If it's still marked as prime, mark all its multiples as non-prime\n4. Collect all numbers still marked as prime\n</think>\n\nTo find all prime numbers from 0 to 125, we can use the Sieve of \nEratosthenes algorithm:\n\n[Python code here]\n```\n\n**Key Features:**\n- `<think>` tags show reasoning process\n- Step-by-step logical thinking\n- Followed by clear solution with code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Setup and Load Model\n\n### Install Unsloth\n\n**Unsloth** is an optimized library for LLM fine-tuning that:\n- \u2705 2x faster training than standard methods\n- \u2705 Uses 70% less memory\n- \u2705 Works on consumer GPUs\n- \u2705 Fully compatible with Hugging Face\n- \u2705 Supports latest models (DeepSeek, Qwen, Llama, etc.)\n\n### Model Selection\n\nWe'll use **DeepSeek-R1-Distill-Qwen-1.5B** (4-bit quantized):\n- Smaller than the 32B generation model\n- Optimized for training efficiency\n- Maintains reasoning capabilities\n- Fine-tuning improves accuracy"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Install Unsloth and dependencies\n!pip install -q \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n!pip install -q datasets trl\n\nprint(\"\u2705 All packages installed successfully!\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Import necessary libraries\nfrom unsloth import FastLanguageModel\nfrom datasets import load_dataset\nfrom trl import SFTTrainer\nfrom transformers import TrainingArguments\nimport torch\n\nprint(f\"PyTorch version: {torch.__version__}\")\nprint(f\"CUDA available: {torch.cuda.is_available()}\")\nif torch.cuda.is_available():\n    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")\n    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Model configuration\nMODEL = \"unsloth/DeepSeek-R1-Distill-Qwen-1.5B-unsloth-bnb-4bit\"\n\nprint(f\"Model: {MODEL}\")\nprint(f\"\\nThis is a 4-bit quantized version optimized for training\")\nprint(f\"Original model: deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Load the 4-bit pre-quantized model and tokenizer\nprint(f\"Loading {MODEL}...\")\nprint(\"This may take a few minutes...\")\n\nmodel, tokenizer = FastLanguageModel.from_pretrained(\n    model_name=MODEL,\n    max_seq_length=2048,\n    dtype=None,  # Auto-detect\n    load_in_4bit=True,\n)\n\nprint(\"\\n\u2705 Model and tokenizer loaded successfully!\")\nprint(f\"\\nMax sequence length: 2048 tokens\")\nprint(f\"Quantization: 4-bit (BitsAndBytes)\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure LoRA Adapters\n\nWe'll add **LoRA (Low-Rank Adaptation)** adapters to enable efficient fine-tuning.\n\n**LoRA Configuration:**\n- **r=16**: Rank of adaptation (balance between capacity and efficiency)\n- **lora_alpha=16**: Scaling factor\n- **lora_dropout=0**: No dropout (recommended for small models)\n- **Target modules**: All attention and MLP layers\n\n**Why these modules?**\n- `q_proj, k_proj, v_proj, o_proj`: Attention layers\n- `gate_proj, up_proj, down_proj`: MLP layers\n- Targeting all layers gives best reasoning performance"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Add LoRA adapters to the model\nmodel = FastLanguageModel.get_peft_model(\n    model,\n    r=16,  # LoRA rank\n    target_modules=[\n        \"q_proj\",    # Query projection\n        \"k_proj\",    # Key projection\n        \"v_proj\",    # Value projection\n        \"o_proj\",    # Output projection\n        \"gate_proj\", # Gate projection (MLP)\n        \"up_proj\",   # Up projection (MLP)\n        \"down_proj\", # Down projection (MLP)\n    ],\n    lora_alpha=16,  # Scaling factor\n    lora_dropout=0,  # Dropout (0 for small models)\n    bias=\"none\",  # Don't train bias\n    use_gradient_checkpointing=\"unsloth\",  # Memory optimization\n    random_state=3407,  # Reproducibility\n    use_rslora=False,  # Standard LoRA\n    loftq_config=None,  # No LoftQ\n)\n\nprint(\"\u2705 LoRA adapters added successfully!\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Check trainable parameters\ndef print_trainable_parameters(model):\n    \"\"\"\n    Prints the number of trainable parameters in the model.\n    \"\"\"\n    trainable_params = 0\n    all_param = 0\n    for _, param in model.named_parameters():\n        all_param += param.numel()\n        if param.requires_grad:\n            trainable_params += param.numel()\n    \n    trainable_pct = 100 * trainable_params / all_param\n    \n    print(f\"Trainable params: {trainable_params:,}\")\n    print(f\"All params: {all_param:,}\")\n    print(f\"Trainable%: {trainable_pct:.4f}%\")\n    \n    return trainable_params, all_param, trainable_pct\n\nprint(\"=== Model Parameters ===\")\ntrainable, total, pct = print_trainable_parameters(model)\n\nprint(f\"\\n\ud83d\udca1 With LoRA, we're training only {pct:.2f}% of parameters!\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Prepare Training Data\n\n### Load the Synthetic Reasoning Dataset\n\nWe'll load the pre-generated dataset from Hugging Face Hub.\n\n**Dataset**: `sdiazlor/python-reasoning-dataset`\n- 500 Python coding problems\n- Each with reasoning chains\n- Format: prompt + completion with `<think>` tags"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Load the reasoning dataset\nprint(\"Loading python-reasoning-dataset...\")\n\ndataset = load_dataset(\"sdiazlor/python-reasoning-dataset\", split=\"train\")\n\nprint(f\"\\n\u2705 Dataset loaded successfully!\")\nprint(f\"Number of examples: {len(dataset):,}\")\nprint(f\"\\nDataset columns: {dataset.column_names}\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Explore a sample from the dataset\nprint(\"=== Sample Example ===\\n\")\nsample_idx = 0\n\nprint(f\"Prompt:\")\nprint(dataset[sample_idx]['prompt'])\nprint(f\"\\n{'='*70}\\n\")\n\nprint(f\"Completion (first 500 chars):\")\nprint(dataset[sample_idx]['completion'][:500] + \"...\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Show a few more examples\nprint(\"=== Additional Examples ===\\n\")\n\nfor i in range(1, 4):\n    print(f\"--- Example {i} ---\")\n    print(f\"Prompt: {dataset[i]['prompt'][:80]}...\")\n    print(f\"Has reasoning: {'<think>' in dataset[i]['completion']}\")\n    print()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Prompt Template\n\nWe need to format the dataset into a structured prompt that:\n1. Provides clear instructions\n2. Includes the coding problem\n3. Includes the reasoning chain in `<think>` tags\n4. Ends with EOS token\n\n**Format:**\n```\nBelow is an instruction that describes a task, paired with a question.\nWrite a response that appropriately answers the question.\nBefore answering, think carefully but concisely about the question.\n\n### Instruction:\n[System prompt]\n\n### Question:\n[Coding problem]\n\n### Response:\n<think>\n[Reasoning chain]\n</think>\n[Solution]\n<\uff5cend\u2581of\u2581sentence\uff5c>\n```"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Define the prompt template\nprompt_style = \"\"\"Below is an instruction that describes a task, paired with a question that provides further context.\nWrite a response that appropriately answers the question.\nBefore answering, think carefully but concisely about the question and create a step-by-step chain of thoughts to ensure a logical and accurate response.\n\n### Instruction:\nYou are an expert programmer with advanced knowledge of Python. Your task is to provide concise and easy-to-understand solutions. Please answer the following python question.\n\n### Question:\n{}\n\n### Response:\n<think>\n{}\n\"\"\"\n\nEOS_TOKEN = tokenizer.eos_token\n\ndef formatting_prompts_func(examples):\n    \"\"\"\n    Format the dataset examples into the training format.\n    \n    Args:\n        examples: Batch of examples from the dataset\n        \n    Returns:\n        Dictionary with formatted texts\n    \"\"\"\n    prompts = examples[\"prompt\"]\n    completions = examples[\"completion\"]\n    texts = []\n    \n    for prompt, completion in zip(prompts, completions):\n        # Format with template and add EOS token\n        text = prompt_style.format(prompt, completion) + EOS_TOKEN\n        texts.append(text)\n    \n    return {\"text\": texts}\n\nprint(\"\u2705 Formatting function defined!\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Apply formatting to the dataset\nprint(\"Formatting dataset...\")\n\ndataset = dataset.map(\n    formatting_prompts_func,\n    batched=True,\n)\n\nprint(\"\\n\u2705 Dataset formatted successfully!\")\nprint(f\"\\nNew column: 'text'\")\nprint(f\"\\nExample length: {len(dataset['text'][0])} characters\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Inspect a formatted example\nprint(\"=== Formatted Example ===\\n\")\nprint(dataset[\"text\"][0][:1000])  # First 1000 chars\nprint(\"\\n... (truncated) ...\\n\")\nprint(dataset[\"text\"][0][-200:])  # Last 200 chars"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Fine-Tune with Unsloth\n\n### Configure Training Arguments\n\nWe'll set up the training configuration for optimal results.\n\n**Key settings:**\n- **Output directory**: Where to save checkpoints\n- **Epochs**: 3 (good balance for 500 examples)\n- **Batch size**: 2 per device (adjust for your GPU)\n- **Gradient accumulation**: 4 steps (effective batch size = 8)\n- **Learning rate**: 2e-4 (standard for LoRA)\n- **Optimizer**: AdamW 8-bit (memory efficient)\n- **Scheduler**: Linear warmup and decay\n- **Mixed precision**: FP16 or BF16\n- **Logging**: W&B for tracking"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Define training arguments\ntraining_arguments = TrainingArguments(\n    # Output and logging\n    output_dir=\"./deepseek-r1-python-reasoning\",\n    logging_dir=\"./logs\",\n    logging_steps=10,\n    \n    # Training duration\n    num_train_epochs=3,\n    max_steps=-1,  # Train for full epochs\n    \n    # Batch sizes\n    per_device_train_batch_size=2,\n    gradient_accumulation_steps=4,  # Effective batch size = 8\n    \n    # Optimization\n    learning_rate=2e-4,\n    weight_decay=0.01,\n    optim=\"adamw_8bit\",  # Memory efficient optimizer\n    \n    # Learning rate schedule\n    lr_scheduler_type=\"linear\",\n    warmup_steps=5,\n    \n    # Mixed precision\n    fp16=not torch.cuda.is_bf16_supported(),\n    bf16=torch.cuda.is_bf16_supported(),\n    \n    # Checkpointing\n    save_strategy=\"epoch\",\n    save_total_limit=2,  # Keep only last 2 checkpoints\n    \n    # Evaluation\n    # eval_strategy=\"epoch\",  # Uncomment if you have a validation set\n    \n    # Performance\n    gradient_checkpointing=True,\n    max_grad_norm=0.3,\n    \n    # Experiment tracking\n    report_to=\"none\",  # Set to \"wandb\" if using W&B\n    run_name=\"deepseek-r1-python-reasoning\",\n    \n    # Misc\n    seed=3407,\n)\n\nprint(\"\u2705 Training arguments configured!\")\nprint(f\"\\nKey settings:\")\nprint(f\"  Epochs: {training_arguments.num_train_epochs}\")\nprint(f\"  Batch size (per device): {training_arguments.per_device_train_batch_size}\")\nprint(f\"  Gradient accumulation: {training_arguments.gradient_accumulation_steps}\")\nprint(f\"  Effective batch size: {training_arguments.per_device_train_batch_size * training_arguments.gradient_accumulation_steps}\")\nprint(f\"  Learning rate: {training_arguments.learning_rate}\")\nprint(f\"  Mixed precision: FP16={training_arguments.fp16}, BF16={training_arguments.bf16}\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Initialize SFTTrainer\ntrainer = SFTTrainer(\n    model=model,\n    tokenizer=tokenizer,\n    train_dataset=dataset,\n    dataset_text_field=\"text\",\n    max_seq_length=2048,\n    dataset_num_proc=2,\n    packing=False,  # Don't pack sequences (important for reasoning chains)\n    args=training_arguments,\n)\n\nprint(\"\u2705 SFTTrainer initialized!\")\nprint(\"\\nReady to start training...\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Start training!\nprint(\"=\" * 70)\nprint(\"STARTING FINE-TUNING\")\nprint(\"=\" * 70)\nprint()\nprint(f\"Training on {len(dataset):,} examples\")\nprint(f\"Running for {training_arguments.num_train_epochs} epochs\")\nprint(f\"Effective batch size: {training_arguments.per_device_train_batch_size * training_arguments.gradient_accumulation_steps}\")\nprint()\nprint(\"This will take approximately 1-2 hours...\")\nprint(\"Monitor progress via logging output\")\nprint()\nprint(\"=\" * 70)\nprint()\n\n# Train the model\n# Uncomment the line below to start training\n# trainer_stats = trainer.train()\n\nprint(\"\u26a0\ufe0f Training is commented out by default.\")\nprint(\"Uncomment 'trainer_stats = trainer.train()' above to start actual training.\")\nprint()\nprint(\"For testing, you can:\")\nprint(\"  - Reduce num_train_epochs to 1\")\nprint(\"  - Use a smaller subset of data\")\nprint(\"  - Adjust batch size based on your GPU\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Tips\n\n**If you run out of memory:**\n- Reduce `per_device_train_batch_size` to 1\n- Increase `gradient_accumulation_steps` to maintain effective batch size\n- Reduce `max_seq_length` to 1024 or 1536\n\n**If training is slow:**\n- Increase `per_device_train_batch_size` if you have more memory\n- Ensure you're using FP16 or BF16\n- Check that gradient checkpointing is enabled\n\n**To monitor training:**\n- Set `report_to=\"wandb\"` and login to W&B\n- Watch the logging output\n- Check GPU utilization with `nvidia-smi`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the Fine-Tuned Model\n\nAfter training, we can save the model in different formats:\n\n1. **Merged 16-bit**: Full precision merged model\n2. **Merged to Hub**: Push to Hugging Face Hub\n3. **GGUF format**: For llama.cpp and Ollama\n\n**GGUF** (GPT-Generated Unified Format):\n- Optimized for CPU and edge deployment\n- Used by llama.cpp, Ollama, LM Studio\n- Smaller file size with quantization"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Save the fine-tuned model\n# Uncomment when training is complete\n\nMODEL_NAME = \"deepseek-r1-python-reasoning\"\nREPO_NAME = \"your-username/deepseek-r1-python-reasoning\"  # Update with your HF username\n\n# Save locally in 16-bit\n# model.save_pretrained_merged(MODEL_NAME, tokenizer, save_method=\"merged_16bit\")\n# print(f\"\u2705 Model saved locally to: {MODEL_NAME}\")\n\n# Push to Hub in 16-bit\n# model.push_to_hub_merged(REPO_NAME, tokenizer, save_method=\"merged_16bit\")\n# print(f\"\u2705 Model pushed to Hub: {REPO_NAME}\")\n\n# Push to Hub in GGUF format (q4_k_m quantization)\n# model.push_to_hub_gguf(\n#     f\"{REPO_NAME}_q4_k_m\",\n#     tokenizer,\n#     quantization_method=\"q4_k_m\"\n# )\n# print(f\"\u2705 GGUF model pushed to Hub: {REPO_NAME}_q4_k_m\")\n\nprint(\"\u26a0\ufe0f Model saving is commented out\")\nprint(\"Uncomment after training completes\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Inference and Evaluation\n\nNow let's test our fine-tuned model and compare it with the base model!\n\n### Test Question\n\nWe'll ask both models to solve a Python coding problem:\n**\"How can I get the prime numbers from 0 to 125?\"**\n\n### What to Look For\n\n**Base Model (Before Fine-tuning):**\n- May provide high-level outline\n- Less detailed reasoning\n- Might miss code implementation\n- Shorter responses\n\n**Fine-Tuned Model (After Fine-tuning):**\n- Detailed reasoning in `<think>` tags\n- Step-by-step logical thinking\n- Complete code implementation\n- Clear explanation of approach"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Prepare the model for inference\nFastLanguageModel.for_inference(model)  # Enable faster inference\n\nprint(\"\u2705 Model ready for inference!\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Define our test question\nquestion = \"How can I get the prime numbers from 0 to 125?\"\n\nprint(\"=== Test Question ===\")\nprint(question)\nprint(\"=\" * 70)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Response from Base Model (Before Fine-tuning)\n\nLet's see how the base model responds without our fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Note: To truly test the base model, you'd need to load it separately\n# Here we're showing what the response looks like after fine-tuning\n# The comparison would show improvement in reasoning detail and code quality\n\nprint(\"=== BASE MODEL RESPONSE (Before Fine-tuning) ===\\n\")\n\n# Typical base model response (example from the tutorial):\nbase_response = \"\"\"<think>\nTo find all prime numbers between 0 and 125, I can follow these steps:\n\n1. **Define the range**: Identify the start and end of the range, which in this case are 0 and 125.\n\n2. **Create a boolean array**: Initialize an array of booleans with the same length as the range.\n\n3. **Mark non-prime numbers**: Starting from the first prime number (2), iterate through each number.\n\n4. **Identify primes**: After marking the non-primes, the remaining True values correspond to prime numbers.\n\n5. **Output the results**: Extract the indices and list them to obtain all prime numbers from 0 to 125.\n\nThis method ensures that we efficiently identify primes using the Sieve of Eratosthenes algorithm.\n</think>\n\nTo find all prime numbers between 0 and 125, follow these steps:\n[high-level outline without code]\n\"\"\"\n\nprint(base_response)\nprint(\"\\n\" + \"=\" * 70)\nprint(\"\\nNotice: High-level outline but NO code implementation\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Response from Fine-Tuned Model\n\nNow let's see the improved response from our fine-tuned model!"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Generate response with fine-tuned model\nprint(\"=== FINE-TUNED MODEL RESPONSE ===\\n\")\n\n# Format the prompt\nformatted_prompt = prompt_style.format(question, \"\")\n\n# Tokenize\ninputs = tokenizer([formatted_prompt], return_tensors=\"pt\").to(\"cuda\")\n\n# Generate\noutputs = model.generate(\n    input_ids=inputs.input_ids,\n    attention_mask=inputs.attention_mask,\n    max_new_tokens=2048,\n    use_cache=True,\n    temperature=0.7,\n    top_p=0.9,\n    do_sample=True,\n)\n\n# Decode\nresponse = tokenizer.batch_decode(outputs, skip_special_tokens=True)[0]\n\n# Extract just the response part (after \"### Response:\")\nif \"### Response:\" in response:\n    response = response.split(\"### Response:\")[1].strip()\n\nprint(response[:1500])  # Print first 1500 chars\nprint(\"\\n... (truncated for display) ...\")\nprint(\"\\n\" + \"=\" * 70)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis: Before vs After Fine-Tuning\n\n**Key Improvements:**\n\n1. **Detailed Reasoning Chain** \ud83e\udde0\n   - Before: High-level steps\n   - After: Detailed step-by-step thinking in `<think>` tags\n\n2. **Code Implementation** \ud83d\udcbb\n   - Before: No code provided\n   - After: Complete, working Python code\n\n3. **Explanation Quality** \ud83d\udcdd\n   - Before: Brief overview\n   - After: Detailed explanation of the algorithm\n\n4. **Problem-Solving Approach** \ud83c\udfaf\n   - Before: Conceptual understanding\n   - After: Practical, executable solution\n\n**Example Improvements:**\n- Reasoning includes edge case considerations\n- Code is well-commented and structured\n- Solution is immediately usable\n- Explanation connects theory to implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test with More Examples\n\nLet's test the model with various coding problems to validate its reasoning capabilities."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Define multiple test cases\ntest_cases = [\n    \"Write a function to find the factorial of a number\",\n    \"How do I reverse a string in Python?\",\n    \"Create a function to check if a number is palindrome\",\n    \"Write code to find the longest word in a sentence\",\n    \"How can I remove duplicates from a list while preserving order?\",\n]\n\nprint(\"=== Test Cases Prepared ===\\n\")\nfor i, case in enumerate(test_cases, 1):\n    print(f\"{i}. {case}\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Function to test multiple examples\ndef test_reasoning_model(model, tokenizer, question, max_tokens=1024):\n    \"\"\"\n    Test the model on a coding question.\n    \n    Args:\n        model: The fine-tuned model\n        tokenizer: The tokenizer\n        question: The coding question\n        max_tokens: Maximum tokens to generate\n        \n    Returns:\n        Generated response\n    \"\"\"\n    # Format prompt\n    prompt = prompt_style.format(question, \"\")\n    \n    # Tokenize\n    inputs = tokenizer([prompt], return_tensors=\"pt\").to(\"cuda\")\n    \n    # Generate\n    outputs = model.generate(\n        input_ids=inputs.input_ids,\n        attention_mask=inputs.attention_mask,\n        max_new_tokens=max_tokens,\n        use_cache=True,\n        temperature=0.7,\n        do_sample=True,\n    )\n    \n    # Decode\n    response = tokenizer.batch_decode(outputs, skip_special_tokens=True)[0]\n    \n    # Extract response\n    if \"### Response:\" in response:\n        response = response.split(\"### Response:\")[1].strip()\n    \n    return response\n\nprint(\"\u2705 Testing function defined!\")\nprint(\"\\nYou can use this to test your model on any coding problem.\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Test a few cases (uncomment to run)\nprint(\"=== BATCH TEST RESULTS ===\\n\")\n\nfor i, case in enumerate(test_cases[:2], 1):  # Test first 2\n    print(f\"Test {i}: {case}\")\n    print(\"-\" * 70)\n    \n    # Uncomment to run actual inference\n    # result = test_reasoning_model(model, tokenizer, case, max_tokens=800)\n    # print(result[:300] + \"\\n... (truncated) ...\\n\")\n    \n    print(\"[Run inference to see results]\\n\")\n\nprint(\"\ud83d\udca1 Uncomment inference code to test all cases\")\nprint(\"=\" * 70)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \ud83c\udf89 Congratulations!\n\nYou've successfully:\n- \u2705 Learned about synthetic reasoning dataset generation\n- \u2705 Loaded and configured DeepSeek-R1-Distill-Qwen-1.5B\n- \u2705 Applied LoRA for efficient fine-tuning\n- \u2705 Prepared reasoning data with proper formatting\n- \u2705 Fine-tuned the model with Unsloth\n- \u2705 Evaluated reasoning improvements\n- \u2705 Compared base vs fine-tuned model\n\n## \ud83c\udfaf Key Takeaways\n\n### Synthetic Data Generation\n- **Solves data scarcity** in specialized domains\n- **Customizable** for specific tasks\n- **Cost-effective** compared to manual labeling\n- **Scalable** to thousands of examples\n\n### Reasoning Models\n- **Explicit reasoning chains** (`<think>` tags)\n- **Step-by-step logical thinking**\n- **Better for complex problems**\n- **More interpretable** than standard models\n\n### Efficient Fine-Tuning\n- **Unsloth**: 2x faster, 70% less memory\n- **LoRA**: Train tiny fraction of parameters\n- **4-bit quantization**: Fits on consumer GPUs\n- **Works on single GPU** (16GB+)\n\n### Practical Impact\n- **Smaller model** (1.5B) achieves good results\n- **Domain-specific** fine-tuning improves accuracy\n- **Reasoning chains** make thinking transparent\n- **Production-ready** after 1-2 hours training\n\n## \ud83d\ude80 Next Steps\n\n### Immediate\n1. **Train on your domain**: Generate custom reasoning data\n2. **Experiment with hyperparameters**: Learning rate, LoRA rank, etc.\n3. **Evaluate systematically**: Create test sets with expected outputs\n4. **Deploy**: Convert to GGUF for edge deployment\n\n### Advanced\n1. **Larger models**: Try 7B or 14B variants\n2. **More data**: Generate 1000-5000 examples\n3. **Multi-task**: Combine different reasoning tasks\n4. **RL fine-tuning**: GRPO or DPO for further improvement\n\n### Production\n1. **Quantization**: Convert to 4-bit or 8-bit GGUF\n2. **Optimization**: Use llama.cpp or vLLM for serving\n3. **API deployment**: Wrap in FastAPI or similar\n4. **Monitoring**: Track reasoning quality over time\n\n## \ud83d\udcda Resources\n\n### Papers\n- [DeepSeek-R1 Technical Report](https://arxiv.org/abs/2401.14196)\n- [LoRA Paper](https://arxiv.org/abs/2106.09685)\n- [Chain-of-Thought Prompting](https://arxiv.org/abs/2201.11903)\n\n### Tools\n- [Unsloth](https://github.com/unslothai/unsloth)\n- [Synthetic Data Generator](https://huggingface.co/spaces/argilla/synthetic-data-generator)\n- [TRL Documentation](https://huggingface.co/docs/trl)\n- [DeepSeek Models](https://huggingface.co/deepseek-ai)\n\n### Datasets\n- [Python Reasoning Dataset](https://huggingface.co/datasets/sdiazlor/python-reasoning-dataset)\n- [Code Contests](https://huggingface.co/datasets/deepmind/code_contests)\n- [APPS Dataset](https://huggingface.co/datasets/codeparrot/apps)\n\n## \ud83d\udca1 Tips for Better Results\n\n### Data Generation\n1. **Quality over quantity**: 500 good examples > 5000 poor ones\n2. **Diverse problems**: Cover different difficulty levels\n3. **Clear instructions**: Well-defined system prompts\n4. **Reasoning depth**: Balance detail vs conciseness\n\n### Training\n1. **Learning rate**: 1e-4 to 5e-4 works well for LoRA\n2. **Epochs**: 3-5 for 500 examples, 1-2 for 5000+\n3. **LoRA rank**: 16-32 for small models, 64+ for large\n4. **Sequence length**: 2048 for code, 512-1024 for text\n\n### Evaluation\n1. **Hold-out test set**: Never seen during training\n2. **Code execution**: Test if generated code runs\n3. **Reasoning quality**: Check logic in `<think>` tags\n4. **Human review**: Sample random outputs\n\n## \ud83d\udd27 Troubleshooting\n\n### Out of Memory\n- Use 4-bit quantization (`load_in_4bit=True`)\n- Reduce batch size to 1\n- Enable gradient checkpointing\n- Reduce `max_seq_length` to 1024\n\n### Poor Reasoning Quality\n- Generate more diverse training data\n- Increase LoRA rank (32 or 64)\n- Train for more epochs\n- Check data quality and format\n\n### Slow Inference\n- Use GGUF format with llama.cpp\n- Reduce `max_new_tokens`\n- Use lower precision (INT4, INT8)\n- Batch multiple requests\n\n---\n\n**Happy Fine-Tuning!** \ud83c\udf93\u2728\n\nFor more tutorials, check out:\n- [GPT-2 From Scratch](../01-Full-Fine-Tuning/)\n- [Falcon-7B LoRA](../02-PEFT/)\n- [FLAN-T5 Summarization](../03-Instruction-Tuning/Summarization-FLAN-T5.ipynb)\n- [Financial Sentiment OPT](../03-Instruction-Tuning/Financial-Sentiment-OPT.ipynb)\n\n---\n\n**This completes the LLM-Finetuning-Cookbook!** \ud83c\udf8a\n\nYou've mastered:\n- Full fine-tuning\n- PEFT/LoRA\n- Instruction tuning\n- Summarization\n- Sentiment analysis\n- **Reasoning with synthetic data** \u2b50\n\n**Thank you for learning with us!** \ud83d\ude80"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}