{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tune Qwen3 (14B) for Reasoning & Conversation\n\nThis notebook demonstrates how to fine-tune **Qwen3-14B** to **combine reasoning and conversational capabilities** using **Unsloth** for optimized training.\n\n## \ud83c\udf1f What is Qwen3?\n\n**Qwen3** (Qwen 3.0) is Alibaba's latest open-source LLM family:\n- **Sizes**: 1.7B to 32B parameters\n- **Dual mode**: Reasoning (`<think>` tags) + conversational\n- **High performance**: Competitive with GPT-4 level models\n- **Multilingual**: Strong Chinese and English support\n- **Efficient**: Optimized architecture\n\n## \ud83c\udfaf What You'll Learn\n\n- How to combine **reasoning** and **conversational** datasets\n- How to use **Unsloth** for 2x faster training\n- How to balance reasoning vs chat capabilities\n- How to enable/disable thinking mode at inference\n- How to save models in multiple formats (LoRA, merged, GGUF)\n\n## \ud83d\udca1 Why Mix Reasoning + Conversation?\n\n**Pure reasoning models:**\n- \u2705 Great at complex problems\n- \u274c Verbose for simple questions\n- \u274c Higher inference cost\n\n**Pure chat models:**\n- \u2705 Fast, concise responses\n- \u274c Struggle with complex reasoning\n\n**Combined approach (this notebook):**\n- \u2705 Reason when needed\n- \u2705 Chat normally otherwise\n- \u2705 User controls mode\n- \u2705 Best of both worlds!\n\n## \ud83d\udd27 Requirements\n\n- **GPU**: 16GB+ VRAM (T4, V100, A100)\n- **Time**: 30-60 minutes for quick training\n- **Model**: Qwen3-14B (4-bit quantized)\n- **Library**: Unsloth (2x faster than standard)\n\n## \ud83d\udcca Key Stats\n\n| Metric | Value |\n|--------|-------|\n| Base Model | Qwen3-14B (4-bit) |\n| LoRA Rank | 32 |\n| Reasoning Data | ~10K examples (75%) |\n| Chat Data | ~3K examples (25%) |\n| Training Steps | 30 (demo) / 1000+ (production) |\n| Training Time | ~30 min (demo) / 2-3 hours (full) |\n| GPU Memory | ~12-14GB |\n\n## \ud83d\udcd6 Table of Contents\n\n1. [Installation and Setup](#1-installation-and-setup)\n2. [Load Model with Unsloth](#2-load-model-with-unsloth)\n3. [Prepare Mixed Dataset](#3-prepare-mixed-dataset)\n4. [Train the Model](#4-train-the-model)\n5. [Inference: Thinking vs Non-Thinking](#5-inference-thinking-vs-non-thinking)\n6. [Save in Multiple Formats](#6-save-in-multiple-formats)\n\n---\n\n**Credits**: Based on Unsloth's official Qwen3 reasoning notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Installation and Setup\n\n### Install Unsloth\n\n**Unsloth** provides:\n- \u2705 **2x faster** training than standard methods\n- \u2705 **70% less memory** usage\n- \u2705 **No accuracy loss**\n- \u2705 Support for latest models (Qwen3, Gemma 3, Llama, etc.)\n\n### Installation Options\n\n**For Colab/Kaggle:**\n- Automatic detection and installation\n\n**For local:**\n```bash\npip install unsloth\n```\n\n**For this repository:**\n- Already included in Poetry dependencies!\n```bash\nmake install-reasoning\n```"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Install Unsloth and dependencies\n%%capture\nimport os, re\n\n# Check if in Colab\nif \"COLAB_\" not in \"\".join(os.environ.keys()):\n    !pip install unsloth\nelse:\n    # Colab-specific installation\n    import torch\n    v = re.match(r\"[0-9\\.]{3,}\", str(torch.__version__)).group(0)\n    xformers = \"xformers==\" + (\"0.0.32.post2\" if v == \"2.8.0\" else \"0.0.29.post3\")\n    !pip install --no-deps bitsandbytes accelerate {xformers} peft trl triton cut_cross_entropy unsloth_zoo\n    !pip install sentencepiece protobuf \"datasets>=3.4.1,<4.0.0\" \"huggingface_hub>=0.34.0\" hf_transfer\n    !pip install --no-deps unsloth\n\n# Install specific versions for compatibility\n!pip install transformers==4.55.4\n!pip install --no-deps trl==0.22.2\n\nprint(\"\u2705 All packages installed!\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Model with Unsloth\n\n### Available Qwen3 Models\n\nUnsloth provides optimized 4-bit versions of all Qwen3 models:\n\n- `unsloth/Qwen3-1.7B-unsloth-bnb-4bit` (smallest, fastest)\n- `unsloth/Qwen3-4B-unsloth-bnb-4bit`\n- `unsloth/Qwen3-8B-unsloth-bnb-4bit`\n- `unsloth/Qwen3-14B-unsloth-bnb-4bit` \u2190 **We'll use this**\n- `unsloth/Qwen3-32B-unsloth-bnb-4bit` (largest, best quality)\n\n### Model Configuration\n\n- **max_seq_length**: 2048 tokens (can be longer but uses more memory)\n- **load_in_4bit**: Enable 4-bit quantization\n- **full_finetuning**: False (we'll use LoRA)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Import Unsloth\nfrom unsloth import FastLanguageModel\nimport torch\n\n# List of available Qwen3 models\nfourbit_models = [\n    \"unsloth/Qwen3-1.7B-unsloth-bnb-4bit\",\n    \"unsloth/Qwen3-4B-unsloth-bnb-4bit\",\n    \"unsloth/Qwen3-8B-unsloth-bnb-4bit\",\n    \"unsloth/Qwen3-14B-unsloth-bnb-4bit\",  # We'll use this\n    \"unsloth/Qwen3-32B-unsloth-bnb-4bit\",\n]\n\nprint(\"Available Qwen3 models:\")\nfor model_name in fourbit_models:\n    print(f\"  - {model_name}\")\n\nprint(\"\\nWe'll use: Qwen3-14B for best quality on consumer hardware\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Load Qwen3-14B with Unsloth\nprint(\"Loading Qwen3-14B...\")\nprint(\"This may take a few minutes...\")\n\nmodel, tokenizer = FastLanguageModel.from_pretrained(\n    model_name = \"unsloth/Qwen3-14B\",\n    max_seq_length = 2048,      # Context length\n    load_in_4bit = True,        # 4-bit uses much less memory\n    load_in_8bit = False,       # 8-bit is more accurate but uses 2x memory\n    full_finetuning = False,    # Use LoRA (parameter-efficient)\n    # token = \"hf_...\",         # Use if accessing gated models\n)\n\nprint(\"\\n\u2705 Model and tokenizer loaded successfully!\")\nprint(f\"Model: Qwen3-14B (4-bit quantized)\")\nprint(f\"Max sequence length: 2048 tokens\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add LoRA Adapters\n\nWe'll add **LoRA adapters** to make training efficient.\n\n**Configuration:**\n- **r=32**: LoRA rank (balance of quality and speed)\n- **lora_alpha=32**: Scaling factor (typically equal to rank)\n- **lora_dropout=0**: No dropout (optimized for Unsloth)\n- **Target modules**: All attention and MLP layers\n\n**Unsloth optimization:**\n- `use_gradient_checkpointing=\"unsloth\"`: 30% less VRAM, 2x larger batch sizes!"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Add LoRA adapters\nmodel = FastLanguageModel.get_peft_model(\n    model,\n    r = 32,  # LoRA rank - higher = more capacity\n    target_modules = [\n        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",  # Attention\n        \"gate_proj\", \"up_proj\", \"down_proj\",      # MLP\n    ],\n    lora_alpha = 32,  # Scaling factor\n    lora_dropout = 0,  # 0 is optimized for Unsloth\n    bias = \"none\",  # \"none\" is optimized\n    use_gradient_checkpointing = \"unsloth\",  # [NEW] 30% less VRAM!\n    random_state = 3407,\n    use_rslora = False,  # Rank-stabilized LoRA\n    loftq_config = None,  # LoftQ\n)\n\nprint(\"\u2705 LoRA adapters added!\")\nprint(f\"\\nConfiguration:\")\nprint(f\"  Rank: 32\")\nprint(f\"  Alpha: 32\")\nprint(f\"  Dropout: 0 (optimized)\")\nprint(f\"  Gradient checkpointing: unsloth (optimized)\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Prepare Mixed Dataset\n\n### Dual Dataset Approach\n\nWe'll combine TWO types of data:\n\n1. **Reasoning Dataset** (75%)\n   - OpenMathReasoning dataset\n   - Mathematical problems with detailed reasoning\n   - Uses `<think>` tags for step-by-step solving\n   - Teaches the model HOW to reason\n\n2. **Conversational Dataset** (25%)\n   - FineTome-100k dataset\n   - General conversations\n   - Normal chat without reasoning overhead\n   - Teaches the model WHEN to reason\n\n### Why Mix Both?\n\n- \u2705 **Balanced model**: Reason when needed, chat normally otherwise\n- \u2705 **Efficiency**: Don't waste compute on simple questions\n- \u2705 **Flexibility**: User controls thinking mode\n- \u2705 **Practical**: Real-world usage pattern\n\n### Dataset Ratio\n\nWe'll use **75% reasoning, 25% chat**:\n- Maintains reasoning capabilities\n- Adds conversational fluency\n- Adjustable based on your needs"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Load both datasets\nfrom datasets import load_dataset\n\nprint(\"Loading datasets...\")\nprint(\"This may take a moment...\")\n\n# 1. Reasoning dataset (math problems with CoT)\nreasoning_dataset = load_dataset(\n    \"unsloth/OpenMathReasoning-mini\",\n    split=\"cot\"  # Chain-of-thought split\n)\n\n# 2. Conversational dataset (general chat)\nnon_reasoning_dataset = load_dataset(\n    \"mlabonne/FineTome-100k\",\n    split=\"train\"\n)\n\nprint(\"\\n\u2705 Both datasets loaded successfully!\")\nprint(f\"\\nReasoning dataset: {len(reasoning_dataset):,} examples\")\nprint(f\"Conversational dataset: {len(non_reasoning_dataset):,} examples\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Explore reasoning dataset structure\nprint(\"=== Reasoning Dataset ===\")\nprint(reasoning_dataset)\nprint(f\"\\nColumns: {reasoning_dataset.column_names}\")\nprint(f\"\\nSample:\")\nprint(f\"  Problem: {reasoning_dataset[0]['problem'][:100]}...\")\nprint(f\"  Solution length: {len(reasoning_dataset[0]['generated_solution'])} chars\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Explore conversational dataset structure\nprint(\"=== Conversational Dataset ===\")\nprint(non_reasoning_dataset)\nprint(f\"\\nColumns: {non_reasoning_dataset.column_names}\")\nprint(f\"\\nSample:\")\nif 'conversations' in non_reasoning_dataset.column_names:\n    print(f\"  Conversations: {non_reasoning_dataset[0]['conversations'][:2]}\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert Reasoning Dataset to Conversational Format\n\nWe need to convert the problem-solution pairs into a conversational format:\n\n```python\n[\n  {\"role\": \"user\", \"content\": \"Solve (x + 2)^2 = 0.\"},\n  {\"role\": \"assistant\", \"content\": \"<think>...</think> x = -2\"}\n]\n```"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Function to convert reasoning data to conversations\ndef generate_conversation(examples):\n    \"\"\"\n    Convert problem-solution pairs to conversational format.\n    \n    Args:\n        examples: Batch of examples with 'problem' and 'generated_solution'\n        \n    Returns:\n        Dictionary with conversations\n    \"\"\"\n    problems = examples[\"problem\"]\n    solutions = examples[\"generated_solution\"]\n    conversations = []\n    \n    for problem, solution in zip(problems, solutions):\n        conversations.append([\n            {\"role\": \"user\", \"content\": problem},\n            {\"role\": \"assistant\", \"content\": solution},\n        ])\n    \n    return {\"conversations\": conversations}\n\nprint(\"\u2705 Conversion function defined!\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Convert reasoning dataset to conversations\nprint(\"Converting reasoning dataset to conversation format...\")\n\nreasoning_conversations = reasoning_dataset.map(\n    generate_conversation,\n    batched=True\n)[\"conversations\"]\n\nprint(f\"\\n\u2705 Converted {len(reasoning_conversations):,} reasoning conversations!\")\nprint(f\"\\nSample conversation:\")\nprint(reasoning_conversations[0])"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Apply Qwen3 chat template to reasoning data\nreasoning_conversations_formatted = tokenizer.apply_chat_template(\n    reasoning_conversations,\n    tokenize=False,\n)\n\nprint(\"\u2705 Chat template applied to reasoning data!\")\nprint(f\"\\nFormatted examples: {len(reasoning_conversations_formatted):,}\")\nprint(f\"\\nFirst example (first 500 chars):\")\nprint(reasoning_conversations_formatted[0][:500])"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process Conversational Dataset\n\nThe conversational dataset is in ShareGPT format, which we need to standardize first."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Standardize ShareGPT format\nfrom unsloth.chat_templates import standardize_sharegpt\n\nprint(\"Standardizing conversational dataset...\")\ndataset_standardized = standardize_sharegpt(non_reasoning_dataset)\n\nprint(\"\\n\u2705 Dataset standardized!\")\nprint(f\"Conversations: {len(dataset_standardized):,}\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Apply chat template to conversational data\nnon_reasoning_conversations = tokenizer.apply_chat_template(\n    dataset_standardized[\"conversations\"],\n    tokenize=False,\n)\n\nprint(\"\u2705 Chat template applied to conversational data!\")\nprint(f\"\\nFormatted examples: {len(non_reasoning_conversations):,}\")\nprint(f\"\\nFirst example (first 300 chars):\")\nprint(non_reasoning_conversations[0][:300])"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Check dataset sizes\nprint(\"=== Dataset Sizes ===\")\nprint(f\"Reasoning conversations: {len(reasoning_conversations_formatted):,}\")\nprint(f\"Chat conversations: {len(non_reasoning_conversations):,}\")\nprint(f\"\\nRatio: {len(non_reasoning_conversations) / len(reasoning_conversations_formatted):.2f}:1 (chat:reasoning)\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mix Datasets with Custom Ratio\n\nWe'll create a balanced dataset with:\n- **75% reasoning** (for problem-solving skills)\n- **25% conversational** (for chat fluency)\n\nThis ratio can be adjusted based on your needs:\n- **More reasoning**: Better at complex problems, more verbose\n- **More chat**: Better at normal conversation, less verbose\n\n**Formula:**\n```python\nchat_samples = reasoning_samples \u00d7 (chat_% / reasoning_%)\n```"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Define the chat percentage (reasoning will be 1 - chat_percentage)\nchat_percentage = 0.25  # 25% chat, 75% reasoning\n\nprint(f\"Target mix:\")\nprint(f\"  Reasoning: {(1 - chat_percentage) * 100:.0f}%\")\nprint(f\"  Chat: {chat_percentage * 100:.0f}%\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Sample the conversational dataset to match desired ratio\nimport pandas as pd\n\n# Convert to pandas for easy sampling\nnon_reasoning_subset = pd.Series(non_reasoning_conversations)\n\n# Calculate how many chat examples we need\nn_chat_samples = int(len(reasoning_conversations_formatted) * (chat_percentage / (1 - chat_percentage)))\n\n# Sample\nnon_reasoning_subset = non_reasoning_subset.sample(\n    n_chat_samples,\n    random_state=2407,\n)\n\nprint(f\"\\n=== Sampled Dataset ===\")\nprint(f\"Reasoning examples: {len(reasoning_conversations_formatted):,}\")\nprint(f\"Chat examples: {len(non_reasoning_subset):,}\")\nprint(f\"\\nActual chat percentage: {len(non_reasoning_subset) / (len(non_reasoning_subset) + len(reasoning_conversations_formatted)) * 100:.1f}%\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Combine both datasets\ndata = pd.concat([\n    pd.Series(reasoning_conversations_formatted),\n    pd.Series(non_reasoning_subset)\n])\ndata.name = \"text\"\n\n# Convert to HuggingFace Dataset\nfrom datasets import Dataset\ncombined_dataset = Dataset.from_pandas(pd.DataFrame(data))\n\n# Shuffle for better training\ncombined_dataset = combined_dataset.shuffle(seed=3407)\n\nprint(\"\u2705 Combined dataset created!\")\nprint(f\"\\nTotal examples: {len(combined_dataset):,}\")\nprint(f\"  Reasoning: {len(reasoning_conversations_formatted):,} ({(1-chat_percentage)*100:.0f}%)\")\nprint(f\"  Chat: {len(non_reasoning_subset):,} ({chat_percentage*100:.0f}%)\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Inspect the combined dataset\nprint(\"=== Combined Dataset Sample ===\\n\")\nprint(combined_dataset[0][:600])\nprint(\"\\n... (truncated) ...\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Train the Model\n\n### Configure Training with SFTTrainer\n\nWe'll use TRL's **SFTTrainer** with optimized settings.\n\n**Training Configuration:**\n- **Batch size**: 2 per device\n- **Gradient accumulation**: 4 steps (effective batch size = 8)\n- **Learning rate**: 2e-4 (standard for LoRA)\n- **Optimizer**: AdamW 8-bit (memory efficient)\n- **Steps**: 30 (demo) / 1000+ (production)\n\n**For production:**\n- Set `num_train_epochs=1` and `max_steps=None`\n- Increase batch size if you have more memory\n- Add evaluation dataset for monitoring"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Initialize SFTTrainer\nfrom trl import SFTTrainer, SFTConfig\n\ntrainer = SFTTrainer(\n    model=model,\n    tokenizer=tokenizer,\n    train_dataset=combined_dataset,\n    eval_dataset=None,  # Add validation set for monitoring\n    args=SFTConfig(\n        dataset_text_field=\"text\",\n        per_device_train_batch_size=2,\n        gradient_accumulation_steps=4,  # Effective batch size = 8\n        warmup_steps=5,\n        # num_train_epochs=1,  # Uncomment for full training\n        max_steps=30,  # Quick demo - increase to 1000+ for real training\n        learning_rate=2e-4,  # Reduce to 2e-5 for longer runs\n        logging_steps=1,\n        optim=\"adamw_8bit\",\n        weight_decay=0.01,\n        lr_scheduler_type=\"linear\",\n        seed=3407,\n        report_to=\"none\",  # Set to \"wandb\" for experiment tracking\n        output_dir=\"output\",\n    ),\n)\n\nprint(\"\u2705 SFTTrainer initialized!\")\nprint(f\"\\nTraining configuration:\")\nprint(f\"  Batch size: 2 (per device)\")\nprint(f\"  Gradient accumulation: 4\")\nprint(f\"  Effective batch size: 8\")\nprint(f\"  Max steps: 30 (demo)\")\nprint(f\"  Learning rate: 2e-4\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Check current memory stats\ngpu_stats = torch.cuda.get_device_properties(0)\nstart_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\nmax_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n\nprint(f\"GPU: {gpu_stats.name}\")\nprint(f\"Max memory: {max_memory} GB\")\nprint(f\"Reserved memory: {start_gpu_memory} GB\")\nprint(f\"Available: {max_memory - start_gpu_memory} GB\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Start training!\nprint(\"=\" * 70)\nprint(\"STARTING TRAINING\")\nprint(\"=\" * 70)\nprint()\nprint(f\"Training on {len(combined_dataset):,} mixed examples\")\nprint(f\"  - Reasoning: ~{len(reasoning_conversations_formatted):,}\")\nprint(f\"  - Chat: ~{len(non_reasoning_subset):,}\")\nprint(f\"Running for {trainer.args.max_steps} steps (demo)\")\nprint()\nprint(\"For production:\")\nprint(\"  - Set num_train_epochs=1, max_steps=None\")\nprint(\"  - Expected time: 2-3 hours\")\nprint()\nprint(\"=\" * 70)\nprint()\n\n# Train the model\n# Uncomment to start training\n# trainer_stats = trainer.train()\n\nprint(\"\u26a0\ufe0f Training is commented out by default.\")\nprint(\"Uncomment 'trainer_stats = trainer.train()' to start training.\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Show final memory and time stats (after training)\n# Uncomment when training is complete\n\n# used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n# used_memory_for_lora = round(used_memory - start_gpu_memory, 3)\n# used_percentage = round(used_memory / max_memory * 100, 3)\n# lora_percentage = round(used_memory_for_lora / max_memory * 100, 3)\n\n# print(f\"{trainer_stats.metrics['train_runtime']} seconds used for training.\")\n# print(f\"{round(trainer_stats.metrics['train_runtime']/60, 2)} minutes used for training.\")\n# print(f\"Peak reserved memory = {used_memory} GB.\")\n# print(f\"Peak reserved memory for training = {used_memory_for_lora} GB.\")\n# print(f\"Peak reserved memory % of max memory = {used_percentage}%.\")\n# print(f\"Peak reserved memory for training % of max memory = {lora_percentage}%.\")\n\nprint(\"\u26a0\ufe0f Uncomment after training to see memory stats\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Inference: Thinking vs Non-Thinking\n\n### Qwen3's Dual Modes\n\nQwen3 has a unique feature: **controllable thinking**!\n\n**Non-Thinking Mode** (Fast):\n- `enable_thinking=False` in chat template\n- `temperature=0.7, top_p=0.8, top_k=20`\n- Quick, concise responses\n- Best for: Simple questions, chat, Q&A\n\n**Thinking Mode** (Detailed):\n- `enable_thinking=True` in chat template\n- `temperature=0.6, top_p=0.95, top_k=20`\n- Detailed reasoning in `<think>` tags\n- Best for: Math, logic, complex problems\n\n### When to Use Each\n\n| Scenario | Mode | Why |\n|----------|------|-----|\n| \"What is 2+2?\" | Non-thinking | Simple calculation |\n| \"Solve (x+2)^2=0\" | Thinking | Needs steps |\n| \"Hi, how are you?\" | Non-thinking | Simple chat |\n| \"Explain quantum physics\" | Thinking | Complex topic |"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Test 1: Non-Thinking Mode (Fast Chat)\nprint(\"=\" * 70)\nprint(\"TEST 1: NON-THINKING MODE (Fast Chat)\")\nprint(\"=\" * 70)\nprint()\n\nmessages = [\n    {\"role\": \"user\", \"content\": \"Solve (x + 2)^2 = 0.\"}\n]\n\n# Apply chat template WITHOUT thinking\ntext = tokenizer.apply_chat_template(\n    messages,\n    tokenize=False,\n    add_generation_prompt=True,  # Must add for generation\n    enable_thinking=False,  # Disable thinking for fast response\n)\n\nprint(\"Prompt (without thinking enabled):\")\nprint(text)\nprint(\"\\n\" + \"=\" * 70)\nprint(\"Response:\")\nprint()\n\nfrom transformers import TextStreamer\n_ = model.generate(\n    **tokenizer(text, return_tensors=\"pt\").to(\"cuda\"),\n    max_new_tokens=256,\n    temperature=0.7, top_p=0.8, top_k=20,  # Settings for non-thinking\n    streamer=TextStreamer(tokenizer, skip_prompt=True),\n)\n\nprint(\"\\n\" + \"=\" * 70)\nprint(\"\ud83d\udca1 Notice: Concise answer without detailed reasoning\")\nprint(\"=\" * 70)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Test 2: Thinking Mode (Detailed Reasoning)\nprint(\"\\n\" + \"=\" * 70)\nprint(\"TEST 2: THINKING MODE (Detailed Reasoning)\")\nprint(\"=\" * 70)\nprint()\n\nmessages = [\n    {\"role\": \"user\", \"content\": \"Solve (x + 2)^2 = 0.\"}\n]\n\n# Apply chat template WITH thinking\ntext = tokenizer.apply_chat_template(\n    messages,\n    tokenize=False,\n    add_generation_prompt=True,\n    enable_thinking=True,  # Enable thinking for detailed reasoning\n)\n\nprint(\"Prompt (with thinking enabled):\")\nprint(text)\nprint(\"\\n\" + \"=\" * 70)\nprint(\"Response:\")\nprint()\n\n_ = model.generate(\n    **tokenizer(text, return_tensors=\"pt\").to(\"cuda\"),\n    max_new_tokens=1024,  # More tokens for reasoning\n    temperature=0.6, top_p=0.95, top_k=20,  # Settings for thinking\n    streamer=TextStreamer(tokenizer, skip_prompt=True),\n)\n\nprint(\"\\n\" + \"=\" * 70)\nprint(\"\ud83d\udca1 Notice: Detailed reasoning in <think> tags before answer\")\nprint(\"=\" * 70)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test with Various Questions\n\nLet's test both modes with different types of questions to see when each mode shines."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Define test cases\ntest_cases = [\n    {\n        \"question\": \"What is 15% of 80?\",\n        \"best_mode\": \"Non-thinking (simple calculation)\"\n    },\n    {\n        \"question\": \"If I invest $1000 at 5% annual interest compounded monthly for 3 years, how much will I have?\",\n        \"best_mode\": \"Thinking (multi-step calculation)\"\n    },\n    {\n        \"question\": \"Hello! How are you today?\",\n        \"best_mode\": \"Non-thinking (simple chat)\"\n    },\n    {\n        \"question\": \"Explain the concept of present value in finance and show me an example calculation.\",\n        \"best_mode\": \"Thinking (complex explanation + calculation)\"\n    }\n]\n\nprint(\"=== Test Cases ===\\n\")\nfor i, case in enumerate(test_cases, 1):\n    print(f\"{i}. Question: {case['question']}\")\n    print(f\"   Best mode: {case['best_mode']}\")\n    print()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Helper function to test both modes\ndef test_both_modes(question, max_tokens_non_thinking=256, max_tokens_thinking=1024):\n    \"\"\"\n    Test a question with both thinking and non-thinking modes.\n    \n    Args:\n        question: The question to ask\n        max_tokens_non_thinking: Max tokens for non-thinking mode\n        max_tokens_thinking: Max tokens for thinking mode\n    \"\"\"\n    messages = [{\"role\": \"user\", \"content\": question}]\n    \n    # Non-thinking mode\n    print(\"\\n\" + \"=\" * 70)\n    print(\"NON-THINKING MODE:\")\n    print(\"=\" * 70)\n    text = tokenizer.apply_chat_template(\n        messages, tokenize=False, add_generation_prompt=True, enable_thinking=False\n    )\n    _ = model.generate(\n        **tokenizer(text, return_tensors=\"pt\").to(\"cuda\"),\n        max_new_tokens=max_tokens_non_thinking,\n        temperature=0.7, top_p=0.8, top_k=20,\n        streamer=TextStreamer(tokenizer, skip_prompt=True),\n    )\n    \n    # Thinking mode\n    print(\"\\n\" + \"=\" * 70)\n    print(\"THINKING MODE:\")\n    print(\"=\" * 70)\n    text = tokenizer.apply_chat_template(\n        messages, tokenize=False, add_generation_prompt=True, enable_thinking=True\n    )\n    _ = model.generate(\n        **tokenizer(text, return_tensors=\"pt\").to(\"cuda\"),\n        max_new_tokens=max_tokens_thinking,\n        temperature=0.6, top_p=0.95, top_k=20,\n        streamer=TextStreamer(tokenizer, skip_prompt=True),\n    )\n    print(\"=\" * 70)\n\nprint(\"\u2705 Testing function defined!\")\nprint(\"\\nUse this to compare both modes on any question.\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Save in Multiple Formats\n\nUnsloth supports saving your model in **multiple formats** for different use cases:\n\n### Saving Options\n\n| Format | Size | Use Case | Load With |\n|--------|------|----------|-----------|\n| **LoRA adapters** | ~100MB | Sharing, experimentation | PEFT library |\n| **Merged 16-bit** | ~28GB | Production (GPU) | Transformers |\n| **Merged 4-bit** | ~7GB | Production (limited GPU) | Transformers + BitsAndBytes |\n| **GGUF (q8_0)** | ~14GB | llama.cpp, Ollama | llama.cpp |\n| **GGUF (q4_k_m)** | ~8GB | Edge deployment | llama.cpp |\n\n### Recommended Workflow\n\n1. **Development**: Save LoRA adapters only\n2. **Testing**: Merge to 16-bit for quality check\n3. **Deployment**: Convert to GGUF for efficiency"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Option 1: Save LoRA Adapters Only (Smallest)\n# This saves only the trained adapter weights (~100MB)\n\nlocal_path = \"lora_model\"\nhub_path = \"your-username/qwen3-14b-reasoning-chat-lora\"  # Update username\n\nprint(f\"Saving LoRA adapters...\")\n\n# Local save\nmodel.save_pretrained(local_path)\ntokenizer.save_pretrained(local_path)\nprint(f\"\\n\u2705 Saved locally to: {local_path}\")\n\n# Hub save (uncomment when ready)\n# model.push_to_hub(hub_path, token=\"...\")\n# tokenizer.push_to_hub(hub_path, token=\"...\")\n# print(f\"\u2705 Pushed to Hub: {hub_path}\")\n\nprint(\"\\n\ud83d\udca1 LoRA adapters are small (~100MB) and easy to share\")\nprint(\"   Load with: PeftModel.from_pretrained(base_model, 'lora_model')\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Load the LoRA adapters (for future use)\n# Set to True when you have a saved model\n\nif False:\n    from unsloth import FastLanguageModel\n    \n    model, tokenizer = FastLanguageModel.from_pretrained(\n        model_name=\"lora_model\",  # Your saved model path\n        max_seq_length=2048,\n        load_in_4bit=True,\n    )\n    \n    print(\"\u2705 Model loaded from saved LoRA adapters!\")\n\nprint(\"\ud83d\udca1 Set if False to if True above to load saved model\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Merged Models\n\nFor production deployment, you may want to merge LoRA adapters with the base model.\n\n**Merged 16-bit:**\n- Full precision\n- Best quality\n- Large file size (~28GB)\n- For GPU deployment\n\n**Merged 4-bit:**\n- Quantized\n- Good quality\n- Smaller size (~7GB)\n- For limited GPU memory"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Option 2: Save Merged 16-bit (Full Precision)\nif False:\n    model.save_pretrained_merged(\"model\", tokenizer, save_method=\"merged_16bit\")\n    print(\"\u2705 Saved as merged 16-bit model\")\n\n# Push to Hub\nif False:\n    model.push_to_hub_merged(\"hf/model\", tokenizer, save_method=\"merged_16bit\", token=\"\")\n    print(\"\u2705 Pushed merged 16-bit to Hub\")\n\n# Option 3: Save Merged 4-bit (Quantized)\nif False:\n    model.save_pretrained_merged(\"model\", tokenizer, save_method=\"merged_4bit\")\n    print(\"\u2705 Saved as merged 4-bit model\")\n\n# Push to Hub\nif False:\n    model.push_to_hub_merged(\"hf/model\", tokenizer, save_method=\"merged_4bit\", token=\"\")\n    print(\"\u2705 Pushed merged 4-bit to Hub\")\n\n# Option 4: Save LoRA Only\nif False:\n    model.save_pretrained(\"model\")\n    tokenizer.save_pretrained(\"model\")\n    print(\"\u2705 Saved LoRA adapters\")\n\n# Push LoRA to Hub\nif False:\n    model.push_to_hub(\"hf/model\", token=\"\")\n    tokenizer.push_to_hub(\"hf/model\", token=\"\")\n    print(\"\u2705 Pushed LoRA to Hub\")\n\nprint(\"\ud83d\udca1 Uncomment the option you want to use\")\nprint(\"\\nRecommended:\")\nprint(\"  - Development: LoRA adapters\")\nprint(\"  - Production (GPU): Merged 16-bit\")\nprint(\"  - Production (limited GPU): Merged 4-bit\")\nprint(\"  - Edge devices: GGUF (see next section)\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save to GGUF for llama.cpp\n\n**GGUF** (GPT-Generated Unified Format) is for:\n- **llama.cpp**: Fast CPU/GPU inference\n- **Ollama**: Local model serving\n- **LM Studio**: Desktop app\n- **Jan**: Desktop app\n\n### GGUF Quantization Methods\n\n| Method | Size | Quality | Speed | Use Case |\n|--------|------|---------|-------|----------|\n| **q8_0** | ~14GB | Best | Slower | GPU with memory |\n| **q5_k_m** | ~10GB | Very good | Fast | Recommended |\n| **q4_k_m** | ~8GB | Good | Faster | Most common |\n| **f16** | ~28GB | Perfect | Slower | Quality check |\n\n**Recommended**: `q4_k_m` for best balance of size/quality/speed"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Option 5: Save to GGUF (for llama.cpp, Ollama, etc.)\n\n# Save to 8-bit Q8_0\nif False:\n    model.save_pretrained_gguf(\"model\", tokenizer)\n    print(\"\u2705 Saved as GGUF (q8_0)\")\n\n# Push to Hub\nif False:\n    model.push_to_hub_gguf(\"hf/model\", tokenizer, token=\"\")\n    print(\"\u2705 Pushed GGUF (q8_0) to Hub\")\n\n# Save to 16-bit GGUF (highest quality)\nif False:\n    model.save_pretrained_gguf(\"model\", tokenizer, quantization_method=\"f16\")\n    print(\"\u2705 Saved as GGUF (f16)\")\n\n# Save to q4_k_m GGUF (recommended)\nif False:\n    model.save_pretrained_gguf(\"model\", tokenizer, quantization_method=\"q4_k_m\")\n    print(\"\u2705 Saved as GGUF (q4_k_m)\")\n\n# Push multiple GGUF formats at once (much faster!)\nif False:\n    model.push_to_hub_gguf(\n        \"hf/model\",  # Update with your username!\n        tokenizer,\n        quantization_method=[\"q4_k_m\", \"q8_0\", \"q5_k_m\"],\n        token=\"\",  # Get token at https://huggingface.co/settings/tokens\n    )\n    print(\"\u2705 Pushed multiple GGUF formats to Hub!\")\n\nprint(\"\ud83d\udca1 Uncomment to save in GGUF format\")\nprint(\"\\nGGUF files can be used with:\")\nprint(\"  - llama.cpp (fast CPU/GPU inference)\")\nprint(\"  - Ollama (local serving)\")\nprint(\"  - LM Studio (desktop app)\")\nprint(\"  - Jan (desktop app)\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \ud83c\udf89 Congratulations!\n\nYou've successfully:\n- \u2705 Loaded Qwen3-14B with Unsloth (2x faster!)\n- \u2705 Mixed reasoning and conversational datasets (75/25)\n- \u2705 Trained a model that can both reason and chat\n- \u2705 Tested thinking vs non-thinking modes\n- \u2705 Learned to save in multiple formats (LoRA, merged, GGUF)\n\n## \ud83c\udfaf Key Achievements\n\n### Dual-Mode Model\n- **Thinking mode**: Detailed reasoning for complex problems\n- **Non-thinking mode**: Fast, concise responses for simple questions\n- **User control**: Choose mode at inference time\n- **Best of both**: Efficiency when needed, depth when required\n\n### Efficient Training\n- **Unsloth**: 2x faster than standard training\n- **70% less memory**: Fits on consumer GPUs\n- **4-bit quantization**: 14B model on 16GB GPU\n- **LoRA**: Train only ~1-2% of parameters\n\n### Practical Deployment\n- **Multiple formats**: LoRA, merged, GGUF\n- **Flexible deployment**: GPU, CPU, edge devices\n- **Production-ready**: Save and share easily\n- **Tool compatibility**: Works with llama.cpp, Ollama, etc.\n\n## \ud83d\udcca Performance Summary\n\n| Metric | Value |\n|--------|-------|\n| **Training speedup** | 2x faster (Unsloth) |\n| **Memory savings** | 70% less VRAM |\n| **Model size** | 14B parameters (4-bit) |\n| **Trainable params** | ~1-2% (LoRA) |\n| **Training time** | ~30 min (demo) / 2-3 hours (full) |\n| **Inference modes** | Thinking + Non-thinking |\n\n## \ud83d\ude80 Next Steps\n\n### Immediate\n1. **Train longer**: Set `num_train_epochs=1` for full training\n2. **Adjust ratio**: Try different reasoning/chat percentages\n3. **Evaluate**: Test on math benchmarks (GSM8K, MATH)\n4. **Deploy**: Convert to GGUF and use with Ollama\n\n### Advanced\n1. **Larger model**: Try Qwen3-32B for better quality\n2. **More data**: Add domain-specific datasets\n3. **Custom datasets**: Create your own reasoning data\n4. **Multi-GPU**: Scale training with DeepSpeed\n\n### Production\n1. **Quantize**: Use q4_k_m GGUF for deployment\n2. **Serve**: Deploy with vLLM or llama.cpp\n3. **API**: Wrap in FastAPI with mode selection\n4. **Monitor**: Track thinking vs non-thinking usage\n\n## \ud83d\udcda Resources\n\n### Documentation\n- [Unsloth Docs](https://docs.unsloth.ai/)\n- [Qwen3 Model Card](https://huggingface.co/Qwen/Qwen3-14B)\n- [TRL Documentation](https://huggingface.co/docs/trl)\n- [llama.cpp](https://github.com/ggerganov/llama.cpp)\n\n### Datasets\n- [OpenMathReasoning](https://huggingface.co/datasets/unsloth/OpenMathReasoning-mini)\n- [FineTome-100k](https://huggingface.co/datasets/mlabonne/FineTome-100k)\n- [GSM8K](https://huggingface.co/datasets/gsm8k) (math evaluation)\n\n### Related Notebooks\n- [Qwen3 Full Docs](https://qwenlm.github.io/blog/qwen3/)\n- [Unsloth Notebooks](https://docs.unsloth.ai/get-started/unsloth-notebooks)\n- [GGUF Quantization Guide](https://github.com/unslothai/unsloth/wiki#gguf-quantization-options)\n\n## \ud83d\udca1 Tips for Better Results\n\n### Dataset Mixing\n1. **Math-heavy use case**: 80-90% reasoning, 10-20% chat\n2. **Balanced use case**: 50-50 split\n3. **Chat-heavy use case**: 20-30% reasoning, 70-80% chat\n\n### Training\n1. **Full epochs**: For best quality, train 1-2 full epochs\n2. **Learning rate**: Start with 2e-4, reduce to 2e-5 for long runs\n3. **Batch size**: Increase if you have more memory\n4. **LoRA rank**: Try 64 or 128 for larger models\n\n### Inference\n1. **Simple questions**: Use non-thinking mode (faster, cheaper)\n2. **Complex problems**: Use thinking mode (better quality)\n3. **Temperature**: Lower (0.6) for thinking, higher (0.7) for chat\n4. **Max tokens**: More (1024+) for thinking, less (256) for chat\n\n## \ud83d\udd27 Troubleshooting\n\n### Out of Memory\n- Reduce `per_device_train_batch_size` to 1\n- Increase `gradient_accumulation_steps` to 8\n- Reduce `max_seq_length` to 1024\n- Use smaller model (Qwen3-8B or 4B)\n\n### Poor Reasoning Quality\n- Increase reasoning dataset percentage (80-90%)\n- Train for more steps/epochs\n- Increase LoRA rank to 64\n- Use larger model (Qwen3-32B)\n\n### Slow Training\n- Already using Unsloth (2x faster!)\n- Increase batch size if memory allows\n- Use gradient checkpointing: `\"unsloth\"`\n- Ensure you're on GPU\n\n### Model Doesn't Use Thinking\n- Ensure `enable_thinking=True` in chat template\n- Check training data has `<think>` tags\n- Train on more reasoning examples\n- Use thinking-specific temperature settings\n\n---\n\n**Happy Fine-Tuning!** \ud83c\udf93\u2728\n\nFor more tutorials, check out:\n- [DeepSeek-R1 (Synthetic Data)](./Math-Reasoning-Qwen-GRPO.ipynb)\n- [Gemma 3 (Financial Q&A)](./Financial-Reasoning-Gemma-3.ipynb)\n- [GPT-2 From Scratch](../01-Full-Fine-Tuning/)\n- [Falcon-7B LoRA](../02-PEFT/)\n- [FLAN-T5 Summarization](../03-Instruction-Tuning/Summarization-FLAN-T5.ipynb)\n\n---\n\n## \ud83c\udf1f Special Features of This Notebook\n\n### Unique Capabilities\n\n1. **Dual-Mode Training** \ud83d\udd00\n   - Mix reasoning and chat data\n   - Adjustable ratios\n   - Single model, multiple capabilities\n\n2. **Unsloth Optimization** \u26a1\n   - 2x faster training\n   - 70% less memory\n   - No quality loss\n\n3. **Controllable Thinking** \ud83e\udde0\n   - Enable/disable at inference\n   - Recommended settings per mode\n   - Flexibility for different use cases\n\n4. **Multiple Export Formats** \ud83d\udcbe\n   - LoRA adapters\n   - Merged models\n   - GGUF for llama.cpp\n   - Single command for multiple formats\n\n---\n\n## \ud83c\udf8a You've Completed the Reasoning Trilogy!\n\nThis is the **3rd reasoning notebook** in this repository:\n\n1. **DeepSeek-R1**: Synthetic reasoning data + Unsloth + code generation\n2. **Gemma 3**: Financial reasoning with expert datasets\n3. **Qwen3**: Mixed reasoning + chat with controllable thinking \u2b50\n\n**Each teaches different aspects of reasoning fine-tuning!**\n\n---\n\n**Thank you for completing this tutorial!** \ud83d\ude4f\n\n**You now have a powerful model that can reason deeply AND chat naturally!** \ud83d\ude80"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}