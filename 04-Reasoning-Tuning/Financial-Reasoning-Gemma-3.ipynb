{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-Tune Gemma 3 for Financial Q&A: Step-by-Step Guide\n\nThis notebook demonstrates how to fine-tune **Google's Gemma 3** model on a **financial reasoning dataset** to improve accuracy and reasoning capabilities.\n\n## \ud83c\udf1f What is Gemma 3?\n\n**Gemma 3** is Google's latest open-source model family, built on Gemini 2.0 technology:\n- **Sizes**: 1B to 27B parameters (we'll use 4B)\n- **Performance**: Rivals proprietary models (Llama3-405B, DeepSeek-V3)\n- **Multimodal**: Text, image, and video reasoning\n- **Multilingual**: 140+ languages supported\n- **Long context**: 128K token window\n- **Quantized**: Official 4-bit and 8-bit versions\n\n## \ud83c\udfaf What You'll Learn\n\n- How to fine-tune Gemma 3 for financial reasoning\n- How to work with reasoning chains (`<think>` tags)\n- How to use the TRL library with Gemma 3\n- How to evaluate financial Q&A models\n- How to save and share models on Hugging Face\n\n## \ud83d\udcbc Use Case: Financial Reasoning\n\n**Why this matters:**\n- Financial decisions require complex reasoning\n- Multi-step calculations need clear logic\n- Transparent thinking builds trust\n- Domain-specific accuracy is critical\n\n## \ud83d\udd27 Requirements\n\n- **GPU**: 16GB+ VRAM (dual GPU setup recommended)\n- **Time**: 1-2 hours for 1 epoch on 500 samples\n- **Dataset**: FinQA with reasoning paths (500 examples)\n- **Model**: Gemma 3 4B Instruct\n\n## \ud83d\udcca Key Stats\n\n| Metric | Value |\n|--------|-------|\n| Base Model | Gemma 3 4B IT |\n| LoRA Rank | 64 |\n| Training Examples | 500 |\n| Training Epochs | 1 |\n| Training Time | ~1.5 hours |\n| Improved Accuracy | Significant |\n\n## \ud83d\udcd6 Table of Contents\n\n1. [Setup Environment](#1-setup-environment)\n2. [Load Model and Tokenizer](#2-load-model-and-tokenizer)\n3. [Load and Process Dataset](#3-load-and-process-dataset)\n4. [Test Before Fine-tuning](#4-test-before-fine-tuning)\n5. [Configure and Train](#5-configure-and-train)\n6. [Test After Fine-tuning](#6-test-after-fine-tuning)\n7. [Save and Share Model](#7-save-and-share-model)\n\n---\n\n**Credits**: Based on DataCamp tutorial and research on Gemma 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup Environment\n\n### Install Required Packages\n\nWe'll install the latest versions of key libraries:\n\n- **transformers**: Latest version with Gemma 3 support\n- **datasets**: For loading datasets\n- **accelerate**: For optimized training\n- **peft**: For LoRA implementation\n- **trl**: For SFTTrainer\n- **bitsandbytes**: For quantization\n\n**Important**: We need the latest transformers version for Gemma 3 support!"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Install required packages\n%%capture\n!pip install -U datasets \n!pip install -U accelerate \n!pip install -U peft \n!pip install -U trl \n!pip install -U bitsandbytes\n!pip install git+https://github.com/huggingface/transformers@v4.49.0-Gemma-3\n\nprint(\"\u2705 All packages installed successfully!\")\nprint(\"   Note: Using latest transformers version for Gemma 3 support\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Import necessary libraries\nimport torch\nfrom transformers import (\n    AutoTokenizer,\n    Gemma3ForConditionalGeneration,\n    TrainingArguments,\n    DataCollatorForLanguageModeling\n)\nfrom peft import LoraConfig\nfrom trl import SFTTrainer\nfrom datasets import load_dataset\n\nprint(f\"PyTorch version: {torch.__version__}\")\nprint(f\"CUDA available: {torch.cuda.is_available()}\")\nif torch.cuda.is_available():\n    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")\n    print(f\"Number of GPUs: {torch.cuda.device_count()}\")\n    for i in range(torch.cuda.device_count()):\n        print(f\"  GPU {i}: {torch.cuda.get_device_name(i)} ({torch.cuda.get_device_properties(i).total_memory / 1e9:.2f} GB)\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Login to Hugging Face\n\nYou'll need a Hugging Face account and API token to:\n- Download Gemma 3 model (requires agreement to terms)\n- Push your fine-tuned model to the Hub\n\n**Get your token**: [huggingface.co/settings/tokens](https://huggingface.co/settings/tokens)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Login to Hugging Face\nfrom huggingface_hub import login\n\n# If running on Kaggle with secrets:\n# from kaggle_secrets import UserSecretsClient\n# user_secrets = UserSecretsClient()\n# hf_token = user_secrets.get_secret(\"HUGGINGFACE_TOKEN\")\n# login(hf_token)\n\n# For Jupyter/Colab:\nlogin()\n\nprint(\"\u2705 Logged in to Hugging Face!\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Model and Tokenizer\n\n### Download Gemma 3 4B Instruct\n\nWe'll use the **Gemma 3 4B Instruct** model:\n- 4 billion parameters (good balance of quality and efficiency)\n- Instruction-tuned variant (better for Q&A)\n- Supports reasoning with proper prompting\n\n**Important Settings:**\n- `device_map=\"auto\"`: Automatically distribute across GPUs\n- `attn_implementation='eager'`: Use standard attention (more compatible)\n- `.eval()`: Set to evaluation mode initially"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Define model path\n# If downloaded from Kaggle, use local path:\n# GEMMA_PATH = \"/kaggle/input/gemma-3/transformers/gemma-3-4b-it/1\"\n\n# Otherwise, use Hugging Face Hub:\nGEMMA_PATH = \"google/gemma-3-4b-it\"\n\nprint(f\"Model: {GEMMA_PATH}\")\nprint(\"\\nNote: This model requires accepting terms on Hugging Face\")\nprint(\"Visit: https://huggingface.co/google/gemma-3-4b-it\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Load Gemma 3 model\nprint(f\"Loading {GEMMA_PATH}...\")\nprint(\"This may take several minutes on first load...\")\n\nmodel = Gemma3ForConditionalGeneration.from_pretrained(\n    GEMMA_PATH,\n    device_map=\"auto\",  # Automatically distribute across available GPUs\n    attn_implementation='eager',  # Standard attention implementation\n    torch_dtype=torch.bfloat16,  # Use BFloat16 for efficiency\n).eval()\n\nprint(\"\\n\u2705 Model loaded successfully!\")\nprint(f\"Model device map: {model.hf_device_map if hasattr(model, 'hf_device_map') else 'auto'}\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Load tokenizer\ntokenizer = AutoTokenizer.from_pretrained(GEMMA_PATH)\n\nprint(\"\u2705 Tokenizer loaded successfully!\")\nprint(f\"\\nVocabulary size: {len(tokenizer):,}\")\nprint(f\"Model max length: {tokenizer.model_max_length}\")\nprint(f\"Padding side: {tokenizer.padding_side}\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load and Process Dataset\n\n### About the Financial Reasoning Dataset\n\n**Dataset**: `TheFinAI/Fino1_Reasoning_Path_FinQA`\n\nThis dataset contains:\n- **Financial questions** from FinQA benchmark\n- **Reasoning paths** generated by GPT-4o\n- **Structured answers** with calculations\n- **Complex financial scenarios**\n\n**Format:**\n- `Open-ended Verifiable Question`: The financial question\n- `Complex_CoT`: Chain-of-thought reasoning\n- `Response`: The final answer\n\n### Prompt Template\n\nWe'll structure the data with:\n```\n### Question:\n[financial question]\n\n### Response:\n<think>\n[reasoning chain]\n</think>\n[final answer]\n```"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Define training prompt style\ntrain_prompt_style = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. \nWrite a response that appropriately completes the request. \nBefore answering, think carefully about the question and create a step-by-step chain of thoughts to ensure a logical and accurate response.\n\n### Question:\n{}\n\n### Response:\n<think>\n{}\n</think>\n{}\n\"\"\"\n\nprint(\"\u2705 Prompt template defined!\")\nprint(\"\\nThis template structures the data with reasoning chains.\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Define formatting function\ndef formatting_prompts_func(examples):\n    \"\"\"\n    Format dataset examples into training format.\n    \n    Args:\n        examples: Batch of examples from dataset\n        \n    Returns:\n        Dictionary with formatted texts\n    \"\"\"\n    inputs = examples[\"Open-ended Verifiable Question\"]\n    complex_cots = examples[\"Complex_CoT\"]\n    outputs = examples[\"Response\"]\n    texts = []\n    \n    for question, cot, response in zip(inputs, complex_cots, outputs):\n        # Append EOS token if not present\n        if not response.endswith(tokenizer.eos_token):\n            response += tokenizer.eos_token\n        \n        # Format with template\n        text = train_prompt_style.format(question, cot, response)\n        texts.append(text)\n    \n    return {\"text\": texts}\n\nprint(\"\u2705 Formatting function defined!\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Load the financial reasoning dataset\nprint(\"Loading FinQA reasoning dataset...\")\nprint(\"Using 500 samples for efficient training...\")\n\ndataset = load_dataset(\n    \"TheFinAI/Fino1_Reasoning_Path_FinQA\",\n    split=\"train[0:500]\",  # Use first 500 samples\n    trust_remote_code=True\n)\n\nprint(f\"\\n\u2705 Dataset loaded successfully!\")\nprint(f\"Number of examples: {len(dataset):,}\")\nprint(f\"\\nDataset columns: {dataset.column_names}\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Explore the dataset structure\nprint(\"=== Sample Example ===\\n\")\n\nsample = dataset[0]\nprint(f\"Question: {sample['Open-ended Verifiable Question']}\\n\")\nprint(f\"Reasoning (first 300 chars): {sample['Complex_CoT'][:300]}...\\n\")\nprint(f\"Response: {sample['Response']}\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Apply formatting to dataset\nprint(\"Applying formatting to dataset...\")\n\ndataset = dataset.map(\n    formatting_prompts_func,\n    batched=True,\n)\n\nprint(\"\\n\u2705 Dataset formatted successfully!\")\nprint(f\"New column added: 'text'\")\nprint(f\"\\nExample length: {len(dataset['text'][0])} characters\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Inspect a formatted example\nprint(\"=== Formatted Example (first 800 chars) ===\\n\")\nprint(dataset[\"text\"][0][:800])\nprint(\"\\n... (truncated) ...\\n\")\nprint(\"=\" * 70)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Create data collator for language modeling\n# SFTTrainer requires this instead of tokenizer directly\n\ndata_collator = DataCollatorForLanguageModeling(\n    tokenizer=tokenizer,\n    mlm=False  # Causal LM, not masked LM\n)\n\nprint(\"\u2705 Data collator created!\")\nprint(\"   This will handle tokenization during training\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Test Before Fine-tuning\n\nLet's establish a **baseline** by testing the model before fine-tuning.\n\n### Test Setup\n\nWe'll use a financial question from the dataset and see how the base model performs.\n\n**Expected behavior of base model:**\n- May provide generic/short answers\n- Limited reasoning depth\n- Possible inaccuracies in calculations\n- Doesn't follow reasoning format perfectly"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Define prompt style for inference (without answer/reasoning)\nprompt_style = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. \nWrite a response that appropriately completes the request. \nBefore answering, think carefully about the question and create a step-by-step chain of thoughts to ensure a logical and accurate response.\n\n### Question:\n{}\n\n### Response:\n<think>\n{}\n\"\"\"\n\nprint(\"\u2705 Inference prompt template defined!\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Select a test question from the dataset\ntest_idx = 0\nquestion = dataset[test_idx]['Open-ended Verifiable Question']\n\nprint(\"=== Test Question ===\")\nprint(question)\nprint(\"\\n\" + \"=\" * 70)\nprint(\"\\n=== Expected Answer (from dataset) ===\")\nprint(dataset[test_idx]['Response'])\nprint(\"=\" * 70)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Generate response with base model (before fine-tuning)\nprint(\"\\n=== BASE MODEL RESPONSE (Before Fine-tuning) ===\\n\")\n\n# Format the prompt\ntest_prompt = prompt_style.format(question, \"\") + tokenizer.eos_token\n\n# Tokenize\ninputs = tokenizer([test_prompt], return_tensors=\"pt\").to(\"cuda\")\n\n# Generate\noutputs = model.generate(\n    input_ids=inputs.input_ids,\n    attention_mask=inputs.attention_mask,\n    max_new_tokens=1200,\n    eos_token_id=tokenizer.eos_token_id,\n    use_cache=True,\n    do_sample=False,  # Greedy for consistency\n)\n\n# Decode\nresponse = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n\n# Extract just the response part\nif \"### Response:\" in response[0]:\n    response_text = response[0].split(\"### Response:\")[1].strip()\nelse:\n    response_text = response[0]\n\nprint(response_text)\nprint(\"\\n\" + \"=\" * 70)\n\nprint(\"\\n\ud83d\udca1 Observation: The base model may provide short, less accurate answers\")\nprint(\"   We'll see significant improvement after fine-tuning!\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Configure and Train\n\n### Configure LoRA for Gemma 3\n\n**LoRA Settings:**\n- **r=64**: Higher rank for better quality (suitable for 4B model)\n- **lora_alpha=16**: Scaling factor\n- **lora_dropout=0.05**: Slight dropout for regularization\n- **target_modules**: All attention and MLP projections\n\n**Why target all modules?**\n- Gemma 3 benefits from full coverage\n- Better reasoning capabilities\n- More accurate financial calculations"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Configure LoRA\npeft_config = LoraConfig(\n    lora_alpha=16,  # Scaling factor for LoRA\n    lora_dropout=0.05,  # Dropout for regularization\n    r=64,  # Rank of LoRA update matrices (higher = more capacity)\n    bias=\"none\",  # No bias reparameterization\n    task_type=\"CAUSAL_LM\",  # Causal language modeling\n    target_modules=[\n        \"q_proj\",      # Query projection\n        \"k_proj\",      # Key projection\n        \"v_proj\",      # Value projection\n        \"o_proj\",      # Output projection\n        \"gate_proj\",   # Gate projection (MLP)\n        \"up_proj\",     # Up projection (MLP)\n        \"down_proj\",   # Down projection (MLP)\n    ],\n)\n\nprint(\"\u2705 LoRA configuration created!\")\nprint(f\"\\nLoRA settings:\")\nprint(f\"  Rank (r): {peft_config.r}\")\nprint(f\"  Alpha: {peft_config.lora_alpha}\")\nprint(f\"  Dropout: {peft_config.lora_dropout}\")\nprint(f\"  Target modules: {len(peft_config.target_modules)} modules\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Define training arguments\ntraining_arguments = TrainingArguments(\n    output_dir=\"output\",\n    \n    # Training duration\n    num_train_epochs=1,\n    \n    # Batch sizes\n    per_device_train_batch_size=1,\n    per_device_eval_batch_size=1,\n    gradient_accumulation_steps=2,  # Effective batch size = 2\n    \n    # Optimization\n    optim=\"paged_adamw_32bit\",  # Memory-efficient optimizer\n    learning_rate=2e-4,\n    warmup_steps=10,\n    \n    # Mixed precision\n    fp16=False,\n    bf16=False,  # Set to True if your GPU supports it\n    \n    # Logging\n    logging_strategy=\"steps\",\n    logging_steps=0.2,  # Log 5 times per epoch\n    \n    # Efficiency\n    group_by_length=True,  # Group similar lengths for efficiency\n    \n    # Experiment tracking\n    report_to=\"none\",  # Set to \"wandb\" if using W&B\n)\n\nprint(\"\u2705 Training arguments configured!\")\nprint(f\"\\nKey settings:\")\nprint(f\"  Epochs: {training_arguments.num_train_epochs}\")\nprint(f\"  Batch size: {training_arguments.per_device_train_batch_size}\")\nprint(f\"  Gradient accumulation: {training_arguments.gradient_accumulation_steps}\")\nprint(f\"  Effective batch size: {training_arguments.per_device_train_batch_size * training_arguments.gradient_accumulation_steps}\")\nprint(f\"  Learning rate: {training_arguments.learning_rate}\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Initialize SFTTrainer\ntrainer = SFTTrainer(\n    model=model,\n    args=training_arguments,\n    train_dataset=dataset,\n    peft_config=peft_config,\n    data_collator=data_collator,\n    dataset_text_field=\"text\",  # Use the formatted 'text' column\n)\n\nprint(\"\u2705 SFTTrainer initialized!\")\nprint(\"\\nReady to start training...\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Clear CUDA cache before training\ntorch.cuda.empty_cache()\n\nprint(\"=\" * 70)\nprint(\"STARTING FINE-TUNING\")\nprint(\"=\" * 70)\nprint()\nprint(f\"Training on {len(dataset):,} financial reasoning examples\")\nprint(f\"Running for {training_arguments.num_train_epochs} epoch(s)\")\nprint()\nprint(\"Expected training time: ~1-2 hours\")\nprint(\"Monitor GPU usage with: nvidia-smi\")\nprint()\nprint(\"=\" * 70)\nprint()\n\n# Start training\n# Uncomment the line below to start training\n# trainer_stats = trainer.train()\n\nprint(\"\u26a0\ufe0f Training is commented out by default.\")\nprint(\"Uncomment 'trainer_stats = trainer.train()' above to start actual training.\")\nprint()\nprint(\"\ud83d\udca1 Expected: Loss will gradually decrease, indicating learning\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Tips\n\n**Monitor these during training:**\n- **Loss**: Should decrease gradually (~2.5 \u2192 ~1.0 typical)\n- **GPU Memory**: Should stay under 16GB per GPU\n- **Speed**: ~1-2 seconds per step\n- **Total time**: ~1-1.5 hours for 500 samples\n\n**If you run out of memory:**\n- Reduce `max_seq_length` in SFTTrainer\n- Set `gradient_accumulation_steps=4`\n- Enable `gradient_checkpointing=True`\n\n**If training is slow:**\n- Ensure `bf16=True` (if supported)\n- Check GPU utilization with `nvidia-smi`\n- Consider using fewer target modules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Test After Fine-tuning\n\nNow let's test the **fine-tuned model** and compare it with the baseline!\n\n### What to Expect\n\n**After fine-tuning, the model should:**\n- \u2705 Provide detailed reasoning in `<think>` tags\n- \u2705 Show step-by-step calculations\n- \u2705 Give accurate final answers\n- \u2705 Follow the reasoning format consistently\n- \u2705 Understand financial terminology better"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Test the fine-tuned model\n# Use the same question as before for comparison\n\nprint(\"=== FINE-TUNED MODEL RESPONSE ===\\n\")\n\n# Format the prompt\ntest_prompt = prompt_style.format(question, \"\") + tokenizer.eos_token\n\n# Tokenize\ninputs = tokenizer([test_prompt], return_tensors=\"pt\").to(\"cuda\")\n\n# Generate\noutputs = model.generate(\n    input_ids=inputs.input_ids,\n    attention_mask=inputs.attention_mask,\n    max_new_tokens=1200,\n    eos_token_id=tokenizer.eos_token_id,\n    use_cache=True,\n    do_sample=False,\n)\n\n# Decode\nresponse_finetuned = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n\n# Extract response\nif \"### Response:\" in response_finetuned[0]:\n    response_text_finetuned = response_finetuned[0].split(\"### Response:\")[1].strip()\nelse:\n    response_text_finetuned = response_finetuned[0]\n\nprint(response_text_finetuned)\nprint(\"\\n\" + \"=\" * 70)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison: Before vs After\n\nLet's analyze the improvement:\n\n**Base Model (Before):**\n- \u274c Short, potentially inaccurate answers\n- \u274c Limited reasoning shown\n- \u274c May not understand financial context\n- \u274c Doesn't follow format\n\n**Fine-Tuned Model (After):**\n- \u2705 Detailed reasoning in `<think>` tags\n- \u2705 Step-by-step calculations\n- \u2705 Accurate financial understanding\n- \u2705 Follows format consistently\n- \u2705 Provides precise answers\n\n**Example from Tutorial:**\n\n*Question:* \"What portion of the estimated amortization expense will be recognized in 2017?\"\n\n*Base Model:* \"About $10,509\" (incorrect - gives amount instead of ratio)\n\n*Fine-Tuned:* \"About 18.05%\" (correct - shows reasoning and calculates ratio)\n\n**The fine-tuning enables the model to:**\n1. Understand what \"portion\" means (ratio, not amount)\n2. Show all calculation steps\n3. Arrive at the correct answer format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test with Multiple Financial Questions\n\nLet's test the model on various types of financial questions to validate its reasoning across different scenarios."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Test with another question from the dataset\ntest_idx_2 = 10\nquestion_2 = dataset[test_idx_2]['Open-ended Verifiable Question']\n\nprint(\"=== Test Question 2 ===\")\nprint(question_2)\nprint(\"\\n\" + \"=\" * 70)\n\n# Generate response\ntest_prompt_2 = prompt_style.format(question_2, \"\") + tokenizer.eos_token\ninputs_2 = tokenizer([test_prompt_2], return_tensors=\"pt\").to(\"cuda\")\n\noutputs_2 = model.generate(\n    input_ids=inputs_2.input_ids,\n    attention_mask=inputs_2.attention_mask,\n    max_new_tokens=1200,\n    eos_token_id=tokenizer.eos_token_id,\n    use_cache=True,\n)\n\nresponse_2 = tokenizer.batch_decode(outputs_2, skip_special_tokens=True)\n\n# Extract and display\nif \"### Response:\" in response_2[0]:\n    print(\"\\n=== RESPONSE ===\")\n    print(response_2[0].split(\"### Response:\")[1].strip())\nelse:\n    print(response_2[0])\n\nprint(\"\\n\" + \"=\" * 70)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Function for batch testing\ndef test_financial_reasoning(model, tokenizer, question, max_tokens=1200):\n    \"\"\"\n    Test the model on a financial question.\n    \n    Args:\n        model: The fine-tuned model\n        tokenizer: The tokenizer\n        question: The financial question\n        max_tokens: Maximum tokens to generate\n        \n    Returns:\n        Generated response with reasoning\n    \"\"\"\n    # Format prompt\n    prompt = prompt_style.format(question, \"\") + tokenizer.eos_token\n    \n    # Tokenize\n    inputs = tokenizer([prompt], return_tensors=\"pt\").to(\"cuda\")\n    \n    # Generate\n    outputs = model.generate(\n        input_ids=inputs.input_ids,\n        attention_mask=inputs.attention_mask,\n        max_new_tokens=max_tokens,\n        eos_token_id=tokenizer.eos_token_id,\n        use_cache=True,\n    )\n    \n    # Decode\n    response = tokenizer.batch_decode(outputs, skip_special_tokens=True)[0]\n    \n    # Extract response\n    if \"### Response:\" in response:\n        return response.split(\"### Response:\")[1].strip()\n    return response\n\nprint(\"\u2705 Testing function defined!\")\nprint(\"\\nUse this to test your model on any financial question.\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Save and Share Model\n\nAfter training, we'll save the model in two ways:\n1. **Locally**: For immediate use\n2. **Hugging Face Hub**: For sharing with community\n\n### Model Formats\n\n**Standard PyTorch**: Full model with LoRA merged\n- Best for: Production deployment\n- Size: ~4-8GB\n- Load with: `AutoModelForCausalLM.from_pretrained()`\n\n**LoRA Adapters Only**: Just the fine-tuned weights\n- Best for: Sharing, experimentation\n- Size: ~100-200MB\n- Load with: `PeftModel.from_pretrained()`"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Save model and tokenizer locally\nnew_model_local = \"Gemma-3-4B-Fin-QA-Reasoning\"\n\nprint(f\"Saving model to: {new_model_local}\")\n\n# Uncomment when training is complete\n# model.save_pretrained(new_model_local)\n# tokenizer.save_pretrained(new_model_local)\n# print(\"\\n\u2705 Model and tokenizer saved locally!\")\n\nprint(\"\u26a0\ufe0f Saving is commented out - uncomment after training\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Push to Hugging Face Hub\nnew_model_online = \"your-username/Gemma-3-4B-Fin-QA-Reasoning\"  # Update with your username\n\nprint(f\"Pushing model to: {new_model_online}\")\n\n# Uncomment when training is complete\n# model.push_to_hub(new_model_online)\n# tokenizer.push_to_hub(new_model_online)\n# print(\"\\n\u2705 Model and tokenizer pushed to Hub!\")\n\nprint(\"\u26a0\ufe0f Pushing to Hub is commented out\")\nprint(\"Update 'your-username' with your HF username, then uncomment\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Your Fine-Tuned Model\n\nOnce saved, you can load your model anytime:\n\n```python\nfrom transformers import AutoTokenizer, Gemma3ForConditionalGeneration\n\n# From local\nmodel = Gemma3ForConditionalGeneration.from_pretrained(\"./Gemma-3-4B-Fin-QA-Reasoning\")\ntokenizer = AutoTokenizer.from_pretrained(\"./Gemma-3-4B-Fin-QA-Reasoning\")\n\n# From Hub\nmodel = Gemma3ForConditionalGeneration.from_pretrained(\"your-username/Gemma-3-4B-Fin-QA-Reasoning\")\ntokenizer = AutoTokenizer.from_pretrained(\"your-username/Gemma-3-4B-Fin-QA-Reasoning\")\n```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \ud83c\udf89 Congratulations!\n\nYou've successfully:\n- \u2705 Loaded and configured Gemma 3 4B model\n- \u2705 Prepared financial reasoning dataset with proper formatting\n- \u2705 Applied LoRA for efficient fine-tuning\n- \u2705 Trained on 500 financial Q&A examples\n- \u2705 Tested and compared before/after performance\n- \u2705 Learned to save and share your model\n\n## \ud83c\udfaf Key Achievements\n\n### Gemma 3 Mastery\n- **Latest model**: Using Google's newest open-source model\n- **Efficient training**: LoRA with rank 64\n- **Reasoning capabilities**: Step-by-step thinking in `<think>` tags\n- **Domain expertise**: Financial question answering\n\n### Reasoning Improvement\n- **Before**: Short, potentially inaccurate answers\n- **After**: Detailed reasoning chains with accurate results\n- **Example**: \n  - Base: \"About $10,509\" (wrong format)\n  - Fine-tuned: \"About 18.05%\" (correct ratio with reasoning)\n\n### Production Skills\n- **Data formatting**: Structured prompt engineering\n- **Training pipeline**: End-to-end workflow\n- **Model deployment**: Saving and sharing\n- **Evaluation**: Before/after comparison\n\n## \ud83d\ude80 Next Steps\n\n### Immediate\n1. **Train longer**: Try 2-3 epochs for better results\n2. **More data**: Use full dataset (1000+ examples)\n3. **Evaluate systematically**: Create test set with ground truth\n4. **Hyperparameter tuning**: Adjust learning rate, LoRA rank\n\n### Advanced\n1. **Larger model**: Try Gemma 3 9B or 27B\n2. **Multi-task**: Combine financial Q&A with other tasks\n3. **Multimodal**: Add financial charts/graphs (Gemma 3 supports images)\n4. **Ensemble**: Combine multiple fine-tuned models\n\n### Production\n1. **Quantization**: Convert to 4-bit/8-bit for deployment\n2. **API**: Wrap in FastAPI or similar\n3. **Monitoring**: Track answer quality over time\n4. **Benchmarking**: Test on FinQA benchmark\n\n## \ud83d\udcda Resources\n\n### Papers & Research\n- [Gemma 3 Technical Report](https://storage.googleapis.com/deepmind-media/gemma/gemma-3-report.pdf)\n- [LoRA Paper](https://arxiv.org/abs/2106.09685)\n- [FinQA Dataset Paper](https://arxiv.org/abs/2109.00122)\n\n### Tools & Libraries\n- [Gemma Models](https://huggingface.co/collections/google/gemma-3-release-67761845aef84b8eaca4cb1a)\n- [TRL Documentation](https://huggingface.co/docs/trl)\n- [PEFT Documentation](https://huggingface.co/docs/peft)\n- [Unsloth](https://github.com/unslothai/unsloth) (alternative training library)\n\n### Datasets\n- [FinQA](https://huggingface.co/datasets/dreamerdeo/finqa)\n- [Financial Reasoning Datasets](https://huggingface.co/datasets?search=financial+reasoning)\n- [TAT-QA](https://huggingface.co/datasets/nightdessert/TAT-QA)\n\n## \ud83d\udca1 Tips for Better Results\n\n### Data Quality\n1. **More examples**: 1000-5000 for best results\n2. **Diverse questions**: Cover different financial topics\n3. **Quality reasoning**: Ensure CoT chains are logical\n4. **Balanced difficulty**: Mix easy and hard questions\n\n### Training\n1. **Learning rate**: Try 1e-4 to 5e-4\n2. **LoRA rank**: Higher (128) for larger models\n3. **Epochs**: 2-3 typically optimal\n4. **Batch size**: Increase if you have memory\n\n### Evaluation\n1. **Hold-out test set**: Never seen during training\n2. **Answer accuracy**: Check if final answers are correct\n3. **Reasoning quality**: Evaluate logic in `<think>` tags\n4. **Human review**: Sample random predictions\n\n## \ud83d\udd27 Troubleshooting\n\n### Out of Memory\n- Reduce `max_new_tokens` to 1024\n- Set `per_device_train_batch_size=1`\n- Increase `gradient_accumulation_steps=4`\n- Use `gradient_checkpointing=True` in TrainingArguments\n\n### Poor Quality Results\n- Train for more epochs (2-3)\n- Increase LoRA rank to 128\n- Check data quality and formatting\n- Ensure reasoning chains are complete\n\n### Slow Training\n- Enable BF16: `bf16=True` (if supported)\n- Reduce `max_new_tokens` in generation\n- Use `group_by_length=True` (already set)\n- Consider using Unsloth for 2x speedup\n\n### Model Not Following Format\n- Check prompt template formatting\n- Ensure EOS tokens are present\n- Verify dataset formatting is correct\n- Train for more steps/epochs\n\n## \ud83c\udf1f Why This Approach Works\n\n### Gemma 3 Advantages\n- **Latest technology**: Based on Gemini 2.0\n- **Multimodal ready**: Can be extended to charts/images\n- **Efficient**: Smaller than comparable models\n- **Open-source**: Full access and customization\n\n### Fine-Tuning Benefits\n- **Domain adaptation**: Learns financial terminology\n- **Reasoning improvement**: Explicit thinking process\n- **Accuracy gains**: Better than generic models\n- **Format consistency**: Follows desired style\n\n### LoRA Efficiency\n- **Fast training**: 1-2 hours vs days\n- **Low memory**: Fits on consumer GPUs\n- **Comparable quality**: Nearly as good as full FT\n- **Easy deployment**: Small adapter files\n\n---\n\n**Happy Fine-Tuning!** \ud83c\udf93\u2728\n\nFor more tutorials, check out:\n- [DeepSeek-R1 Reasoning](./Math-Reasoning-Qwen-GRPO.ipynb) (Synthetic data approach)\n- [GPT-2 From Scratch](../01-Full-Fine-Tuning/)\n- [Falcon-7B LoRA](../02-PEFT/)\n- [FLAN-T5 Summarization](../03-Instruction-Tuning/Summarization-FLAN-T5.ipynb)\n- [Financial Sentiment OPT](../03-Instruction-Tuning/Financial-Sentiment-OPT.ipynb)\n\n---\n\n**You've now mastered reasoning fine-tuning with Gemma 3!** \ud83d\ude80\n\nThis completes a comprehensive understanding of:\n- \u2705 Different reasoning approaches (DeepSeek vs Gemma)\n- \u2705 Synthetic vs real reasoning data\n- \u2705 Various model architectures\n- \u2705 Production deployment patterns\n\n**Thank you for learning with us!** \ud83d\ude4f"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}