{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Financial Sentiment Analysis with OPT-1.3B: Step-by-Step Guide\n\nThis notebook demonstrates **instruction fine-tuning** of the **OPT-1.3B** model for **financial sentiment analysis** using **Supervised Fine-Tuning (SFT)** with **LoRA**.\n\n## \ud83c\udfaf What You'll Learn\n\n- How to fine-tune LLMs for domain-specific tasks (finance)\n- How to use Deep Lake for dataset management\n- How to combine LoRA with SFT for efficient training\n- How to work with financial text data\n- How to merge LoRA adapters with base models\n- How to evaluate sentiment analysis models\n\n## \ud83d\udcbc Use Case: Financial Sentiment Analysis\n\n**Why this matters:**\n- Financial markets are driven by sentiment\n- Automated sentiment analysis saves time\n- Domain-specific models outperform generic ones\n- Real-world application in trading, risk assessment\n\n## \ud83d\udd27 Requirements\n\n- **GPU**: 16GB+ VRAM recommended\n- **Time**: 2-4 hours (depending on epochs)\n- **Dataset**: FinGPT sentiment (20K training samples)\n- **Model**: Facebook OPT-1.3B (1.3 billion parameters)\n\n## \ud83d\udcca Key Stats\n\n| Metric | Value |\n|--------|-------|\n| Base Model | OPT-1.3B (1.3B params) |\n| Trainable with LoRA | ~0.24% (3.1M params) |\n| Training Examples | 20,000 |\n| Validation Examples | 2,000 |\n| Training Time | 2-4 hours |\n\n## \ud83d\udcd6 Table of Contents\n\n1. [Setup Environment](#1-setup-environment)\n2. [Load Deep Lake Dataset](#2-load-deep-lake-dataset)\n3. [Initialize Model and Trainer](#3-initialize-model-and-trainer)\n4. [Fine-Tune with SFT](#4-fine-tune-with-sft)\n5. [Merge LoRA and OPT](#5-merge-lora-and-opt)\n6. [Inference and Testing](#6-inference-and-testing)\n\n---\n\n**Credits**: Based on the tutorial by [Youssef Hosni](https://youssef-hosni.medium.com/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup Environment\n\n### Install Required Packages\n\nWe'll install:\n- **transformers**: Hugging Face Transformers\n- **deeplake**: Dataset management\n- **trl**: Supervised Fine-Tuning Trainer\n- **peft**: LoRA implementation\n- **wandb**: Experiment tracking"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Install required packages\n!pip install -q transformers==4.32.0 deeplake==3.6.19 trl==0.6.0 peft==0.5.0 wandb==0.15.8\n\nprint(\"\u2705 All packages installed successfully!\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Import necessary libraries\nimport torch\nfrom transformers import (\n    AutoModelForCausalLM,\n    AutoTokenizer,\n    TrainingArguments\n)\nfrom peft import LoraConfig, PeftModel\nfrom trl import SFTTrainer\nfrom trl.trainer import ConstantLengthDataset\nimport deeplake\n\nprint(f\"PyTorch version: {torch.__version__}\")\nprint(f\"CUDA available: {torch.cuda.is_available()}\")\nif torch.cuda.is_available():\n    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")\n    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Deep Lake Dataset\n\n### About the FinGPT Sentiment Dataset\n\nThe **FinGPT sentiment dataset** contains:\n- **Financial tweets** from various sources\n- **Sentiment labels**: positive, negative, neutral\n- **Instructions** for each example\n- **20,000** training samples\n- **2,000** validation samples\n\n**Format**: Each example has three fields:\n- `instruction`: \"What is the sentiment of this news?\"\n- `input`: The financial text\n- `output`: The sentiment label\n\n### Why Deep Lake?\n\n- Efficient dataset management\n- Easy streaming for large datasets\n- Version control for data\n- Works seamlessly with PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Load the FinGPT sentiment dataset from Deep Lake\nprint(\"Loading FinGPT sentiment dataset...\")\nprint(\"This may take a moment on first load...\")\n\nds = deeplake.load('hub://genai360/FingGPT-sentiment-train-set')\nds_valid = deeplake.load('hub://genai360/FingGPT-sentiment-valid-set')\n\nprint(\"\\n\u2705 Datasets loaded successfully!\")\nprint(f\"\\nTraining dataset:\")\nprint(ds)\nprint(f\"\\nTraining samples: {len(ds):,}\")\nprint(f\"Validation samples: {len(ds_valid):,}\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Explore the dataset structure\nprint(\"=== Dataset Structure ===\")\nprint(f\"Tensors: {ds.tensors}\")\nprint()\n\n# Look at a sample\nprint(\"=== Sample Example ===\")\nsample_idx = 0\nprint(f\"Instruction: {ds[sample_idx]['instruction'].text()}\")\nprint(f\"\\nInput: {ds[sample_idx]['input'].text()}\")\nprint(f\"\\nOutput: {ds[sample_idx]['output'].text()}\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Show a few more examples\nprint(\"=== More Examples from Dataset ===\\n\")\n\nfor i in range(3):\n    print(f\"--- Example {i+1} ---\")\n    print(f\"Instruction: {ds[i]['instruction'].text()}\")\n    print(f\"Input: {ds[i]['input'].text()}\")\n    print(f\"Output: {ds[i]['output'].text()}\")\n    print()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Dataset for Training\n\nWe need to format each example into a structured text format that the model can learn from.\n\n**Format:**\n```\n{instruction}\n\nContent: {input}\n\nSentiment: {output}\n```\n\nThis creates a consistent pattern for the model to learn."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Define function to prepare sample text\ndef prepare_sample_text(example):\n    \"\"\"\n    Prepare the text from a sample of the dataset.\n    \n    Combines instruction, input, and output into a formatted string.\n    \n    Args:\n        example: A sample from the dataset\n        \n    Returns:\n        Formatted text string\n    \"\"\"\n    text = f\"\"\"{example['instruction'].text()}\n\nContent: {example['input'].text()}\n\nSentiment: {example['output'].text()}\"\"\"\n    return text\n\n# Test the function\nprint(\"=== Formatted Example ===\")\nprint(prepare_sample_text(ds[0]))"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Load the tokenizer for OPT-1.3B\nmodel_name = \"facebook/opt-1.3b\"\n\nprint(f\"Loading tokenizer for {model_name}...\")\ntokenizer = AutoTokenizer.from_pretrained(model_name)\n\nprint(\"\\n\u2705 Tokenizer loaded successfully!\")\nprint(f\"\\nVocabulary size: {len(tokenizer):,}\")\nprint(f\"Model max length: {tokenizer.model_max_length}\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Test tokenization\ntest_text = prepare_sample_text(ds[0])\ntokens = tokenizer(test_text, return_tensors=\"pt\")\n\nprint(f\"Original text length: {len(test_text)} characters\")\nprint(f\"Number of tokens: {len(tokens['input_ids'][0])}\")\nprint(f\"\\nFirst 10 token IDs: {tokens['input_ids'][0][:10].tolist()}\")\nprint(f\"\\nDecoded back: {tokenizer.decode(tokens['input_ids'][0][:100])}...\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create ConstantLengthDataset\n\nThe `ConstantLengthDataset` from TRL:\n- Ensures all sequences have the same length (1024 tokens)\n- Handles padding and truncation automatically\n- Enables efficient batch processing\n- Supports infinite sampling for long training"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Create training dataset with constant length\nseq_length = 1024\n\nprint(f\"Creating training dataset with sequence length: {seq_length}\")\n\ntrain_dataset = ConstantLengthDataset(\n    tokenizer,\n    ds,\n    formatting_func=prepare_sample_text,\n    infinite=True,  # Allow infinite sampling\n    seq_length=seq_length\n)\n\nprint(\"\\n\u2705 Training dataset created!\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Inspect a sample from the prepared dataset\niterator = iter(train_dataset)\nsample = next(iterator)\n\nprint(\"=== Prepared Sample ===\")\nprint(f\"Keys: {sample.keys()}\")\nprint(f\"\\nInput IDs shape: {sample['input_ids'].shape}\")\nprint(f\"Labels shape: {sample['labels'].shape}\")\nprint(f\"\\nFirst 20 input IDs: {sample['input_ids'][:20].tolist()}\")\nprint(f\"\\nDecoded text (first 200 chars):\")\nprint(tokenizer.decode(sample['input_ids'][:100]))"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Create validation dataset\nprint(\"Creating validation dataset...\")\n\neval_dataset = ConstantLengthDataset(\n    tokenizer,\n    ds_valid,\n    formatting_func=prepare_sample_text,\n    seq_length=seq_length\n)\n\nprint(\"\u2705 Validation dataset created!\")\nprint(f\"\\nDatasets ready for training!\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Initialize Model and Trainer\n\n### Configure LoRA for Efficient Fine-Tuning\n\n**LoRA (Low-Rank Adaptation)** allows us to:\n- Train only ~0.24% of model parameters\n- Significantly reduce memory requirements\n- Maintain high performance\n- Enable faster training\n\n### LoRA Parameters\n\n- **r=16**: Rank of low-rank matrices (higher = more capacity)\n- **lora_alpha=32**: Scaling factor for LoRA updates\n- **lora_dropout=0.05**: Dropout for regularization\n- **task_type**: CAUSAL_LM for language modeling"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Configure LoRA\nlora_config = LoraConfig(\n    r=16,  # Rank of the low-rank matrices\n    lora_alpha=32,  # Scaling factor\n    lora_dropout=0.05,  # Dropout for regularization\n    bias=\"none\",  # Don't train bias terms\n    task_type=\"CAUSAL_LM\",  # Causal language modeling\n)\n\nprint(\"\u2705 LoRA configuration created!\")\nprint(f\"\\nLoRA settings:\")\nprint(f\"  Rank (r): {lora_config.r}\")\nprint(f\"  Alpha: {lora_config.lora_alpha}\")\nprint(f\"  Dropout: {lora_config.lora_dropout}\")\nprint(f\"  Task type: {lora_config.task_type}\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure Training Arguments\n\nKey settings for financial sentiment fine-tuning:\n\n**Training Duration:**\n- 10 epochs (can adjust based on convergence)\n- Save and evaluate after each epoch\n\n**Optimization:**\n- Learning rate: 1e-4 (conservative for fine-tuning)\n- Cosine learning rate schedule\n- 100 warmup steps\n\n**Efficiency:**\n- Batch size: 12 per device\n- BFloat16 mixed precision\n- Gradient accumulation: 1 step\n\n**Monitoring:**\n- Weights & Biases integration\n- Log every 5 steps"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Define training arguments\ntraining_args = TrainingArguments(\n    output_dir=\"./OPT-fine_tuned-FinGPT\",\n    \n    # Training duration\n    num_train_epochs=10,\n    \n    # Batch sizes\n    per_device_train_batch_size=12,\n    per_device_eval_batch_size=12,\n    \n    # Optimization\n    learning_rate=1e-4,\n    lr_scheduler_type=\"cosine\",\n    warmup_steps=100,\n    weight_decay=0.05,\n    \n    # Gradient settings\n    gradient_accumulation_steps=1,\n    gradient_checkpointing=False,\n    \n    # Mixed precision\n    fp16=False,\n    bf16=True,  # BFloat16 for better stability\n    \n    # Evaluation and saving\n    evaluation_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n    logging_steps=5,\n    \n    # Data handling\n    dataloader_drop_last=True,\n    \n    # Distributed training\n    ddp_find_unused_parameters=False,\n    \n    # Experiment tracking\n    run_name=\"OPT-fine_tuned-FinGPT\",\n    report_to=\"wandb\",  # Set to \"none\" if not using W&B\n)\n\nprint(\"\u2705 Training arguments configured!\")\nprint(f\"\\nKey settings:\")\nprint(f\"  Output dir: {training_args.output_dir}\")\nprint(f\"  Epochs: {training_args.num_train_epochs}\")\nprint(f\"  Batch size: {training_args.per_device_train_batch_size}\")\nprint(f\"  Learning rate: {training_args.learning_rate}\")\nprint(f\"  Mixed precision: BF16={training_args.bf16}\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load OPT-1.3B Model\n\nWe'll load the base OPT-1.3B model and prepare it for LoRA fine-tuning.\n\n**Steps:**\n1. Load model in BFloat16 precision\n2. Freeze base model parameters\n3. Keep small parameters (layer norms) in FP32 for stability\n4. Enable gradient checkpointing\n5. Prepare for LoRA adapter injection"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Load the OPT-1.3B model\nprint(f\"Loading {model_name}...\")\nprint(\"This may take a few minutes...\")\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    torch_dtype=torch.bfloat16\n)\n\nprint(\"\\n\u2705 Model loaded successfully!\")\nprint(f\"Model: {model_name}\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Prepare model for LoRA training\nfrom torch import nn\n\nprint(\"Preparing model for LoRA training...\")\n\n# Freeze base model parameters\nfor param in model.parameters():\n    param.requires_grad = False  # Freeze the model - train adapters later\n    if param.ndim == 1:\n        # Cast small parameters (e.g., layernorm) to FP32 for stability\n        param.data = param.data.to(torch.float32)\n\n# Enable gradient checkpointing to reduce memory usage\nmodel.gradient_checkpointing_enable()\n\n# Enable input gradients\nmodel.enable_input_require_grads()\n\n# Define custom class to cast output to FP32\nclass CastOutputToFloat(nn.Sequential):\n    def forward(self, x):\n        return super().forward(x).to(torch.float32)\n\n# Apply to language model head\nmodel.lm_head = CastOutputToFloat(model.lm_head)\n\nprint(\"\\n\u2705 Model prepared for training!\")\nprint(\"  - Base parameters frozen\")\nprint(\"  - Layer norms in FP32\")\nprint(\"  - Gradient checkpointing enabled\")\nprint(\"  - Output cast to FP32\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Fine-Tune with SFT\n\n### Initialize SFTTrainer\n\nThe **SFTTrainer** (Supervised Fine-Tuning Trainer) from TRL:\n- Handles the training loop automatically\n- Integrates LoRA seamlessly\n- Supports packing for efficient training\n- Manages evaluation and checkpointing\n\n**Packing**: Combines multiple examples into single sequences to minimize padding"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Initialize SFTTrainer\ntrainer = SFTTrainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=eval_dataset,\n    peft_config=lora_config,\n    packing=True,  # Pack multiple examples per sequence\n)\n\nprint(\"\u2705 SFTTrainer initialized!\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Function to print trainable parameters\ndef print_trainable_parameters(model):\n    \"\"\"\n    Prints the number of trainable parameters in the model.\n    \"\"\"\n    trainable_params = 0\n    all_param = 0\n    for _, param in model.named_parameters():\n        all_param += param.numel()\n        if param.requires_grad:\n            trainable_params += param.numel()\n    \n    trainable_pct = 100 * trainable_params / all_param\n    \n    print(f\"trainable params: {trainable_params:,}\")\n    print(f\"all params: {all_param:,}\")\n    print(f\"trainable%: {trainable_pct:.4f}%\")\n    \n    return trainable_params, all_param, trainable_pct\n\nprint(\"=== Model Parameters ===\")\ntrainable, total, pct = print_trainable_parameters(trainer.model)\n\nprint(f\"\\n\ud83d\udca1 With LoRA, we're only training {pct:.2f}% of parameters!\")\nprint(f\"   That's {trainable:,} out of {total:,} parameters.\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Start training!\nprint(\"=\" * 70)\nprint(\"STARTING FINE-TUNING\")\nprint(\"=\" * 70)\nprint()\nprint(f\"Training on {len(train_dataset)} samples\")\nprint(f\"Validating on {len(eval_dataset)} samples\")\nprint(f\"Running for {training_args.num_train_epochs} epochs\")\nprint()\nprint(\"This will take approximately 2-4 hours...\")\nprint(\"Monitor progress via logging output or W&B dashboard\")\nprint()\nprint(\"=\" * 70)\nprint()\n\n# Train the model\n# Uncomment the line below to start training\n# trainer.train()\n\nprint(\"\u26a0\ufe0f Training is commented out by default.\")\nprint(\"Uncomment 'trainer.train()' above to start actual training.\")\nprint()\nprint(\"For testing, you can:\")\nprint(\"  - Reduce num_train_epochs to 1-2\")\nprint(\"  - Use a smaller subset of data\")\nprint(\"  - Adjust batch size based on your GPU\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# After training completes, save the model\n# Uncomment when training is done\n\n# print(\"Saving fine-tuned model...\")\n# trainer.save_model(\"./OPT-fine_tuned-FinGPT/final\")\n# print(\"\u2705 Model saved!\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Merge LoRA and OPT\n\nAfter training, we need to:\n1. Load the base OPT-1.3B model\n2. Load the trained LoRA adapters\n3. Merge them together\n4. Save the merged model for inference\n\n**Why merge?**\n- Simpler deployment (single model file)\n- Faster inference (no adapter overhead)\n- Standard model format (compatible with all tools)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Load the base model\nprint(\"Loading base OPT-1.3B model...\")\n\nmodel_base = AutoModelForCausalLM.from_pretrained(\n    \"facebook/opt-1.3b\",\n    return_dict=True,\n    torch_dtype=torch.bfloat16\n)\n\nprint(\"\u2705 Base model loaded!\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Load the LoRA adapters and merge\n# Replace <desired_checkpoint> with your actual checkpoint folder\n# Example: \"checkpoint-500\" or \"final\"\n\ncheckpoint_path = \"./OPT-fine_tuned-FinGPT/final\"  # Update this path\n\nprint(f\"Loading LoRA adapters from: {checkpoint_path}\")\n\n# Note: This will only work after training is complete\n# Uncomment the lines below when you have a trained checkpoint\n\n# model_merged = PeftModel.from_pretrained(model_base, checkpoint_path)\n# model_merged.eval()\n# model_merged = model_merged.merge_and_unload()\n\n# print(\"\\n\u2705 LoRA adapters merged with base model!\")\n\n# # Save the merged model\n# merged_path = \"./OPT-fine_tuned-FinGPT/merged\"\n# model_merged.save_pretrained(merged_path)\n# print(f\"\\n\u2705 Merged model saved to: {merged_path}\")\n\nprint(\"\u26a0\ufe0f Merging is commented out - run after training completes\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Inference and Testing\n\nNow let's test both the **vanilla (base)** model and our **fine-tuned** model to compare their performance on financial sentiment analysis.\n\n### Test Setup\n\nWe'll use a sample financial news headline and ask both models to:\n1. Identify the sentiment\n2. Provide reasoning (optional)\n\n**Format:**\n```\nWhat is the sentiment of this news? Please choose an answer from {negative/neutral/positive}\n\nContent: [financial news text]\n\nSentiment:\n```"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Prepare test input\ntest_prompt = \"\"\"What is the sentiment of this news? Please choose an answer from {strong negative/moderately negative/mildly negative/neutral/mildly positive/moderately positive/strong positive}, then provide some short reasons.\n\nContent: UPDATE 1-AstraZeneca sells rare cancer drug to Sanofi for up to $300 mln.\n\nSentiment: \"\"\"\n\nprint(\"=== Test Prompt ===\")\nprint(test_prompt)\nprint(\"=\" * 70)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Tokenize the input\ninputs = tokenizer(test_prompt, return_tensors=\"pt\")\n\n# Move to GPU if available\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ninputs = {k: v.to(device) for k, v in inputs.items()}\n\nprint(f\"Input tokenized: {inputs['input_ids'].shape[1]} tokens\")\nprint(f\"Device: {device}\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Models for Comparison\n\nWe'll load:\n1. **Vanilla Model**: Base OPT-1.3B (no fine-tuning)\n2. **Fine-Tuned Model**: Our trained model with merged LoRA adapters\n\n**Note**: In production, you'd only load the fine-tuned model."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Load vanilla (base) model for comparison\nprint(\"Loading vanilla OPT-1.3B model...\")\n\nmodel_vanilla = AutoModelForCausalLM.from_pretrained(\n    \"facebook/opt-1.3b\",\n    torch_dtype=torch.bfloat16\n)\nmodel_vanilla.to(device)\nmodel_vanilla.eval()\n\nprint(\"\u2705 Vanilla model loaded and ready!\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Load fine-tuned model\n# This requires that training has completed and model has been merged\n\nmerged_model_path = \"./OPT-fine_tuned-FinGPT/merged\"\n\nprint(f\"Loading fine-tuned model from: {merged_model_path}\")\n\n# Uncomment when you have a trained model\n# model_finetuned = AutoModelForCausalLM.from_pretrained(\n#     merged_model_path,\n#     torch_dtype=torch.bfloat16\n# )\n# model_finetuned.to(device)\n# model_finetuned.eval()\n# print(\"\u2705 Fine-tuned model loaded and ready!\")\n\nprint(\"\u26a0\ufe0f Loading fine-tuned model is commented out\")\nprint(\"   Uncomment after training and merging are complete\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Generate with vanilla model\nprint(\"=\" * 70)\nprint(\"VANILLA MODEL OUTPUT\")\nprint(\"=\" * 70)\nprint()\n\ngeneration_output_vanilla = model_vanilla.generate(\n    **inputs,\n    return_dict_in_generate=True,\n    output_scores=True,\n    max_length=256,\n    num_beams=1,\n    do_sample=True,\n    repetition_penalty=1.5,\n    length_penalty=2.0\n)\n\noutput_text_vanilla = tokenizer.decode(\n    generation_output_vanilla['sequences'][0],\n    skip_special_tokens=True\n)\n\nprint(output_text_vanilla)\nprint()\nprint(\"=\" * 70)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Generate with fine-tuned model\n# Uncomment when you have a trained model\n\n# print(\"=\" * 70)\n# print(\"FINE-TUNED MODEL OUTPUT\")\n# print(\"=\" * 70)\n# print()\n\n# generation_output_finetuned = model_finetuned.generate(\n#     **inputs,\n#     return_dict_in_generate=True,\n#     output_scores=True,\n#     max_length=256,\n#     num_beams=1,\n#     do_sample=False,  # Greedy for more consistent output\n#     repetition_penalty=1.5,\n#     length_penalty=2.0\n# )\n\n# output_text_finetuned = tokenizer.decode(\n#     generation_output_finetuned['sequences'][0],\n#     skip_special_tokens=True\n# )\n\n# print(output_text_finetuned)\n# print()\n# print(\"=\" * 70)\n\nprint(\"\u26a0\ufe0f Fine-tuned model inference is commented out\")\nprint(\"   Uncomment after training is complete\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expected Results Analysis\n\n**Vanilla Model:**\n- Often generates irrelevant or repetitive text\n- May not understand the sentiment task\n- Struggles with financial domain terminology\n- Output format may not match instructions\n\n**Fine-Tuned Model:**\n- Correctly identifies sentiment (positive in this case)\n- Follows instruction format precisely\n- Understands financial context\n- Provides concise, accurate response\n\n**Example Expected Output:**\n```\nFine-tuned: \"positive\"\n```\n\n### Why Fine-Tuning Works\n\n1. **Domain Adaptation**: Model learns financial terminology\n2. **Task Understanding**: Learns to classify sentiment\n3. **Format Consistency**: Follows instruction patterns\n4. **Efficient Training**: LoRA trains only 0.24% of parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test with Multiple Examples\n\nLet's test with various financial scenarios to see how the model performs across different sentiments."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Define multiple test cases\ntest_cases = [\n    {\n        \"text\": \"Apple reports record-breaking quarterly revenue, exceeding all analyst expectations\",\n        \"expected\": \"strong positive\"\n    },\n    {\n        \"text\": \"Company announces massive layoffs affecting 20% of workforce\",\n        \"expected\": \"strong negative\"\n    },\n    {\n        \"text\": \"Stock prices remain unchanged after quarterly earnings report\",\n        \"expected\": \"neutral\"\n    },\n    {\n        \"text\": \"Tech giant faces potential antitrust investigation\",\n        \"expected\": \"negative\"\n    },\n    {\n        \"text\": \"Pharmaceutical company receives FDA approval for new drug\",\n        \"expected\": \"positive\"\n    }\n]\n\nprint(\"=== Test Cases Prepared ===\")\nfor i, case in enumerate(test_cases, 1):\n    print(f\"{i}. Text: {case['text']}\")\n    print(f\"   Expected: {case['expected']}\")\n    print()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Function to test multiple examples\ndef test_sentiment(model, tokenizer, text, device):\n    \"\"\"\n    Test sentiment analysis on a given text.\n    \n    Args:\n        model: The model to use\n        tokenizer: The tokenizer\n        text: The financial text to analyze\n        device: Device to run on\n        \n    Returns:\n        Generated sentiment response\n    \"\"\"\n    prompt = f\"\"\"What is the sentiment of this news? Please choose an answer from {{negative/neutral/positive}}\n\nContent: {text}\n\nSentiment: \"\"\"\n    \n    inputs = tokenizer(prompt, return_tensors=\"pt\")\n    inputs = {k: v.to(device) for k, v in inputs.items()}\n    \n    outputs = model.generate(\n        **inputs,\n        max_length=150,\n        num_beams=1,\n        do_sample=False,\n        repetition_penalty=1.5\n    )\n    \n    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n    # Extract just the sentiment part\n    sentiment = response.split(\"Sentiment:\")[-1].strip().split()[0] if \"Sentiment:\" in response else response\n    \n    return sentiment\n\nprint(\"\u2705 Testing function defined!\")\nprint(\"\\nYou can use this to test your fine-tuned model on multiple examples.\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Test all cases with vanilla model\nprint(\"=\" * 70)\nprint(\"VANILLA MODEL - BATCH TEST RESULTS\")\nprint(\"=\" * 70)\nprint()\n\nfor i, case in enumerate(test_cases, 1):\n    print(f\"Test {i}:\")\n    print(f\"  Text: {case['text'][:50]}...\")\n    print(f\"  Expected: {case['expected']}\")\n    \n    # Uncomment to run actual inference\n    # result = test_sentiment(model_vanilla, tokenizer, case['text'], device)\n    # print(f\"  Predicted: {result}\")\n    \n    print(f\"  Predicted: [Run inference to see]\")\n    print()\n\nprint(\"\ud83d\udca1 Uncomment inference code to see actual predictions\")\nprint(\"=\" * 70)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \ud83c\udf89 Congratulations!\n\nYou've successfully:\n- \u2705 Loaded financial sentiment dataset from Deep Lake\n- \u2705 Configured LoRA for efficient fine-tuning\n- \u2705 Set up SFTTrainer with optimal hyperparameters\n- \u2705 Prepared OPT-1.3B for domain-specific training\n- \u2705 Understood the complete fine-tuning pipeline\n- \u2705 Learned how to merge LoRA adapters\n- \u2705 Tested model performance on financial text\n\n## \ud83c\udfaf Key Takeaways\n\n### Efficiency of LoRA\n- **Only 0.24%** of parameters trained (3.1M out of 1.3B)\n- **Significantly faster** than full fine-tuning\n- **Lower memory** requirements (fits on single GPU)\n- **Comparable performance** to full fine-tuning\n\n### Domain Adaptation\n- **Base model** struggles with financial sentiment\n- **Fine-tuned model** understands financial context\n- **Instruction following** improves dramatically\n- **Output format** becomes consistent\n\n### Practical Benefits\n- **Production-ready** after just 2-4 hours training\n- **Easy deployment** after merging adapters\n- **Scalable approach** for other financial tasks\n- **Cost-effective** compared to full fine-tuning\n\n## \ud83d\ude80 Next Steps\n\n### Immediate\n1. **Train the model**: Uncomment training code and run\n2. **Evaluate**: Test on validation set\n3. **Tune hyperparameters**: Adjust learning rate, epochs, etc.\n\n### Advanced\n1. **More data**: Train on larger financial datasets\n2. **Multi-class**: Expand to 7 sentiment categories\n3. **Other tasks**: Adapt for financial NER, summarization\n4. **Ensemble**: Combine multiple fine-tuned models\n\n### Production\n1. **Optimize inference**: Quantization, ONNX conversion\n2. **API deployment**: Serve via FastAPI or similar\n3. **Monitoring**: Track prediction quality over time\n4. **A/B testing**: Compare with baseline models\n\n## \ud83d\udcda Resources\n\n### Papers\n- [LoRA Paper](https://arxiv.org/abs/2106.09685)\n- [OPT Paper](https://arxiv.org/abs/2205.01068)\n- [Instruction Tuning Survey](https://arxiv.org/abs/2308.10792)\n\n### Tools\n- [PEFT Documentation](https://huggingface.co/docs/peft)\n- [TRL Documentation](https://huggingface.co/docs/trl)\n- [Deep Lake](https://docs.activeloop.ai/)\n- [OPT Model Card](https://huggingface.co/facebook/opt-1.3b)\n\n### Datasets\n- [FinGPT Datasets](https://huggingface.co/datasets?search=fingpt)\n- [Financial PhraseBank](https://huggingface.co/datasets/financial_phrasebank)\n- [FiQA Sentiment](https://sites.google.com/view/fiqa/)\n\n## \ud83d\udca1 Tips for Better Results\n\n### Training\n1. **More epochs**: 10-20 epochs often improve results\n2. **Learning rate**: Try 5e-5 to 2e-4\n3. **Batch size**: Increase if you have more GPU memory\n4. **Sequence length**: Adjust based on your text lengths\n\n### Data\n1. **More examples**: 50K+ samples for best results\n2. **Balanced classes**: Equal positive/negative/neutral\n3. **Quality over quantity**: Clean, accurate labels\n4. **Domain coverage**: Diverse financial topics\n\n### Evaluation\n1. **Hold-out test set**: Never seen during training\n2. **Multiple metrics**: Accuracy, F1, precision, recall\n3. **Error analysis**: Study misclassifications\n4. **Human evaluation**: Sample random predictions\n\n## \ud83d\udd27 Troubleshooting\n\n### Out of Memory\n- Reduce `per_device_train_batch_size`\n- Enable `gradient_checkpointing=True`\n- Reduce `seq_length` to 512 or 768\n\n### Poor Performance\n- Train for more epochs\n- Increase LoRA rank (r=32 or r=64)\n- Check data quality and balance\n- Adjust learning rate\n\n### Slow Training\n- Increase batch size if possible\n- Use `bf16=True` for mixed precision\n- Enable `gradient_checkpointing=False`\n- Use multiple GPUs if available\n\n---\n\n**Happy Fine-Tuning!** \ud83c\udf93\u2728\n\nFor more tutorials, check out:\n- [Full Fine-Tuning (GPT-2)](../01-Full-Fine-Tuning/)\n- [PEFT (Falcon-7B LoRA)](../02-PEFT/)\n- [Summarization (FLAN-T5)](./Summarization-FLAN-T5.ipynb)\n- [Reasoning Tuning](../04-Reasoning-Tuning/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}