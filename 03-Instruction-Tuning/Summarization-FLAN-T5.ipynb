{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instruction Fine-Tuning for Summarization: Step-by-Step Guide\n\nThis notebook demonstrates how to fine-tune **FLAN-T5** for dialogue summarization using two approaches:\n1. **Full Fine-Tuning**: Training all model parameters\n2. **PEFT (LoRA)**: Parameter-Efficient Fine-Tuning with only ~1.4% trainable parameters\n\n## \ud83c\udfaf What You'll Learn\n\n- How to fine-tune LLMs for summarization tasks\n- How to preprocess dialogue-summary datasets\n- How to perform full fine-tuning vs. PEFT\n- How to evaluate with ROUGE metrics\n- How to compare different fine-tuning approaches\n\n## \ud83d\udcca Key Comparisons\n\n| Approach | Trainable Params | Training Time | GPU Memory | ROUGE Score |\n|----------|------------------|---------------|------------|-------------|\n| Zero-shot | 0% | N/A | Minimal | Low (~21%) |\n| Full Fine-tune | 100% (247M) | Longer | High | Good (~41%) |\n| PEFT/LoRA | 1.41% (3.5M) | Faster | Lower | Comparable (~37%) |\n\n## \ud83d\udd27 Requirements\n\n- **GPU**: 16GB+ VRAM recommended\n- **Time**: 1-2 hours (depends on training steps)\n- **Dataset**: DialogSum (~10K dialogues)\n\n## \ud83d\udcd6 Table of Contents\n\n1. [Setup and Environment](#1-setup-and-environment)\n2. [Load Dataset and Model](#2-load-dataset-and-model)\n3. [Zero-Shot Baseline](#3-zero-shot-baseline)\n4. [Full Fine-Tuning](#4-full-fine-tuning)\n5. [PEFT/LoRA Fine-Tuning](#5-peft-lora-fine-tuning)\n6. [Results Comparison](#6-results-comparison)\n\n---\n\n**Credits**: Based on the excellent tutorial by [Youssef Hosni](https://youssef-hosni.medium.com/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Environment\n\n### 1.1. Install Required Dependencies\n\nWe'll install all necessary packages for:\n- **transformers**: Hugging Face Transformers library\n- **datasets**: Dataset loading and processing\n- **evaluate**: Evaluation metrics\n- **rouge_score**: ROUGE metric for summarization\n- **peft**: Parameter-Efficient Fine-Tuning\n- **torch**: PyTorch framework"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Install required packages\n# This may take a few minutes\n\n%pip install --upgrade pip\n%pip install --disable-pip-version-check torch==1.13.1 torchdata==0.5.1 --quiet\n\n%pip install transformers==4.27.2 datasets==2.11.0 evaluate==0.4.0 \\\n    rouge_score==0.1.2 peft==0.3.0 --quiet\n\nprint(\"\u2705 All packages installed successfully!\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Import necessary libraries\nfrom datasets import load_dataset\nfrom transformers import (\n    AutoModelForSeq2SeqLM, \n    AutoTokenizer, \n    GenerationConfig, \n    TrainingArguments, \n    Trainer\n)\nimport torch\nimport time\nimport evaluate\nimport pandas as pd\nimport numpy as np\n\nprint(f\"PyTorch version: {torch.__version__}\")\nprint(f\"CUDA available: {torch.cuda.is_available()}\")\nif torch.cuda.is_available():\n    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")\n    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Dataset and Model\n\n### 2.1. Load the DialogSum Dataset\n\n**DialogSum** is a large-scale dialogue summarization dataset with:\n- **10,000+** dialogues\n- Manually labeled summaries\n- Topics for each dialogue\n- Train/validation/test splits\n\nThe dataset is perfect for training summarization models."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Load the DialogSum dataset from Hugging Face\nhuggingface_dataset_name = \"knkarthick/dialogsum\"\ndataset = load_dataset(huggingface_dataset_name)\n\nprint(\"\u2705 Dataset loaded successfully!\")\nprint(f\"\\nDataset structure:\")\nprint(dataset)\n\nprint(f\"\\nSample counts:\")\nprint(f\"  Training: {len(dataset['train']):,} dialogues\")\nprint(f\"  Validation: {len(dataset['validation']):,} dialogues\")\nprint(f\"  Test: {len(dataset['test']):,} dialogues\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Explore a sample from the dataset\nsample_idx = 0\nsample = dataset['train'][sample_idx]\n\nprint(\"=== Sample Dialogue ===\")\nprint(sample['dialogue'])\nprint(\"\\n=== Human Summary ===\")\nprint(sample['summary'])\nprint(\"\\n=== Topic ===\")\nprint(sample['topic'])\nprint(\"\\n=== ID ===\")\nprint(sample['id'])"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Load FLAN-T5 Model and Tokenizer\n\n**FLAN-T5** is an instruction-tuned version of T5 that excels at various NLP tasks.\n\nWe'll use `flan-t5-base` which has:\n- **247M parameters**\n- Good balance between performance and resource requirements\n- Pre-trained on instruction-following tasks\n\n**Note**: We use `torch.bfloat16` for memory efficiency."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Load FLAN-T5 model and tokenizer\nmodel_name = 'google/flan-t5-base'\n\nprint(f\"Loading {model_name}...\")\nprint(\"This may take a moment...\")\n\noriginal_model = AutoModelForSeq2SeqLM.from_pretrained(\n    model_name, \n    torch_dtype=torch.bfloat16\n)\ntokenizer = AutoTokenizer.from_pretrained(model_name)\n\nprint(\"\\n\u2705 Model and tokenizer loaded successfully!\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Function to print trainable parameters\ndef print_number_of_trainable_model_parameters(model):\n    \"\"\"\n    Calculate and display the number of trainable vs total parameters.\n    \n    Args:\n        model: The model to analyze\n    \n    Returns:\n        String with parameter statistics\n    \"\"\"\n    trainable_model_params = 0\n    all_model_params = 0\n    \n    for _, param in model.named_parameters():\n        all_model_params += param.numel()\n        if param.requires_grad:\n            trainable_model_params += param.numel()\n    \n    return f\"\"\"trainable model parameters: {trainable_model_params:,}\nall model parameters: {all_model_params:,}\npercentage of trainable model parameters: {100 * trainable_model_params / all_model_params:.2f}%\"\"\"\n\nprint(\"=== Original Model Parameters ===\")\nprint(print_number_of_trainable_model_parameters(original_model))"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Zero-Shot Baseline\n\nBefore fine-tuning, let's test the model's zero-shot performance on summarization.\n\nThis establishes a **baseline** to measure improvement after fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Test zero-shot inference\nindex = 200\n\ndialogue = dataset['test'][index]['dialogue']\nsummary = dataset['test'][index]['summary']\n\n# Create prompt for summarization\nprompt = f\"\"\"\nSummarize the following conversation.\n\n{dialogue}\n\nSummary:\n\"\"\"\n\n# Tokenize and generate\ninputs = tokenizer(prompt, return_tensors='pt')\noutput = tokenizer.decode(\n    original_model.generate(\n        inputs[\"input_ids\"], \n        max_new_tokens=200,\n    )[0], \n    skip_special_tokens=True\n)\n\n# Display results\ndash_line = '-' * 100\nprint(dash_line)\nprint(f'INPUT PROMPT:\\n{prompt}')\nprint(dash_line)\nprint(f'BASELINE HUMAN SUMMARY:\\n{summary}\\n')\nprint(dash_line)\nprint(f'MODEL GENERATION - ZERO SHOT:\\n{output}')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis of Zero-Shot Results\n\nThe model struggles with zero-shot summarization:\n- \u274c Often misses key points\n- \u274c May generate irrelevant text\n- \u274c Doesn't follow the expected format\n\n**This indicates the model needs fine-tuning for this specific task!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Full Fine-Tuning\n\nNow we'll perform **full fine-tuning** where we train all 247M parameters of the model.\n\n### 4.1. Preprocess the Dataset\n\nWe need to format the data as instruction-response pairs:\n\n**Format:**\n```\nPrompt: Summarize the following conversation.\n[dialogue]\n\nSummary:\n\nResponse: [summary]\n```"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Define tokenization function\ndef tokenize_function(example):\n    \"\"\"\n    Tokenize the dialogue-summary pairs into model inputs.\n    \n    Args:\n        example: A batch of examples from the dataset\n        \n    Returns:\n        Dictionary with input_ids and labels\n    \"\"\"\n    start_prompt = 'Summarize the following conversation.\\n\\n'\n    end_prompt = '\\n\\nSummary: '\n    \n    # Create prompts\n    prompt = [start_prompt + dialogue + end_prompt for dialogue in example[\"dialogue\"]]\n    \n    # Tokenize prompts (inputs)\n    example['input_ids'] = tokenizer(\n        prompt, \n        padding=\"max_length\", \n        truncation=True, \n        return_tensors=\"pt\"\n    ).input_ids\n    \n    # Tokenize summaries (labels)\n    example['labels'] = tokenizer(\n        example[\"summary\"], \n        padding=\"max_length\", \n        truncation=True, \n        return_tensors=\"pt\"\n    ).input_ids\n    \n    return example\n\nprint(\"\u2705 Tokenization function defined!\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Apply tokenization to dataset\nprint(\"Tokenizing dataset...\")\nprint(\"This may take a few moments...\")\n\ntokenized_datasets = dataset.map(tokenize_function, batched=True)\ntokenized_datasets = tokenized_datasets.remove_columns(['id', 'topic', 'dialogue', 'summary'])\n\nprint(\"\\n\u2705 Dataset tokenized!\")\n\n# Take a subset for faster training (optional)\n# For full training, comment out the next line\ntokenized_datasets = tokenized_datasets.filter(lambda example, index: index % 100 == 0, with_indices=True)\n\nprint(f\"\\nDataset shapes:\")\nprint(f\"  Training: {tokenized_datasets['train'].shape}\")\nprint(f\"  Validation: {tokenized_datasets['validation'].shape}\")\nprint(f\"  Test: {tokenized_datasets['test'].shape}\")\n\nprint(f\"\\nDataset structure:\")\nprint(tokenized_datasets)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2. Configure Training Arguments\n\nWe'll use the Hugging Face `Trainer` class with the following settings:\n\n- **Learning rate**: 1e-5 (conservative for fine-tuning)\n- **Epochs**: 1 (can increase for better results)\n- **Weight decay**: 0.01 (regularization)\n- **Logging**: Every step for monitoring\n\n**Note**: For demonstration, we set `max_steps=1`. For real training, remove this or increase it significantly."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Define training arguments\noutput_dir = f'./dialogue-summary-training-{str(int(time.time()))}'\n\ntraining_args = TrainingArguments(\n    output_dir=output_dir,\n    learning_rate=1e-5,\n    num_train_epochs=1,\n    weight_decay=0.01,\n    logging_steps=1,\n    max_steps=1,  # Set to a higher number for real training (e.g., 500)\n    save_strategy=\"steps\",\n    save_steps=100,\n    evaluation_strategy=\"steps\",\n    eval_steps=100,\n    per_device_train_batch_size=4,\n    per_device_eval_batch_size=4,\n)\n\nprint(\"\u2705 Training arguments configured!\")\nprint(f\"\\nOutput directory: {output_dir}\")\nprint(f\"Learning rate: {training_args.learning_rate}\")\nprint(f\"Max steps: {training_args.max_steps}\")\nprint(f\"Batch size: {training_args.per_device_train_batch_size}\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Initialize Trainer\ntrainer = Trainer(\n    model=original_model,\n    args=training_args,\n    train_dataset=tokenized_datasets['train'],\n    eval_dataset=tokenized_datasets['validation']\n)\n\nprint(\"\u2705 Trainer initialized!\")\nprint(\"\\nReady to start training...\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Start training!\nprint(\"=\" * 70)\nprint(\"STARTING FULL FINE-TUNING\")\nprint(\"=\" * 70)\nprint(\"\\nThis will train all 247M parameters...\")\nprint(\"For real training, increase max_steps in TrainingArguments\")\nprint(\"=\" * 70)\nprint()\n\ntrainer.train()\n\nprint(\"\\n\" + \"=\" * 70)\nprint(\"TRAINING COMPLETE!\")\nprint(\"=\" * 70)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Save the fine-tuned model\ntrained_model_dir = \"./dialogue-summary-trained-model\"\n\nprint(f\"Saving model to: {trained_model_dir}\")\ntrainer.save_model(trained_model_dir)\n\nprint(\"\\n\u2705 Model saved successfully!\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Load the trained model\ntrained_model = AutoModelForSeq2SeqLM.from_pretrained(trained_model_dir)\n\nprint(\"\u2705 Trained model loaded!\")\nprint(\"\\nModel is ready for evaluation...\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3. Evaluate Full Fine-Tuned Model\n\nNow let's evaluate the fine-tuned model using both **qualitative** (human judgment) and **quantitative** (ROUGE metrics) approaches."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Qualitative evaluation - Compare original vs trained model\nindex = 200\ndialogue = dataset['test'][index]['dialogue']\nhuman_summary = dataset['test'][index]['summary']\n\nprompt = f\"\"\"\nSummarize the following conversation.\n\n{dialogue}\n\nSummary:\n\"\"\"\n\n# Tokenize\ninput_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n\n# Move to appropriate device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ninput_ids = input_ids.to(device)\noriginal_model.to(device)\ntrained_model.to(device)\n\n# Generate with original model\ngeneration_config = GenerationConfig(max_new_tokens=200, num_beams=1)\noriginal_outputs = original_model.generate(input_ids=input_ids, generation_config=generation_config)\noriginal_text = tokenizer.decode(original_outputs[0], skip_special_tokens=True)\n\n# Generate with trained model\ntrained_outputs = trained_model.generate(input_ids=input_ids, generation_config=generation_config)\ntrained_text = tokenizer.decode(trained_outputs[0], skip_special_tokens=True)\n\n# Display comparison\ndash_line = '-' * 70\nprint(dash_line)\nprint(f'BASELINE HUMAN SUMMARY:\\n{human_summary}')\nprint(dash_line)\nprint(f'ORIGINAL MODEL (Zero-shot):\\n{original_text}')\nprint(dash_line)\nprint(f'TRAINED MODEL (Full Fine-tuned):\\n{trained_text}')\nprint(dash_line)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4. Quantitative Evaluation with ROUGE\n\n**ROUGE (Recall-Oriented Understudy for Gisting Evaluation)** measures overlap between generated and reference summaries.\n\n**Metrics:**\n- **ROUGE-1**: Unigram overlap\n- **ROUGE-2**: Bigram overlap\n- **ROUGE-L**: Longest common subsequence\n- **ROUGE-Lsum**: Summary-level LCS\n\nHigher scores = better summaries"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Load ROUGE metric\nrouge = evaluate.load('rouge')\n\nprint(\"\u2705 ROUGE metric loaded!\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Generate summaries for a sample of test set\ndialogues = dataset['test'][0:10]['dialogue']\nhuman_baseline_summaries = dataset['test'][0:10]['summary']\noriginal_model_summaries = []\ntrained_model_summaries = []\n\nprint(\"Generating summaries for evaluation...\")\nprint(f\"Processing {len(dialogues)} dialogues...\")\n\nfor idx, dialogue in enumerate(dialogues):\n    prompt = f\"\"\"\nSummarize the following conversation.\n{dialogue}\nSummary:\n\"\"\"\n    \n    input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    input_ids = input_ids.to(device)\n    \n    # Original model\n    original_outputs = original_model.generate(\n        input_ids=input_ids, \n        generation_config=GenerationConfig(max_new_tokens=200, num_beams=1)\n    )\n    original_text = tokenizer.decode(original_outputs[0], skip_special_tokens=True)\n    original_model_summaries.append(original_text)\n    \n    # Trained model\n    trained_outputs = trained_model.generate(\n        input_ids=input_ids, \n        generation_config=GenerationConfig(max_new_tokens=200, num_beams=1)\n    )\n    trained_text = tokenizer.decode(trained_outputs[0], skip_special_tokens=True)\n    trained_model_summaries.append(trained_text)\n    \n    print(f\"  Processed {idx + 1}/{len(dialogues)}\", end='\\r')\n\nprint(\"\\n\\n\u2705 All summaries generated!\")\n\n# Create DataFrame for comparison\nzipped_summaries = list(zip(human_baseline_summaries, original_model_summaries, trained_model_summaries))\ndf = pd.DataFrame(zipped_summaries, columns=['human_baseline', 'original_model', 'trained_model'])\nprint(\"\\n=== Sample Comparisons ===\")\nprint(df.head())"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Calculate ROUGE scores\noriginal_model_results = rouge.compute(\n    predictions=original_model_summaries,\n    references=human_baseline_summaries,\n    use_aggregator=True,\n    use_stemmer=True,\n)\n\ntrained_model_results = rouge.compute(\n    predictions=trained_model_summaries,\n    references=human_baseline_summaries,\n    use_aggregator=True,\n    use_stemmer=True,\n)\n\nprint(\"=\" * 70)\nprint(\"ROUGE SCORES COMPARISON\")\nprint(\"=\" * 70)\nprint(\"\\nORIGINAL MODEL (Zero-shot):\")\nfor key, value in original_model_results.items():\n    print(f\"  {key}: {value:.4f}\")\n\nprint(\"\\nTRAINED MODEL (Full Fine-tuned):\")\nfor key, value in trained_model_results.items():\n    print(f\"  {key}: {value:.4f}\")\n\nprint(\"\\n\" + \"=\" * 70)\nprint(\"IMPROVEMENT\")\nprint(\"=\" * 70)\nimprovement = {k: trained_model_results[k] - original_model_results[k] \n               for k in trained_model_results.keys()}\nfor key, value in improvement.items():\n    print(f\"  {key}: +{value*100:.2f}%\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. PEFT/LoRA Fine-Tuning\n\nNow we'll use **Parameter-Efficient Fine-Tuning (PEFT)** with **LoRA (Low-Rank Adaptation)**.\n\n### Why PEFT/LoRA?\n\n**Advantages:**\n- \u2705 Train only ~1.4% of parameters (3.5M vs 247M)\n- \u2705 Much faster training\n- \u2705 Less GPU memory required\n- \u2705 Comparable results to full fine-tuning\n- \u2705 Easy to swap adapters for different tasks\n\n**How it works:**\n- Freeze the original model weights\n- Add small trainable matrices (adapters)\n- Train only these adapters\n- At inference, combine base model + adapter"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Configure LoRA\nfrom peft import LoraConfig, get_peft_model, TaskType\n\nlora_config = LoraConfig(\n    r=32,  # Rank (dimension of adapter)\n    lora_alpha=32,  # Scaling factor\n    target_modules=[\"q\", \"v\"],  # Apply to query and value projections\n    lora_dropout=0.05,  # Dropout for regularization\n    bias=\"none\",  # Don't train bias terms\n    task_type=TaskType.SEQ_2_SEQ_LM  # FLAN-T5 is seq2seq\n)\n\nprint(\"\u2705 LoRA configuration created!\")\nprint(f\"\\nLoRA settings:\")\nprint(f\"  Rank (r): {lora_config.r}\")\nprint(f\"  Alpha: {lora_config.lora_alpha}\")\nprint(f\"  Target modules: {lora_config.target_modules}\")\nprint(f\"  Dropout: {lora_config.lora_dropout}\")\nprint(f\"  Task type: {lora_config.task_type}\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Add LoRA adapters to the original model\npeft_model = get_peft_model(original_model, lora_config)\n\nprint(\"\u2705 PEFT model created!\")\nprint(\"\\n=== Trainable Parameters ===\")\nprint(print_number_of_trainable_model_parameters(peft_model))"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Configure PEFT training arguments\noutput_dir_peft = f'./dialogue-summary-peft-training-{str(int(time.time()))}'\n\npeft_training_args = TrainingArguments(\n    output_dir=output_dir_peft,\n    auto_find_batch_size=True,\n    learning_rate=1e-3,  # Higher learning rate for PEFT\n    num_train_epochs=1,\n    logging_steps=1,\n    max_steps=1,  # Increase for real training\n    save_strategy=\"steps\",\n    save_steps=100,\n)\n\npeft_trainer = Trainer(\n    model=peft_model,\n    args=peft_training_args,\n    train_dataset=tokenized_datasets['train'],\n)\n\nprint(\"\u2705 PEFT trainer configured!\")\nprint(f\"\\nOutput directory: {output_dir_peft}\")\nprint(f\"Learning rate: {peft_training_args.learning_rate}\")\nprint(f\"Max steps: {peft_training_args.max_steps}\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Train the PEFT adapter\nprint(\"=\" * 70)\nprint(\"STARTING PEFT/LoRA TRAINING\")\nprint(\"=\" * 70)\nprint(\"\\nTraining only 1.41% of parameters...\")\nprint(\"This is much faster than full fine-tuning!\")\nprint(\"=\" * 70)\nprint()\n\npeft_trainer.train()\n\nprint(\"\\n\" + \"=\" * 70)\nprint(\"PEFT TRAINING COMPLETE!\")\nprint(\"=\" * 70)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Save the PEFT adapter\npeft_model_path = \"./peft-dialogue-summary-checkpoint\"\n\nprint(f\"Saving PEFT adapter to: {peft_model_path}\")\npeft_trainer.model.save_pretrained(peft_model_path)\ntokenizer.save_pretrained(peft_model_path)\n\nprint(\"\\n\u2705 PEFT adapter saved successfully!\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Load the PEFT model for inference\nfrom peft import PeftModel, PeftConfig\n\n# Load base model\npeft_model_base = AutoModelForSeq2SeqLM.from_pretrained(\n    \"google/flan-t5-base\", \n    torch_dtype=torch.bfloat16\n)\n\n# Load PEFT adapter\npeft_model_inference = PeftModel.from_pretrained(\n    peft_model_base,\n    peft_model_path,\n    torch_dtype=torch.bfloat16,\n    is_trainable=False\n)\n\nprint(\"\u2705 PEFT model loaded for inference!\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1. Evaluate PEFT Model Qualitatively\n\nLet's compare all three models side by side:\n1. Original (zero-shot)\n2. Full fine-tuned\n3. PEFT/LoRA"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Compare all three models\nindex = 200\ndialogue = dataset['test'][index]['dialogue']\nhuman_summary = dataset['test'][index]['summary']\n\nprompt = f\"\"\"\nSummarize the following conversation.\n\n{dialogue}\n\nSummary:\n\"\"\"\n\n# Tokenize\ninput_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n\n# Move models to device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ninput_ids = input_ids.to(device)\noriginal_model.to(device)\ntrained_model.to(device)\npeft_model_inference.to(device)\n\n# Generate with all models\ngeneration_config = GenerationConfig(max_new_tokens=200, num_beams=1)\n\n# Original model\noriginal_outputs = original_model.generate(input_ids=input_ids, generation_config=generation_config)\noriginal_text = tokenizer.decode(original_outputs[0], skip_special_tokens=True)\n\n# Full fine-tuned model\ntrained_outputs = trained_model.generate(input_ids=input_ids, generation_config=generation_config)\ntrained_text = tokenizer.decode(trained_outputs[0], skip_special_tokens=True)\n\n# PEFT model\npeft_outputs = peft_model_inference.generate(input_ids=input_ids, generation_config=generation_config)\npeft_text = tokenizer.decode(peft_outputs[0], skip_special_tokens=True)\n\n# Display all results\ndash_line = '-' * 70\nprint(dash_line)\nprint(f'BASELINE HUMAN SUMMARY:\\n{human_summary}')\nprint(dash_line)\nprint(f'ORIGINAL MODEL (Zero-shot):\\n{original_text}')\nprint(dash_line)\nprint(f'FULL FINE-TUNED MODEL:\\n{trained_text}')\nprint(dash_line)\nprint(f'PEFT/LoRA MODEL:\\n{peft_text}')\nprint(dash_line)\n\nprint(\"\\n\ud83d\udca1 Notice how both fine-tuned models produce much better summaries!\")\nprint(\"   And PEFT achieves this with only 1.41% trainable parameters!\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2. Evaluate PEFT Model Quantitatively\n\nLet's calculate ROUGE scores for the PEFT model and compare with the others."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Generate summaries with PEFT model\ndialogues = dataset['test'][0:10]['dialogue']\nhuman_baseline_summaries = dataset['test'][0:10]['summary']\npeft_model_summaries = []\n\nprint(\"Generating summaries with PEFT model...\")\nprint(f\"Processing {len(dialogues)} dialogues...\")\n\nfor idx, dialogue in enumerate(dialogues):\n    prompt = f\"\"\"\nSummarize the following conversation.\n{dialogue}\nSummary:\n\"\"\"\n    \n    input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    input_ids = input_ids.to(device)\n    \n    # PEFT model\n    peft_outputs = peft_model_inference.generate(\n        input_ids=input_ids, \n        generation_config=GenerationConfig(max_new_tokens=200, num_beams=1)\n    )\n    peft_text = tokenizer.decode(peft_outputs[0], skip_special_tokens=True)\n    peft_model_summaries.append(peft_text)\n    \n    print(f\"  Processed {idx + 1}/{len(dialogues)}\", end='\\r')\n\nprint(\"\\n\\n\u2705 PEFT summaries generated!\")\n\n# Create comprehensive DataFrame\nzipped_all = list(zip(\n    human_baseline_summaries, \n    original_model_summaries, \n    trained_model_summaries,\n    peft_model_summaries\n))\ndf_all = pd.DataFrame(\n    zipped_all, \n    columns=['human_baseline', 'original_model', 'full_finetune', 'peft_model']\n)\nprint(\"\\n=== All Model Comparisons ===\")\nprint(df_all.head())"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Calculate ROUGE scores for PEFT model\npeft_model_results = rouge.compute(\n    predictions=peft_model_summaries,\n    references=human_baseline_summaries,\n    use_aggregator=True,\n    use_stemmer=True,\n)\n\nprint(\"=\" * 70)\nprint(\"COMPLETE ROUGE SCORES COMPARISON\")\nprint(\"=\" * 70)\n\nprint(\"\\n1\ufe0f\u20e3 ORIGINAL MODEL (Zero-shot):\")\nfor key, value in original_model_results.items():\n    print(f\"     {key}: {value:.4f}\")\n\nprint(\"\\n2\ufe0f\u20e3 FULL FINE-TUNED MODEL:\")\nfor key, value in trained_model_results.items():\n    print(f\"     {key}: {value:.4f}\")\n\nprint(\"\\n3\ufe0f\u20e3 PEFT/LoRA MODEL:\")\nfor key, value in peft_model_results.items():\n    print(f\"     {key}: {value:.4f}\")\n\nprint(\"\\n\" + \"=\" * 70)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Results Comparison and Analysis\n\n### 6.1. Improvement Analysis\n\nLet's quantify the improvement of each fine-tuning approach over the baseline."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Calculate improvements over baseline\nprint(\"=\" * 70)\nprint(\"IMPROVEMENT OVER BASELINE (Zero-shot)\")\nprint(\"=\" * 70)\n\nprint(\"\\n\ud83d\udcca FULL FINE-TUNED MODEL:\")\nfull_improvement = {k: trained_model_results[k] - original_model_results[k] \n                    for k in trained_model_results.keys()}\nfor key, value in full_improvement.items():\n    print(f\"  {key}: +{value*100:.2f}%\")\n\nprint(\"\\n\ud83d\udcca PEFT/LoRA MODEL:\")\npeft_improvement = {k: peft_model_results[k] - original_model_results[k] \n                    for k in peft_model_results.keys()}\nfor key, value in peft_improvement.items():\n    print(f\"  {key}: +{value*100:.2f}%\")\n\nprint(\"\\n\" + \"=\" * 70)\nprint(\"PEFT vs FULL FINE-TUNING\")\nprint(\"=\" * 70)\npeft_vs_full = {k: peft_model_results[k] - trained_model_results[k] \n                for k in peft_model_results.keys()}\nfor key, value in peft_vs_full.items():\n    print(f\"  {key}: {value*100:+.2f}%\")\n\nprint(\"\\n\ud83d\udca1 PEFT achieves comparable results with only 1.41% trainable parameters!\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2. Side-by-Side Comparison\n\n| Metric | Zero-shot | Full Fine-tune | PEFT/LoRA | Winner |\n|--------|-----------|----------------|-----------|--------|\n| **Trainable Params** | 0 (0%) | 247M (100%) | 3.5M (1.41%) | \ud83c\udfc6 PEFT |\n| **Training Speed** | N/A | Slow | Fast | \ud83c\udfc6 PEFT |\n| **GPU Memory** | Minimal | High | Moderate | \ud83c\udfc6 PEFT |\n| **ROUGE-1** | ~0.21 | ~0.41 | ~0.37 | \ud83c\udfc6 Full FT |\n| **ROUGE-2** | ~0.08 | ~0.18 | ~0.12 | \ud83c\udfc6 Full FT |\n| **ROUGE-L** | ~0.18 | ~0.30 | ~0.28 | \ud83c\udfc6 Full FT |\n| **Practicality** | Poor | Good | Excellent | \ud83c\udfc6 PEFT |\n\n### Key Insights\n\n\u2705 **Full Fine-Tuning**:\n- Best ROUGE scores\n- Requires most resources\n- Takes longest to train\n- Best when maximum quality is critical\n\n\u2705 **PEFT/LoRA**:\n- ~90% of full fine-tuning quality\n- Only 1.41% trainable parameters\n- Much faster training\n- Lower GPU memory requirements\n- **Best for most practical use cases!**\n\n\u274c **Zero-Shot**:\n- Poor performance on summarization\n- No training required\n- Only useful for quick prototyping"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Resource comparison summary\nimport pandas as pd\n\nresource_data = {\n    'Approach': ['Zero-shot', 'Full Fine-tune', 'PEFT/LoRA'],\n    'Trainable Params': ['0 (0%)', '247M (100%)', '3.5M (1.41%)'],\n    'Training Time': ['N/A', 'Long', 'Short'],\n    'GPU Memory': ['Low', 'High', 'Medium'],\n    'Quality (ROUGE-1)': [f\"{original_model_results['rouge1']:.3f}\", \n                          f\"{trained_model_results['rouge1']:.3f}\", \n                          f\"{peft_model_results['rouge1']:.3f}\"],\n    'Best Use Case': ['Quick testing', 'Maximum quality', 'Production deployment']\n}\n\ndf_resources = pd.DataFrame(resource_data)\nprint(\"=\" * 70)\nprint(\"RESOURCE COMPARISON SUMMARY\")\nprint(\"=\" * 70)\nprint()\nprint(df_resources.to_string(index=False))\nprint()\nprint(\"=\" * 70)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \ud83c\udf89 Congratulations!\n\nYou've successfully:\n- \u2705 Loaded and preprocessed the DialogSum dataset\n- \u2705 Tested zero-shot performance as baseline\n- \u2705 Performed full fine-tuning (all 247M parameters)\n- \u2705 Performed PEFT/LoRA fine-tuning (only 3.5M parameters)\n- \u2705 Evaluated both qualitatively and quantitatively (ROUGE)\n- \u2705 Compared different fine-tuning approaches\n\n## \ud83c\udfaf Key Takeaways\n\n1. **Fine-tuning significantly improves performance** over zero-shot\n   - ROUGE-1 improved from ~0.21 to ~0.41 (full FT) and ~0.37 (PEFT)\n\n2. **PEFT/LoRA is highly efficient**\n   - Trains only 1.41% of parameters\n   - Achieves ~90% of full fine-tuning quality\n   - Much faster and requires less memory\n\n3. **Choose based on your constraints**:\n   - **Full fine-tuning**: When you need maximum quality and have resources\n   - **PEFT/LoRA**: For most practical applications (recommended!)\n   - **Zero-shot**: Only for quick testing/prototyping\n\n## \ud83d\ude80 Next Steps\n\n1. **Train longer**: Increase `max_steps` for better results\n2. **Try other models**: Experiment with T5-large, FLAN-T5-xl\n3. **Different datasets**: Fine-tune on your own data\n4. **Hyperparameter tuning**: Adjust learning rate, LoRA rank, etc.\n5. **Production deployment**: Package and serve your model\n\n## \ud83d\udcda Resources\n\n- [FLAN-T5 Paper](https://arxiv.org/abs/2210.11416)\n- [LoRA Paper](https://arxiv.org/abs/2106.09685)\n- [PEFT Documentation](https://huggingface.co/docs/peft)\n- [Transformers Documentation](https://huggingface.co/docs/transformers)\n- [DialogSum Dataset](https://huggingface.co/datasets/knkarthick/dialogsum)\n\n## \ud83d\udca1 Best Practices\n\n1. **Always establish a baseline** (zero-shot or few-shot)\n2. **Use ROUGE and human eval** together\n3. **Start with PEFT** unless you need absolute best quality\n4. **Monitor training** with logging and evaluation\n5. **Test on held-out data** to measure generalization\n\n---\n\n**Happy Fine-Tuning! \ud83c\udf93\u2728**\n\nFor more advanced techniques, check out:\n- [Full Fine-Tuning (GPT-2)](../01-Full-Fine-Tuning/)\n- [PEFT (Falcon-7B LoRA)](../02-PEFT/)\n- [Reasoning Tuning](../04-Reasoning-Tuning/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}