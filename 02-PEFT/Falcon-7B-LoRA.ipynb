{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tune Falcon-7B with LoRA: A Step-by-Step Guide\n\nThis notebook demonstrates how to efficiently fine-tune the Falcon-7B language model using **LoRA (Low-Rank Adaptation)** and **4-bit quantization** (QLoRA).\n\n## \ud83d\ude80 What You'll Learn\n\n- How to use **4-bit quantization** to reduce memory requirements\n- How to apply **LoRA** for parameter-efficient fine-tuning\n- How to use the **SFTTrainer** from the TRL library\n- How to fine-tune on the **Guanaco dataset** for instruction following\n- How to monitor training and save checkpoints\n\n## \ud83c\udfaf Why This Approach?\n\n- **Memory Efficient**: 4-bit quantization reduces GPU memory by ~75%\n- **Fast Training**: LoRA only trains ~0.1% of parameters\n- **High Quality**: Achieves results comparable to full fine-tuning\n- **Practical**: Works on consumer GPUs (RTX 3090, 4090, etc.)\n\n## \ud83d\udd27 Requirements\n\n- **GPU**: 16GB+ VRAM (RTX 3090, 4090, A100, etc.)\n- **Time**: 2-4 hours for 200 steps\n- **Storage**: ~15GB for model and checkpoints\n\n## \ud83d\udcd6 Table of Contents\n\n1. [Setup and Authentication](#1-setup-and-authentication)\n2. [Install Required Packages](#2-install-required-packages)\n3. [Load and Explore Dataset](#3-load-and-explore-dataset)\n4. [Load Model with 4-bit Quantization](#4-load-model-with-4-bit-quantization)\n5. [Configure LoRA](#5-configure-lora)\n6. [Set Training Arguments](#6-set-training-arguments)\n7. [Initialize Trainer and Train](#7-initialize-trainer-and-train)\n8. [Inference and Testing](#8-inference-and-testing)\n\n---\n\n**Note**: This notebook can run on Google Colab, Jupyter Lab, or any notebook environment with GPU support."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Authentication\n\nFirst, we'll authenticate with Hugging Face to access models and datasets.\n\n**You'll need a Hugging Face account**:\n1. Sign up at [huggingface.co](https://huggingface.co)\n2. Get your access token from [Settings > Access Tokens](https://huggingface.co/settings/tokens)\n3. Create a token with `write` permissions"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Login to Hugging Face\n# This will prompt you to enter your token\nfrom huggingface_hub import notebook_login\n\nnotebook_login()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Install Required Packages\n\nWe'll install the necessary libraries:\n\n- **transformers**: Core library for working with LLMs\n- **peft**: Parameter-Efficient Fine-Tuning library (includes LoRA)\n- **trl**: Transformer Reinforcement Learning library (includes SFTTrainer)\n- **accelerate**: For optimized training\n- **bitsandbytes**: For 4-bit/8-bit quantization\n- **datasets**: For loading datasets\n- **einops**: Tensor operations\n- **wandb**: For experiment tracking (optional)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Install required packages\n# This may take a few minutes\n!pip install -q -U trl transformers accelerate git+https://github.com/huggingface/peft.git\n!pip install -q datasets einops wandb\n!pip install -q bitsandbytes==0.43.1\n\nprint(\"\u2705 All packages installed successfully!\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Import necessary libraries\nimport torch\nfrom transformers import (\n    AutoModelForCausalLM, \n    AutoTokenizer, \n    BitsAndBytesConfig,\n    TrainingArguments\n)\nfrom peft import LoraConfig\nfrom trl import SFTTrainer\nfrom datasets import load_dataset\n\n# Check GPU availability\nprint(f\"PyTorch version: {torch.__version__}\")\nprint(f\"CUDA available: {torch.cuda.is_available()}\")\nif torch.cuda.is_available():\n    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")\n    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load and Explore Dataset\n\nWe'll use the **OpenAssistant Guanaco** dataset, which contains high-quality instruction-response pairs.\n\n### About the Dataset\n\n- **Name**: `timdettmers/openassistant-guanaco`\n- **Type**: Instruction-following conversations\n- **Size**: ~10K examples\n- **Format**: Text field contains instruction and response\n- **Use Case**: Training instruction-following models"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Load the Guanaco dataset\ndataset_name = \"timdettmers/openassistant-guanaco\"\ndataset = load_dataset(dataset_name, split=\"train\")\n\nprint(f\"\u2705 Dataset loaded successfully!\")\nprint(f\"\\nDataset size: {len(dataset):,} examples\")\nprint(f\"\\nDataset features:\")\nprint(dataset.features)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Let's examine a few samples from the dataset\nprint(\"=== Sample 1 ===\")\nprint(dataset[0]['text'])\nprint(\"\\n\" + \"=\"*70 + \"\\n\")\n\nprint(\"=== Sample 2 ===\")\nprint(dataset[1]['text'])\nprint(\"\\n\" + \"=\"*70 + \"\\n\")\n\nprint(\"=== Sample 3 ===\")\nprint(dataset[2]['text'])"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Get dataset statistics\nimport numpy as np\n\n# Calculate text lengths\ntext_lengths = [len(sample['text']) for sample in dataset]\n\nprint(\"=== Dataset Statistics ===\")\nprint(f\"Total samples: {len(dataset):,}\")\nprint(f\"\\nText length statistics (characters):\")\nprint(f\"  Mean: {np.mean(text_lengths):.0f}\")\nprint(f\"  Median: {np.median(text_lengths):.0f}\")\nprint(f\"  Min: {np.min(text_lengths)}\")\nprint(f\"  Max: {np.max(text_lengths)}\")\nprint(f\"  Std: {np.std(text_lengths):.0f}\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Load Model with 4-bit Quantization\n\nWe'll load Falcon-7B with **4-bit quantization** to drastically reduce memory usage.\n\n### What is 4-bit Quantization?\n\n- **Reduces model size** from ~14GB to ~4GB\n- **Enables training on consumer GPUs** (16GB+ VRAM)\n- **Minimal quality loss** with NF4 (NormalFloat4) quantization\n- **Uses bitsandbytes** library for efficient computation\n\n### BitsAndBytes Configuration\n\n- **load_in_4bit**: Load model weights in 4-bit precision\n- **bnb_4bit_quant_type**: Use \"nf4\" (NormalFloat4) - best for LLMs\n- **bnb_4bit_compute_dtype**: Use float16 for computations"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Configure 4-bit quantization\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,                    # Enable 4-bit loading\n    bnb_4bit_quant_type=\"nf4\",            # Use NormalFloat4 quantization\n    bnb_4bit_compute_dtype=torch.float16, # Compute in float16\n)\n\nprint(\"\u2705 Quantization config created:\")\nprint(f\"  - 4-bit loading: {bnb_config.load_in_4bit}\")\nprint(f\"  - Quantization type: {bnb_config.bnb_4bit_quant_type}\")\nprint(f\"  - Compute dtype: {bnb_config.bnb_4bit_compute_dtype}\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Load Falcon-7B with 4-bit quantization\nmodel_name = \"ybelkada/falcon-7b-sharded-bf16\"\n\nprint(f\"Loading {model_name}...\")\nprint(\"This may take a few minutes on first load...\")\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    quantization_config=bnb_config,\n    trust_remote_code=True,  # Falcon requires custom code\n    device_map=\"auto\",       # Automatically distribute across GPUs\n)\n\n# Disable cache for training (saves memory)\nmodel.config.use_cache = False\n\nprint(\"\\n\u2705 Model loaded successfully!\")\nprint(f\"Model: {model_name}\")\nprint(f\"Device map: {model.hf_device_map}\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Display model information\ndef count_parameters(model):\n    \"\"\"Count total and trainable parameters\"\"\"\n    total_params = sum(p.numel() for p in model.parameters())\n    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n    return total_params, trainable_params\n\ntotal, trainable = count_parameters(model)\n\nprint(\"=== Model Information ===\")\nprint(f\"Total parameters: {total:,} ({total/1e9:.2f}B)\")\nprint(f\"Trainable parameters (before LoRA): {trainable:,}\")\nprint(f\"\\nModel architecture:\")\nprint(model)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Load tokenizer\ntokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n\n# Set padding token (Falcon doesn't have one by default)\ntokenizer.pad_token = tokenizer.eos_token\n\nprint(\"\u2705 Tokenizer loaded successfully!\")\nprint(f\"\\nVocabulary size: {len(tokenizer):,}\")\nprint(f\"EOS token: {tokenizer.eos_token} (ID: {tokenizer.eos_token_id})\")\nprint(f\"PAD token: {tokenizer.pad_token} (ID: {tokenizer.pad_token_id})\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Test the tokenizer\ntest_text = \"Hello! How can I assist you today?\"\ntokens = tokenizer(test_text, return_tensors=\"pt\")\n\nprint(f\"Original text: {test_text}\")\nprint(f\"\\nTokenized IDs: {tokens['input_ids'][0].tolist()}\")\nprint(f\"Number of tokens: {len(tokens['input_ids'][0])}\")\nprint(f\"\\nDecoded back: {tokenizer.decode(tokens['input_ids'][0])}\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Configure LoRA\n\nNow we'll configure **LoRA (Low-Rank Adaptation)** for efficient fine-tuning.\n\n### What is LoRA?\n\nLoRA injects trainable low-rank matrices into model layers, allowing us to:\n- Train only ~0.1% of the model's parameters\n- Achieve results comparable to full fine-tuning\n- Train much faster with less memory\n\n### LoRA Hyperparameters\n\n- **r (rank)**: Dimensionality of low-rank matrices (4-64 typical)\n  - Higher = more capacity but slower\n  - We use **64** for good quality\n  \n- **lora_alpha**: Scaling factor for LoRA weights\n  - Usually 2\u00d7r or 1\u00d7r\n  - We use **16**\n\n- **lora_dropout**: Dropout for regularization\n  - We use **0.1** (10%)\n\n- **target_modules**: Which layers to apply LoRA to\n  - For Falcon: `query_key_value`, `dense`, `dense_h_to_4h`, `dense_4h_to_h`"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Configure LoRA\nlora_alpha = 16\nlora_dropout = 0.1\nlora_r = 64\n\npeft_config = LoraConfig(\n    lora_alpha=lora_alpha,\n    lora_dropout=lora_dropout,\n    r=lora_r,\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\",\n    target_modules=[\n        \"query_key_value\",  # Attention projection\n        \"dense\",            # Attention output\n        \"dense_h_to_4h\",    # MLP up-projection\n        \"dense_4h_to_h\",    # MLP down-projection\n    ]\n)\n\nprint(\"\u2705 LoRA configuration created:\")\nprint(f\"  - Rank (r): {lora_r}\")\nprint(f\"  - Alpha: {lora_alpha}\")\nprint(f\"  - Dropout: {lora_dropout}\")\nprint(f\"  - Target modules: {peft_config.target_modules}\")\nprint(f\"  - Task type: {peft_config.task_type}\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Set Training Arguments\n\nWe'll configure the training parameters for optimal results.\n\n### Training Configuration\n\n**Batch Size & Accumulation:**\n- `per_device_train_batch_size=1`: Batch size per GPU\n- `gradient_accumulation_steps=1`: Accumulate gradients over N steps\n- Effective batch size = 1 \u00d7 1 = 1\n\n**Optimization:**\n- `learning_rate=2e-4`: Learning rate (2\u00d710\u207b\u2074)\n- `optim=\"paged_adamw_32bit\"`: Memory-efficient AdamW optimizer\n- `max_grad_norm=0.3`: Gradient clipping for stability\n- `warmup_ratio=0.03`: 3% of steps for learning rate warmup\n\n**Training Duration:**\n- `max_steps=200`: Total training steps\n- For longer training, increase to 500-1000 steps\n\n**Logging & Saving:**\n- `logging_steps=10`: Log metrics every 10 steps\n- `save_steps=10`: Save checkpoint every 10 steps\n\n**Mixed Precision:**\n- `fp16=True`: Use 16-bit floating point (faster, less memory)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Define training arguments\noutput_dir = \"./results\"\nper_device_train_batch_size = 1\ngradient_accumulation_steps = 1\noptim = \"paged_adamw_32bit\"\nsave_steps = 10\nlogging_steps = 10\nlearning_rate = 2e-4\nmax_grad_norm = 0.3\nmax_steps = 200\nwarmup_ratio = 0.03\nlr_scheduler_type = \"constant\"\n\ntraining_arguments = TrainingArguments(\n    output_dir=output_dir,\n    per_device_train_batch_size=per_device_train_batch_size,\n    gradient_accumulation_steps=gradient_accumulation_steps,\n    optim=optim,\n    save_steps=save_steps,\n    logging_steps=logging_steps,\n    learning_rate=learning_rate,\n    fp16=True,\n    max_grad_norm=max_grad_norm,\n    max_steps=max_steps,\n    warmup_ratio=warmup_ratio,\n    group_by_length=True,\n    lr_scheduler_type=lr_scheduler_type,\n)\n\nprint(\"\u2705 Training arguments configured:\")\nprint(f\"  - Output directory: {output_dir}\")\nprint(f\"  - Batch size: {per_device_train_batch_size}\")\nprint(f\"  - Gradient accumulation: {gradient_accumulation_steps}\")\nprint(f\"  - Learning rate: {learning_rate}\")\nprint(f\"  - Max steps: {max_steps}\")\nprint(f\"  - Optimizer: {optim}\")\nprint(f\"  - Mixed precision: FP16\")\nprint(f\"  - Save every: {save_steps} steps\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Initialize Trainer and Train\n\nWe'll use the **SFTTrainer** (Supervised Fine-Tuning Trainer) from the TRL library.\n\n### What is SFTTrainer?\n\n- Specialized trainer for supervised fine-tuning of LLMs\n- Handles tokenization automatically\n- Works seamlessly with PEFT (LoRA)\n- Includes helpful defaults for LLM training\n\n### Training Process\n\n1. Initialize SFTTrainer with model, dataset, and configs\n2. Convert normalization layers to float32 (for stability)\n3. Start training with `trainer.train()`\n4. Monitor progress via logging\n\n**Note**: Training 200 steps takes approximately 1-2 hours on an RTX 4090."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Initialize SFTTrainer\nmax_seq_length = 512  # Maximum sequence length for training\n\nprint(\"Initializing SFTTrainer...\")\n\ntrainer = SFTTrainer(\n    model=model,\n    train_dataset=dataset,\n    peft_config=peft_config,\n    dataset_text_field=\"text\",\n    max_seq_length=max_seq_length,\n    tokenizer=tokenizer,\n    args=training_arguments,\n)\n\nprint(\"\\n\u2705 Trainer initialized successfully!\")\nprint(f\"  - Max sequence length: {max_seq_length}\")\nprint(f\"  - Training samples: {len(dataset):,}\")\nprint(f\"  - Training steps: {max_steps}\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Check trainable parameters after LoRA injection\ndef print_trainable_parameters(model):\n    \"\"\"\n    Prints the number of trainable parameters in the model.\n    \"\"\"\n    trainable_params = 0\n    all_param = 0\n    for _, param in model.named_parameters():\n        all_param += param.numel()\n        if param.requires_grad:\n            trainable_params += param.numel()\n    print(\n        f\"trainable params: {trainable_params:,} || \"\n        f\"all params: {all_param:,} || \"\n        f\"trainable%: {100 * trainable_params / all_param:.4f}\"\n    )\n\nprint(\"=== LoRA Parameters ===\")\nprint_trainable_parameters(trainer.model)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Convert normalization layers to float32 for training stability\n# This is important when using mixed precision with quantized models\nfor name, module in trainer.model.named_modules():\n    if \"norm\" in name:\n        module = module.to(torch.float32)\n\nprint(\"\u2705 Normalization layers converted to float32 for stability\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Start training!\nprint(\"=\" * 70)\nprint(\"STARTING TRAINING\")\nprint(\"=\" * 70)\nprint(f\"\\nTraining for {max_steps} steps...\")\nprint(f\"Saving checkpoints every {save_steps} steps to: {output_dir}\")\nprint(\"\\nThis will take approximately 1-2 hours on an RTX 4090.\")\nprint(\"\\nYou can monitor progress via the logging output below.\")\nprint(\"=\" * 70)\nprint()\n\n# Train the model\ntrainer.train()\n\nprint(\"\\n\" + \"=\" * 70)\nprint(\"TRAINING COMPLETE!\")\nprint(\"=\" * 70)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Save the final trained model\nfinal_model_path = \"./falcon-7b-guanaco-lora-final\"\n\nprint(f\"Saving final model to: {final_model_path}\")\ntrainer.save_model(final_model_path)\n\nprint(\"\\n\u2705 Model saved successfully!\")\nprint(f\"\\nModel location: {final_model_path}\")\nprint(\"\\nYou can load this model later for inference.\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Inference and Testing\n\nNow let's test our fine-tuned model by generating responses!\n\n### Loading the Model\n\nWe'll load:\n1. The base Falcon-7B model (quantized)\n2. Our trained LoRA adapters\n\n### Generation Tips\n\n- **Temperature**: Controls randomness (0.1=focused, 1.0=creative)\n- **top_k**: Consider only top K tokens\n- **top_p**: Nucleus sampling threshold\n- **max_new_tokens**: Maximum tokens to generate"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Load the fine-tuned model for inference\nfrom peft import PeftModel, PeftConfig\n\n# Path to your saved model\nmodel_path = \"./falcon-7b-guanaco-lora-final\"\n\nprint(f\"Loading fine-tuned model from: {model_path}\")\nprint(\"This may take a moment...\")\n\n# Load base model with quantization\nbase_model = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    quantization_config=bnb_config,\n    trust_remote_code=True,\n    device_map=\"auto\",\n)\n\n# Load LoRA adapters\nmodel_inference = PeftModel.from_pretrained(base_model, model_path)\n\n# Load tokenizer\ntokenizer_inference = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\ntokenizer_inference.pad_token = tokenizer_inference.eos_token\n\nprint(\"\\n\u2705 Model loaded successfully for inference!\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Define a helper function for text generation\ndef generate_response(prompt, max_new_tokens=200, temperature=0.7, top_k=50, top_p=0.95):\n    \"\"\"\n    Generate a response for the given prompt.\n    \n    Args:\n        prompt: Input text\n        max_new_tokens: Maximum number of tokens to generate\n        temperature: Sampling temperature (higher = more random)\n        top_k: Consider only top K tokens\n        top_p: Nucleus sampling threshold\n    \"\"\"\n    # Tokenize input\n    inputs = tokenizer_inference(prompt, return_tensors=\"pt\").to(model_inference.device)\n    \n    # Generate\n    with torch.no_grad():\n        outputs = model_inference.generate(\n            **inputs,\n            max_new_tokens=max_new_tokens,\n            temperature=temperature,\n            top_k=top_k,\n            top_p=top_p,\n            do_sample=True,\n            pad_token_id=tokenizer_inference.eos_token_id,\n        )\n    \n    # Decode and return\n    response = tokenizer_inference.decode(outputs[0], skip_special_tokens=True)\n    return response\n\nprint(\"\u2705 Generation function ready!\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Example 1: Simple question\nprompt = \"### Human: What is machine learning? ### Assistant:\"\n\nprint(\"Prompt:\")\nprint(prompt)\nprint(\"\\n\" + \"=\"*70)\nprint(\"Response:\")\nprint()\n\nresponse = generate_response(prompt, max_new_tokens=150, temperature=0.7)\nprint(response)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Example 2: Explanation request\nprompt = \"### Human: Explain how neural networks work in simple terms. ### Assistant:\"\n\nprint(\"Prompt:\")\nprint(prompt)\nprint(\"\\n\" + \"=\"*70)\nprint(\"Response:\")\nprint()\n\nresponse = generate_response(prompt, max_new_tokens=200, temperature=0.7)\nprint(response)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Example 3: Coding question\nprompt = \"### Human: Write a Python function to calculate the Fibonacci sequence. ### Assistant:\"\n\nprint(\"Prompt:\")\nprint(prompt)\nprint(\"\\n\" + \"=\"*70)\nprint(\"Response:\")\nprint()\n\nresponse = generate_response(prompt, max_new_tokens=250, temperature=0.5)\nprint(response)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Try your own prompt!\nyour_prompt = \"### Human: What are the benefits of using LoRA for fine-tuning? ### Assistant:\"\n\nprint(\"Your Prompt:\")\nprint(your_prompt)\nprint(\"\\n\" + \"=\"*70)\nprint(\"Response:\")\nprint()\n\nresponse = generate_response(your_prompt, max_new_tokens=200, temperature=0.7)\nprint(response)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \ud83c\udf89 Congratulations!\n\nYou've successfully:\n- \u2705 Loaded Falcon-7B with 4-bit quantization\n- \u2705 Configured LoRA for efficient fine-tuning\n- \u2705 Trained on the Guanaco instruction dataset\n- \u2705 Generated text with your fine-tuned model\n\n## \ud83d\udcca Training Summary\n\n| Metric | Value |\n|--------|-------|\n| Base Model | Falcon-7B (7B parameters) |\n| Quantization | 4-bit (NF4) |\n| PEFT Method | LoRA (rank 64) |\n| Trainable Parameters | ~0.1% of total |\n| Training Steps | 200 |\n| Training Time | ~1-2 hours (RTX 4090) |\n| GPU Memory | ~16GB |\n\n## \ud83d\ude80 Next Steps\n\n1. **Train Longer**: Increase `max_steps` to 500-1000 for better results\n2. **Adjust LoRA**: Try different `r` values (32, 128) and `lora_alpha`\n3. **Different Datasets**: Fine-tune on your own instruction datasets\n4. **Merge Adapters**: Merge LoRA weights into base model for deployment\n5. **Evaluate**: Test on benchmarks to measure improvement\n\n## \ud83d\udca1 Tips for Better Results\n\n1. **Longer Training**: More steps generally improve quality\n2. **Larger LoRA Rank**: Higher `r` = more capacity (but slower)\n3. **Learning Rate**: Try 1e-4 or 5e-4 if results aren't good\n4. **Batch Size**: Increase if you have more GPU memory\n5. **Dataset Quality**: High-quality data > large quantity\n\n## \ud83d\udcda Resources\n\n- [LoRA Paper](https://arxiv.org/abs/2106.09685)\n- [QLoRA Paper](https://arxiv.org/abs/2305.14314)\n- [PEFT Documentation](https://huggingface.co/docs/peft)\n- [TRL Documentation](https://huggingface.co/docs/trl)\n- [Falcon Model Card](https://huggingface.co/tiiuae/falcon-7b)\n\n## \ud83d\udd27 Saving and Sharing Your Model\n\n### Save to Hugging Face Hub\n\n```python\n# Login to Hugging Face\nfrom huggingface_hub import login\nlogin()\n\n# Push to Hub\ntrainer.push_to_hub(\"your-username/falcon-7b-guanaco-lora\")\n```\n\n### Load Your Model Later\n\n```python\nfrom peft import PeftModel\nfrom transformers import AutoModelForCausalLM\n\n# Load base model\nbase_model = AutoModelForCausalLM.from_pretrained(\"ybelkada/falcon-7b-sharded-bf16\")\n\n# Load your LoRA adapters\nmodel = PeftModel.from_pretrained(base_model, \"your-username/falcon-7b-guanaco-lora\")\n```\n\n---\n\n**Happy Fine-Tuning! \ud83c\udf93\u2728**\n\nFor more advanced techniques, check out the other notebooks in this repository:\n- [Full Fine-Tuning](../01-Full-Fine-Tuning/)\n- [Instruction Tuning](../03-Instruction-Tuning/)\n- [Reasoning Tuning](../04-Reasoning-Tuning/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}