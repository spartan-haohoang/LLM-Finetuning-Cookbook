{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training GPT-2 From Scratch: A Step-by-Step Guide\n\nThis notebook provides a comprehensive guide to training GPT-2 from scratch using the OpenWebText dataset.\n\n## \ud83d\udcda What You'll Learn\n\n- How to load and stream large datasets efficiently with DeepLake\n- How to configure GPT-2 model architecture  \n- How to set up the training pipeline with the Hugging Face Trainer\n- How to monitor training with Weights & Biases\n- How to perform inference with your trained model\n\n## \ud83d\udd27 Requirements\n\n- **GPU**: 8x NVIDIA A100 (40GB each) recommended for full training\n- **Time**: ~40-45 hours for full training on the complete dataset\n- **For Testing**: Can run on single GPU with reduced dataset/model size\n\n## \ud83d\udcd6 Table of Contents\n\n1. [Setting Up Working Environment](#1-setting-up-working-environment)\n2. [Load Dataset from Deep Lake](#2-load-dataset-from-deep-lake)\n3. [Loading the Model & Tokenizer](#3-loading-the-model--tokenizer)\n4. [Training the Model](#4-training-the-model)\n5. [Inference](#5-inference)\n\n---\n\n**Credits**: This tutorial is based on the excellent article by [Youssef Hosni](https://youssef-hosni.medium.com/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setting Up Working Environment\n\nFirst, we'll install all the necessary packages:\n\n- **transformers**: For working with transformer-based models like GPT-2\n- **deeplake**: For managing and streaming large datasets\n- **wandb**: For experiment tracking and visualization\n- **accelerate**: For optimizing and speeding up model training"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Install required packages\n!pip install -q transformers==4.32.0 deeplake==3.6.19 wandb==0.15.8 accelerate==0.22.0"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Import necessary libraries\nimport deeplake\nimport torch\nfrom transformers import (\n    AutoTokenizer,\n    AutoConfig,\n    GPT2LMHeadModel,\n    Trainer,\n    TrainingArguments,\n    pipeline\n)\nimport wandb\n\nprint(f\"PyTorch version: {torch.__version__}\")\nprint(f\"CUDA available: {torch.cuda.is_available()}\")\nif torch.cuda.is_available():\n    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")\n    print(f\"Number of GPUs: {torch.cuda.device_count()}\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Login to Weights & Biases\n\nWeights & Biases (W&B) will help us track our training progress in real-time.\n\n**Note**: You'll need to create a free account at [wandb.ai](https://wandb.ai) and get your API key."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Login to Weights & Biases\n# You can skip this if you don't want to use W&B\n!wandb login"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Dataset from Deep Lake\n\nWe'll use the **OpenWebText** dataset, which is a collection of Reddit posts with at least three upvotes. This dataset is ideal for building a foundational language model.\n\n### Why DeepLake?\n\nDeepLake allows us to **stream** the dataset batch by batch, which means:\n- \u2705 No need to load the entire dataset into memory\n- \u2705 Efficient resource management\n- \u2705 Seamless data streaming\n\n### Dataset Structure\n\nThe dataset contains two tensors:\n- **text**: The raw textual content\n- **tokens**: Pre-tokenized version (we'll tokenize ourselves)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Load the OpenWebText dataset from ActiveLoop\nds = deeplake.load('hub://activeloop/openwebtext-train')\nds_val = deeplake.load('hub://activeloop/openwebtext-val')\n\nprint(\"\\n=== Training Dataset ===\")\nprint(ds)\nprint(f\"\\nDataset size: {len(ds):,} samples\")\n\nprint(\"\\n=== Validation Dataset ===\")\nprint(ds_val)\nprint(f\"\\nDataset size: {len(ds_val):,} samples\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Let's examine a sample from the dataset\nprint(\"=== Sample Text from Dataset ===\")\nprint(ds[0].text.text())\nprint(\"\\n\" + \"=\"*50 + \"\\n\")\nprint(ds[1].text.text())"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and Configure the Tokenizer\n\nWe'll use the GPT-2 tokenizer from Hugging Face. \n\n**Important**: GPT-2 doesn't have a padding token by default, so we set it to the EOS (End of Sentence) token."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Load the GPT-2 tokenizer\ntokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n\n# Set the padding token to the EOS token\ntokenizer.pad_token = tokenizer.eos_token\n\nprint(f\"Vocabulary size: {len(tokenizer):,}\")\nprint(f\"EOS token: {tokenizer.eos_token} (ID: {tokenizer.eos_token_id})\")\nprint(f\"PAD token: {tokenizer.pad_token} (ID: {tokenizer.pad_token_id})\")\nprint(f\"BOS token: {tokenizer.bos_token}\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Test the tokenizer\ntest_text = \"Hello, how are you doing today?\"\ntokens = tokenizer(test_text, return_tensors=\"pt\")\n\nprint(f\"Original text: {test_text}\")\nprint(f\"\\nTokenized IDs: {tokens['input_ids'][0].tolist()}\")\nprint(f\"\\nDecoded back: {tokenizer.decode(tokens['input_ids'][0])}\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create DataLoaders with Tokenization\n\nWe'll create a transformation function that:\n1. Tokenizes the text\n2. Truncates to max_length (512 tokens)\n3. Pads sequences to the same length\n4. Creates input_ids and labels (both are the same; the Trainer will shift labels automatically)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Define transform to tokenize texts on the fly\ndef get_tokens_transform(tokenizer):\n    \"\"\"\n    Creates a transformation function for tokenizing text samples.\n    \n    Args:\n        tokenizer: The tokenizer to use\n        \n    Returns:\n        A function that tokenizes input samples\n    \"\"\"\n    def tokens_transform(sample_in):\n        # Tokenize the text\n        tokenized_text = tokenizer(\n            sample_in[\"text\"],\n            truncation=True,\n            max_length=512,  # Maximum sequence length\n            padding='max_length',  # Pad to max_length\n            return_tensors=\"pt\"\n        )\n        \n        # Extract the input_ids\n        tokenized_text = tokenized_text[\"input_ids\"][0]\n        \n        # Return both input_ids and labels\n        # For language modeling, labels are the same as inputs (shifted by Trainer)\n        return {\n            \"input_ids\": tokenized_text,\n            \"labels\": tokenized_text\n        }\n    \n    return tokens_transform"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Create data loaders\n# Note: Adjust batch size based on your GPU memory\n# For A100 40GB: batch_size=32 works well\n# For smaller GPUs: reduce to 8 or 4\n\nBATCH_SIZE = 32  # Adjust this based on your GPU\n\nprint(f\"Creating dataloaders with batch size: {BATCH_SIZE}\")\n\nds_train_loader = ds.dataloader()\\\n    .batch(BATCH_SIZE)\\\n    .transform(get_tokens_transform(tokenizer))\\\n    .pytorch()\n\nds_eval_loader = ds_val.dataloader()\\\n    .batch(BATCH_SIZE)\\\n    .transform(get_tokens_transform(tokenizer))\\\n    .pytorch()\n\nprint(\"\u2705 DataLoaders created successfully!\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Test the dataloader by fetching one batch\nprint(\"=== Testing DataLoader ===\")\nsample_batch = next(iter(ds_train_loader))\n\nprint(f\"Batch keys: {sample_batch.keys()}\")\nprint(f\"Input IDs shape: {sample_batch['input_ids'].shape}\")\nprint(f\"Labels shape: {sample_batch['labels'].shape}\")\nprint(f\"\\nFirst sample in batch:\")\nprint(tokenizer.decode(sample_batch['input_ids'][0]))"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Loading the Model & Tokenizer\n\nWe'll use the GPT-2 architecture from Hugging Face. This allows us to:\n- \u2705 Use a well-tested, proven architecture\n- \u2705 Easily scale the model by adjusting hyperparameters\n- \u2705 Customize the model size based on available resources\n\n### Key Hyperparameters\n\n- **n_layer**: Number of transformer decoder blocks\n- **n_embd**: Embedding dimension (hidden size)\n- **n_head**: Number of attention heads\n- **n_positions / n_ctx**: Maximum sequence length\n- **vocab_size**: Size of the vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Load the default GPT-2 configuration\nconfig = AutoConfig.from_pretrained(\"gpt2\")\n\nprint(\"=== Default GPT-2 Configuration ===\")\nprint(config)\nprint(\"\\n=== Key Parameters ===\")\nprint(f\"Number of layers: {config.n_layer}\")\nprint(f\"Embedding dimension: {config.n_embd}\")\nprint(f\"Number of attention heads: {config.n_head}\")\nprint(f\"Context length: {config.n_ctx}\")\nprint(f\"Vocabulary size: {config.vocab_size}\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Initialize model with default config (124M parameters)\nmodel = GPT2LMHeadModel(config)\n\n# Count parameters\nmodel_size = sum(t.numel() for t in model.parameters())\nprint(f\"\\n\ud83c\udfaf GPT-2 (default) size: {model_size/1e6:.1f}M parameters\")\n\n# Print model architecture summary\nprint(\"\\n=== Model Architecture ===\")\nprint(model)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaling Up the Model (Optional)\n\nIf you have more resources, you can create a larger model. Here's an example of creating a ~1B parameter model.\n\n**Warning**: This requires significantly more GPU memory and training time!"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Example: Create a larger GPT-2 model (1B parameters)\n# Uncomment if you want to train a larger model\n\n# config_1b = AutoConfig.from_pretrained(\"gpt2\")\n# config_1b.n_layer = 32\n# config_1b.n_embd = 1600\n# config_1b.n_positions = 512\n# config_1b.n_ctx = 512\n# config_1b.n_head = 32\n\n# model_1b = GPT2LMHeadModel(config_1b)\n# model_size_1b = sum(t.numel() for t in model_1b.parameters())\n# print(f\"GPT2-1B size: {model_size_1b/1e6:.1f}M parameters\")\n\nprint(\"Note: For this tutorial, we'll continue with the 124M parameter model.\")\nprint(\"You can uncomment the code above to train a larger model if you have the resources.\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Training the Model\n\nNow we'll set up the training loop using the Hugging Face Trainer class.\n\n### Training Arguments Explained\n\n- **output_dir**: Where to save checkpoints\n- **num_train_epochs**: Number of training epochs (2 for full dataset)\n- **per_device_train_batch_size**: Batch size per GPU (set to 1 since we batch in dataloader)\n- **gradient_accumulation_steps**: Accumulate gradients over multiple steps\n- **learning_rate**: Initial learning rate\n- **weight_decay**: L2 regularization\n- **warmup_steps**: Learning rate warmup steps\n- **lr_scheduler_type**: Learning rate schedule (cosine decay)\n- **bf16/fp16**: Mixed precision training (faster, less memory)\n- **eval_steps/save_steps**: How often to evaluate and save\n\n### Resource Considerations\n\n**For full training (45 hours on 8x A100):**\n- Use the settings below\n- Batch size: 32 in dataloader, 1 per device\n\n**For testing/small-scale training:**\n- Reduce num_train_epochs to 1\n- Use a smaller batch size\n- Limit training to first N samples"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Define training arguments\nargs = TrainingArguments(\n    output_dir=\"GPT2-training-scratch-openwebtext\",\n    \n    # Evaluation and saving\n    evaluation_strategy=\"steps\",\n    save_strategy=\"steps\",\n    eval_steps=500,\n    save_steps=500,\n    \n    # Training duration\n    num_train_epochs=2,\n    \n    # Batch sizes (set to 1 since we batch in dataloader)\n    per_device_train_batch_size=1,\n    per_device_eval_batch_size=1,\n    gradient_accumulation_steps=1,\n    \n    # Optimization\n    learning_rate=5e-4,\n    weight_decay=0.1,\n    warmup_steps=100,\n    lr_scheduler_type=\"cosine\",\n    \n    # Mixed precision (use bf16 for A100, fp16 for older GPUs)\n    bf16=True,  # Set to False if not supported, use fp16=True instead\n    \n    # Logging\n    logging_steps=1,\n    logging_dir=\"./logs\",\n    \n    # Distributed training\n    ddp_find_unused_parameters=False,\n    \n    # Weights & Biases\n    run_name=\"GPT2-scratch-openwebtext\",\n    report_to=\"wandb\",  # Set to \"none\" if you don't want to use W&B\n    \n    # Additional settings\n    save_total_limit=3,  # Keep only last 3 checkpoints\n    load_best_model_at_end=True,\n    metric_for_best_model=\"eval_loss\",\n)\n\nprint(\"\u2705 Training arguments configured!\")\nprint(f\"\\nOutput directory: {args.output_dir}\")\nprint(f\"Number of epochs: {args.num_train_epochs}\")\nprint(f\"Learning rate: {args.learning_rate}\")\nprint(f\"Mixed precision: bf16={args.bf16}, fp16={args.fp16}\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Trainer Class\n\nWe need to create a custom Trainer class to use our DeepLake dataloaders.\n\nThis class overrides the `get_train_dataloader` and `get_eval_dataloader` methods to return our custom dataloaders."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Custom Trainer class for DeepLake dataloaders\nclass TrainerWithDataLoaders(Trainer):\n    \"\"\"\n    Custom Trainer that uses DeepLake dataloaders.\n    \n    This is necessary because we're using DeepLake's dataloader\n    instead of the default PyTorch DataLoader.\n    \"\"\"\n    def __init__(self, *args, train_dataloader=None, eval_dataloader=None, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.train_dataloader_custom = train_dataloader\n        self.eval_dataloader_custom = eval_dataloader\n\n    def get_train_dataloader(self):\n        \"\"\"Return the training dataloader.\"\"\"\n        return self.train_dataloader_custom\n\n    def get_eval_dataloader(self, eval_dataset=None):\n        \"\"\"Return the evaluation dataloader.\"\"\"\n        return self.eval_dataloader_custom\n\nprint(\"\u2705 Custom Trainer class defined!\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Initialize the Trainer\ntrainer = TrainerWithDataLoaders(\n    model=model,\n    args=args,\n    train_dataloader=ds_train_loader,\n    eval_dataloader=ds_eval_loader,\n)\n\nprint(\"\u2705 Trainer initialized!\")\nprint(\"\\nReady to start training...\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start Training!\n\n**\u26a0\ufe0f Important Notes:**\n\n1. **Full training takes ~45 hours on 8x A100 GPUs**\n2. **For testing**, you may want to:\n   - Reduce `num_train_epochs` to 1 or less\n   - Limit the dataset to first N samples\n   - Use smaller batch sizes\n3. **Monitor training** on Weights & Biases dashboard\n4. **Checkpoints** are saved every 500 steps in the output directory\n\n**To resume training** from a checkpoint:\n```python\ntrainer.train(resume_from_checkpoint=\"path/to/checkpoint\")\n```"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Start training!\n# This will take a long time on the full dataset\n\nprint(\"\ud83d\ude80 Starting training...\")\nprint(\"This will take approximately 45 hours on 8x A100 GPUs for the full dataset.\")\nprint(\"\\nCheckpoints will be saved to:\", args.output_dir)\nprint(\"\\nYou can monitor progress at: https://wandb.ai\")\nprint(\"\\n\" + \"=\"*70)\n\n# Uncomment the line below to start training\n# trainer.train()\n\nprint(\"\\n\u26a0\ufe0f Training is commented out by default.\")\nprint(\"Uncomment 'trainer.train()' above to start actual training.\")\nprint(\"\\nFor testing, consider:\")\nprint(\"  - Reducing num_train_epochs to 1\")\nprint(\"  - Using a subset of the data\")\nprint(\"  - Adjusting batch size based on your GPU\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Progress Visualization\n\nDuring training, you can monitor:\n- **Training loss**: Should decrease smoothly\n- **Evaluation loss**: Measures generalization\n- **Learning rate**: Follows cosine schedule\n- **GPU utilization**: Should be near 100%\n- **Throughput**: Samples/second\n\nAll these metrics are available in your W&B dashboard!"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# After training completes, save the final model\n# Uncomment when training is done\n\n# trainer.save_model(\"./GPT2-scratch-openwebtext-final\")\n# print(\"\u2705 Final model saved!\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Inference\n\nNow let's test our trained model by generating text!\n\nWe'll use the Hugging Face `pipeline` API, which makes text generation simple and flexible.\n\n### Generation Parameters\n\n- **max_length**: Maximum number of tokens to generate\n- **min_length**: Minimum number of tokens to generate\n- **temperature**: Controls randomness (0=deterministic, 1=very random)\n- **top_k**: Consider only top K tokens\n- **top_p**: Nucleus sampling (consider tokens with cumulative probability p)\n- **do_sample**: Whether to use sampling (vs greedy)\n- **num_return_sequences**: Number of different outputs to generate"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Load the trained model for inference\n# Change the path to your checkpoint directory\n\nMODEL_PATH = \"./GPT2-scratch-openwebtext-final\"  # or \"./GPT2-training-scratch-openwebtext/checkpoint-XXXX\"\n\n# Check if model exists\nimport os\nif os.path.exists(MODEL_PATH):\n    print(f\"\u2705 Loading model from: {MODEL_PATH}\")\n    \n    # Create text generation pipeline\n    pipe = pipeline(\n        \"text-generation\",\n        model=MODEL_PATH,\n        tokenizer=tokenizer,\n        device=\"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n    )\n    \n    print(\"\u2705 Pipeline created successfully!\")\nelse:\n    print(f\"\u26a0\ufe0f Model not found at: {MODEL_PATH}\")\n    print(\"\\nYou need to:\")\n    print(\"  1. Train the model first\")\n    print(\"  2. Or download a pretrained checkpoint\")\n    print(\"  3. Update MODEL_PATH to the correct location\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Text\n\nLet's generate some text completions with different prompts!"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Example 1: Simple text completion\nif 'pipe' in globals():\n    prompt = \"The house prices dropped down\"\n    \n    print(f\"Prompt: {prompt}\")\n    print(\"=\"*70)\n    \n    completion = pipe(\n        prompt,\n        max_length=100,\n        num_return_sequences=1,\n        temperature=0.8,\n        do_sample=True,\n        top_k=50,\n        top_p=0.95,\n    )\n    \n    print(completion[0]['generated_text'])\nelse:\n    print(\"\u26a0\ufe0f Model not loaded. Train the model first!\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Example 2: Generate multiple completions\nif 'pipe' in globals():\n    prompt = \"In the year 2030, artificial intelligence\"\n    \n    print(f\"Prompt: {prompt}\")\n    print(\"=\"*70)\n    \n    completions = pipe(\n        prompt,\n        max_length=80,\n        num_return_sequences=3,  # Generate 3 different completions\n        temperature=0.9,\n        do_sample=True,\n    )\n    \n    for i, comp in enumerate(completions, 1):\n        print(f\"\\n--- Completion {i} ---\")\n        print(comp['generated_text'])\n        print()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Example 3: More creative generation (higher temperature)\nif 'pipe' in globals():\n    prompt = \"Once upon a time in a distant galaxy\"\n    \n    print(f\"Prompt: {prompt}\")\n    print(\"=\"*70)\n    \n    completion = pipe(\n        prompt,\n        max_length=120,\n        num_return_sequences=1,\n        temperature=1.2,  # Higher temperature = more creative/random\n        do_sample=True,\n        top_k=50,\n    )\n    \n    print(completion[0]['generated_text'])"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Example 4: More deterministic generation (lower temperature)\nif 'pipe' in globals():\n    prompt = \"The key to success in machine learning is\"\n    \n    print(f\"Prompt: {prompt}\")\n    print(\"=\"*70)\n    \n    completion = pipe(\n        prompt,\n        max_length=100,\n        num_return_sequences=1,\n        temperature=0.3,  # Lower temperature = more focused/deterministic\n        do_sample=True,\n        top_k=50,\n    )\n    \n    print(completion[0]['generated_text'])"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interactive Text Generation\n\nTry your own prompts!"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Interactive generation - try your own prompts!\nif 'pipe' in globals():\n    # Modify this prompt\n    your_prompt = \"The future of technology is\"\n    \n    print(f\"Your Prompt: {your_prompt}\")\n    print(\"=\"*70)\n    \n    completion = pipe(\n        your_prompt,\n        max_length=100,\n        num_return_sequences=1,\n        temperature=0.8,\n        do_sample=True,\n    )\n    \n    print(completion[0]['generated_text'])\nelse:\n    print(\"\u26a0\ufe0f Please train the model first to generate text!\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \ud83c\udf89 Congratulations!\n\nYou've successfully:\n- \u2705 Loaded and streamed a large dataset with DeepLake\n- \u2705 Configured GPT-2 architecture\n- \u2705 Set up a complete training pipeline\n- \u2705 Monitored training with Weights & Biases\n- \u2705 Generated text with your trained model\n\n## \ud83d\ude80 Next Steps\n\n1. **Experiment with hyperparameters**: Try different learning rates, batch sizes, model sizes\n2. **Use a different dataset**: Train on domain-specific data (medical, legal, code, etc.)\n3. **Scale up**: Train a larger model (1B+ parameters)\n4. **Fine-tune**: Use your pre-trained model as a starting point for specific tasks\n5. **Explore PEFT**: Learn parameter-efficient fine-tuning (LoRA, QLoRA)\n\n## \ud83d\udcda Resources\n\n- [Hugging Face Transformers Docs](https://huggingface.co/docs/transformers)\n- [DeepLake Documentation](https://docs.activeloop.ai/)\n- [Weights & Biases Guides](https://docs.wandb.ai/)\n- [GPT-2 Paper](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)\n\n## \ud83d\udcac Questions?\n\n- Check the [README](README.md) in this directory\n- Review the [SETUP.md](../SETUP.md) guide\n- Open an issue on GitHub\n\n---\n\n**Happy Training! \ud83c\udf93\u2728**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}